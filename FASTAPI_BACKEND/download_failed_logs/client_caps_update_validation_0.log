2025-04-14 19:32:04,061 - cephci - log:161 - DEBUG - Completed log configuration
2025-04-14 19:32:04,076 - cephci - run:722 - INFO - Running test clients.client_caps_update_validation.py
2025-04-14 19:32:04,078 - cephci - ceph:1576 - INFO - Execute yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.192
2025-04-14 19:32:05,082 - cephci - ceph:1186 - DEBUG - Updating Subscription Management repositories.
2025-04-14 19:32:05,772 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:05,772 - cephci - ceph:1186 - DEBUG - This system has release set to 9.5 and it receives updates only for this release.
2025-04-14 19:32:05,772 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:06,645 - cephci - ceph:1186 - DEBUG - Last metadata expiration check: 2:35:20 ago on Mon Apr 14 16:56:26 2025.
2025-04-14 19:32:06,909 - cephci - ceph:1186 - DEBUG - Package python3-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:06,911 - cephci - ceph:1186 - DEBUG - Package python3-pip-21.3.1-1.el9.noarch is already installed.
2025-04-14 19:32:06,912 - cephci - ceph:1186 - DEBUG - Package fio-3.35-1.el9.x86_64 is already installed.
2025-04-14 19:32:06,913 - cephci - ceph:1186 - DEBUG - Package fuse-2.9.9-16.el9.x86_64 is already installed.
2025-04-14 19:32:06,915 - cephci - ceph:1186 - DEBUG - Package ceph-fuse-2:19.2.1-126.el9cp.x86_64 is already installed.
2025-04-14 19:32:06,917 - cephci - ceph:1186 - DEBUG - Package attr-2.5.1-3.el9.x86_64 is already installed.
2025-04-14 19:32:06,918 - cephci - ceph:1186 - DEBUG - Package gcc-11.5.0-5.el9_5.x86_64 is already installed.
2025-04-14 19:32:06,920 - cephci - ceph:1186 - DEBUG - Package python3-devel-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:06,921 - cephci - ceph:1186 - DEBUG - Package git-2.43.5-2.el9_5.x86_64 is already installed.
2025-04-14 19:32:06,922 - cephci - ceph:1186 - DEBUG - Package lua-5.4.4-4.el9.x86_64 is already installed.
2025-04-14 19:32:07,022 - cephci - ceph:1186 - DEBUG - Dependencies resolved.
2025-04-14 19:32:07,023 - cephci - ceph:1186 - DEBUG - Nothing to do.
2025-04-14 19:32:07,024 - cephci - ceph:1186 - DEBUG - Complete!
2025-04-14 19:32:07,178 - cephci - ceph:1606 - INFO - Execution of yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.192 took 3.099152 seconds
2025-04-14 19:32:07,221 - cephci - ceph:1576 - INFO - Execute pip3 install xattr numpy scipy on 10.0.195.192
2025-04-14 19:32:08,225 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: xattr in /usr/local/lib64/python3.9/site-packages (1.1.4)
2025-04-14 19:32:08,226 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (2.0.2)
2025-04-14 19:32:08,226 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: scipy in /usr/local/lib64/python3.9/site-packages (1.13.1)
2025-04-14 19:32:08,226 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib64/python3.9/site-packages (from xattr) (1.17.1)
2025-04-14 19:32:08,226 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.16.0->xattr) (2.22)
2025-04-14 19:32:08,226 - cephci - ceph:1186 - ERROR - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-14 19:32:08,227 - cephci - ceph:1606 - INFO - Execution of pip3 install xattr numpy scipy on 10.0.195.192 took 1.005546 seconds
2025-04-14 19:32:08,228 - cephci - ceph:1576 - INFO - Execute ls /home/cephuser on 10.0.195.192
2025-04-14 19:32:09,232 - cephci - ceph:1606 - INFO - Execution of ls /home/cephuser on 10.0.195.192 took 1.003705 seconds
2025-04-14 19:32:09,234 - cephci - ceph:1576 - INFO - Execute rpm -qa | grep -w 'dbench' on 10.0.195.192
2025-04-14 19:32:10,238 - cephci - ceph:1606 - INFO - Execution of rpm -qa | grep -w 'dbench' on 10.0.195.192 took 1.003639 seconds
2025-04-14 19:32:10,253 - cephci - ceph:1576 - INFO - Execute yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.94
2025-04-14 19:32:11,257 - cephci - ceph:1186 - DEBUG - Updating Subscription Management repositories.
2025-04-14 19:32:11,855 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:11,856 - cephci - ceph:1186 - DEBUG - This system has release set to 9.5 and it receives updates only for this release.
2025-04-14 19:32:11,856 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:12,714 - cephci - ceph:1186 - DEBUG - Last metadata expiration check: 2:27:18 ago on Mon Apr 14 17:04:34 2025.
2025-04-14 19:32:12,963 - cephci - ceph:1186 - DEBUG - Package python3-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:12,965 - cephci - ceph:1186 - DEBUG - Package python3-pip-21.3.1-1.el9.noarch is already installed.
2025-04-14 19:32:12,967 - cephci - ceph:1186 - DEBUG - Package fio-3.35-1.el9.x86_64 is already installed.
2025-04-14 19:32:12,969 - cephci - ceph:1186 - DEBUG - Package fuse-2.9.9-16.el9.x86_64 is already installed.
2025-04-14 19:32:12,970 - cephci - ceph:1186 - DEBUG - Package ceph-fuse-2:19.2.1-126.el9cp.x86_64 is already installed.
2025-04-14 19:32:12,972 - cephci - ceph:1186 - DEBUG - Package attr-2.5.1-3.el9.x86_64 is already installed.
2025-04-14 19:32:12,974 - cephci - ceph:1186 - DEBUG - Package gcc-11.5.0-5.el9_5.x86_64 is already installed.
2025-04-14 19:32:12,975 - cephci - ceph:1186 - DEBUG - Package python3-devel-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:12,977 - cephci - ceph:1186 - DEBUG - Package git-2.43.5-2.el9_5.x86_64 is already installed.
2025-04-14 19:32:12,979 - cephci - ceph:1186 - DEBUG - Package lua-5.4.4-4.el9.x86_64 is already installed.
2025-04-14 19:32:13,073 - cephci - ceph:1186 - DEBUG - Dependencies resolved.
2025-04-14 19:32:13,074 - cephci - ceph:1186 - DEBUG - Nothing to do.
2025-04-14 19:32:13,075 - cephci - ceph:1186 - DEBUG - Complete!
2025-04-14 19:32:13,203 - cephci - ceph:1606 - INFO - Execution of yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.94 took 2.948974 seconds
2025-04-14 19:32:13,244 - cephci - ceph:1576 - INFO - Execute pip3 install xattr numpy scipy on 10.0.195.94
2025-04-14 19:32:14,248 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: xattr in /usr/local/lib64/python3.9/site-packages (1.1.4)
2025-04-14 19:32:14,249 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (2.0.2)
2025-04-14 19:32:14,249 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: scipy in /usr/local/lib64/python3.9/site-packages (1.13.1)
2025-04-14 19:32:14,249 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib64/python3.9/site-packages (from xattr) (1.17.1)
2025-04-14 19:32:14,249 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.16.0->xattr) (2.22)
2025-04-14 19:32:14,249 - cephci - ceph:1186 - ERROR - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-14 19:32:14,250 - cephci - ceph:1606 - INFO - Execution of pip3 install xattr numpy scipy on 10.0.195.94 took 1.005158 seconds
2025-04-14 19:32:14,251 - cephci - ceph:1576 - INFO - Execute ls /home/cephuser on 10.0.195.94
2025-04-14 19:32:15,256 - cephci - ceph:1606 - INFO - Execution of ls /home/cephuser on 10.0.195.94 took 1.003743 seconds
2025-04-14 19:32:15,258 - cephci - ceph:1576 - INFO - Execute rpm -qa | grep -w 'dbench' on 10.0.195.94
2025-04-14 19:32:16,262 - cephci - ceph:1606 - INFO - Execution of rpm -qa | grep -w 'dbench' on 10.0.195.94 took 1.003811 seconds
2025-04-14 19:32:16,263 - cephci - ceph:1576 - INFO - Execute yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.5
2025-04-14 19:32:17,269 - cephci - ceph:1186 - DEBUG - Updating Subscription Management repositories.
2025-04-14 19:32:17,811 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:17,812 - cephci - ceph:1186 - DEBUG - This system has release set to 9.5 and it receives updates only for this release.
2025-04-14 19:32:17,812 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:18,611 - cephci - ceph:1186 - DEBUG - Last metadata expiration check: 1:40:42 ago on Mon Apr 14 17:51:15 2025.
2025-04-14 19:32:18,848 - cephci - ceph:1186 - DEBUG - Package python3-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:18,850 - cephci - ceph:1186 - DEBUG - Package python3-pip-21.3.1-1.el9.noarch is already installed.
2025-04-14 19:32:18,852 - cephci - ceph:1186 - DEBUG - Package fio-3.35-1.el9.x86_64 is already installed.
2025-04-14 19:32:18,852 - cephci - ceph:1186 - DEBUG - Package fuse-2.9.9-16.el9.x86_64 is already installed.
2025-04-14 19:32:18,852 - cephci - ceph:1186 - DEBUG - Package ceph-fuse-2:19.2.1-126.el9cp.x86_64 is already installed.
2025-04-14 19:32:18,853 - cephci - ceph:1186 - DEBUG - Package attr-2.5.1-3.el9.x86_64 is already installed.
2025-04-14 19:32:18,854 - cephci - ceph:1186 - DEBUG - Package gcc-11.5.0-5.el9_5.x86_64 is already installed.
2025-04-14 19:32:18,856 - cephci - ceph:1186 - DEBUG - Package python3-devel-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:18,857 - cephci - ceph:1186 - DEBUG - Package git-2.43.5-2.el9_5.x86_64 is already installed.
2025-04-14 19:32:18,859 - cephci - ceph:1186 - DEBUG - Package lua-5.4.4-4.el9.x86_64 is already installed.
2025-04-14 19:32:18,952 - cephci - ceph:1186 - DEBUG - Dependencies resolved.
2025-04-14 19:32:18,953 - cephci - ceph:1186 - DEBUG - Nothing to do.
2025-04-14 19:32:18,953 - cephci - ceph:1186 - DEBUG - Complete!
2025-04-14 19:32:19,114 - cephci - ceph:1606 - INFO - Execution of yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.5 took 2.85044 seconds
2025-04-14 19:32:19,156 - cephci - ceph:1576 - INFO - Execute pip3 install xattr numpy scipy on 10.0.195.5
2025-04-14 19:32:20,160 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: xattr in /usr/local/lib64/python3.9/site-packages (1.1.4)
2025-04-14 19:32:20,161 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (2.0.2)
2025-04-14 19:32:20,161 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: scipy in /usr/local/lib64/python3.9/site-packages (1.13.1)
2025-04-14 19:32:20,162 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib64/python3.9/site-packages (from xattr) (1.17.1)
2025-04-14 19:32:20,162 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.16.0->xattr) (2.22)
2025-04-14 19:32:20,162 - cephci - ceph:1186 - ERROR - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-14 19:32:20,162 - cephci - ceph:1606 - INFO - Execution of pip3 install xattr numpy scipy on 10.0.195.5 took 1.005967 seconds
2025-04-14 19:32:20,164 - cephci - ceph:1576 - INFO - Execute ls /home/cephuser on 10.0.195.5
2025-04-14 19:32:21,169 - cephci - ceph:1606 - INFO - Execution of ls /home/cephuser on 10.0.195.5 took 1.004064 seconds
2025-04-14 19:32:21,171 - cephci - ceph:1576 - INFO - Execute rpm -qa | grep -w 'dbench' on 10.0.195.5
2025-04-14 19:32:22,176 - cephci - ceph:1606 - INFO - Execution of rpm -qa | grep -w 'dbench' on 10.0.195.5 took 1.004286 seconds
2025-04-14 19:32:22,178 - cephci - ceph:1576 - INFO - Execute yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.217
2025-04-14 19:32:23,183 - cephci - ceph:1186 - DEBUG - Updating Subscription Management repositories.
2025-04-14 19:32:23,673 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:23,674 - cephci - ceph:1186 - DEBUG - This system has release set to 9.5 and it receives updates only for this release.
2025-04-14 19:32:23,674 - cephci - ceph:1186 - DEBUG - 
2025-04-14 19:32:24,433 - cephci - ceph:1186 - DEBUG - Last metadata expiration check: 2:38:27 ago on Mon Apr 14 16:53:36 2025.
2025-04-14 19:32:24,663 - cephci - ceph:1186 - DEBUG - Package python3-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:24,664 - cephci - ceph:1186 - DEBUG - Package python3-pip-21.3.1-1.el9.noarch is already installed.
2025-04-14 19:32:24,665 - cephci - ceph:1186 - DEBUG - Package fio-3.35-1.el9.x86_64 is already installed.
2025-04-14 19:32:24,667 - cephci - ceph:1186 - DEBUG - Package fuse-2.9.9-16.el9.x86_64 is already installed.
2025-04-14 19:32:24,668 - cephci - ceph:1186 - DEBUG - Package ceph-fuse-2:19.2.1-126.el9cp.x86_64 is already installed.
2025-04-14 19:32:24,669 - cephci - ceph:1186 - DEBUG - Package attr-2.5.1-3.el9.x86_64 is already installed.
2025-04-14 19:32:24,670 - cephci - ceph:1186 - DEBUG - Package gcc-11.5.0-5.el9_5.x86_64 is already installed.
2025-04-14 19:32:24,671 - cephci - ceph:1186 - DEBUG - Package python3-devel-3.9.21-1.el9_5.x86_64 is already installed.
2025-04-14 19:32:24,673 - cephci - ceph:1186 - DEBUG - Package git-2.43.5-2.el9_5.x86_64 is already installed.
2025-04-14 19:32:24,674 - cephci - ceph:1186 - DEBUG - Package lua-5.4.4-4.el9.x86_64 is already installed.
2025-04-14 19:32:24,764 - cephci - ceph:1186 - DEBUG - Dependencies resolved.
2025-04-14 19:32:24,766 - cephci - ceph:1186 - DEBUG - Nothing to do.
2025-04-14 19:32:24,766 - cephci - ceph:1186 - DEBUG - Complete!
2025-04-14 19:32:25,918 - cephci - ceph:1606 - INFO - Execution of yum install -y --nogpgcheck python3 python3-pip fio fuse ceph-fuse attr gcc python3-devel git lua on 10.0.195.217 took 3.739288 seconds
2025-04-14 19:32:25,920 - cephci - ceph:1576 - INFO - Execute pip3 install xattr numpy scipy on 10.0.195.217
2025-04-14 19:32:26,924 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: xattr in /usr/local/lib64/python3.9/site-packages (1.1.4)
2025-04-14 19:32:26,925 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: numpy in /usr/local/lib64/python3.9/site-packages (2.0.2)
2025-04-14 19:32:26,925 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: scipy in /usr/local/lib64/python3.9/site-packages (1.13.1)
2025-04-14 19:32:26,925 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: cffi>=1.16.0 in /usr/local/lib64/python3.9/site-packages (from xattr) (1.17.1)
2025-04-14 19:32:26,926 - cephci - ceph:1186 - DEBUG - Requirement already satisfied: pycparser in /usr/local/lib/python3.9/site-packages (from cffi>=1.16.0->xattr) (2.22)
2025-04-14 19:32:26,926 - cephci - ceph:1186 - ERROR - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-14 19:32:26,926 - cephci - ceph:1606 - INFO - Execution of pip3 install xattr numpy scipy on 10.0.195.217 took 1.005501 seconds
2025-04-14 19:32:26,928 - cephci - ceph:1576 - INFO - Execute ls /home/cephuser on 10.0.195.217
2025-04-14 19:32:27,932 - cephci - ceph:1606 - INFO - Execution of ls /home/cephuser on 10.0.195.217 took 1.004028 seconds
2025-04-14 19:32:27,934 - cephci - ceph:1576 - INFO - Execute rpm -qa | grep -w 'dbench' on 10.0.195.217
2025-04-14 19:32:28,940 - cephci - ceph:1606 - INFO - Execution of rpm -qa | grep -w 'dbench' on 10.0.195.217 took 1.005323 seconds
2025-04-14 19:32:28,942 - cephci - ceph:1576 - INFO - Execute ceph fs ls --format json-pretty on 10.0.195.192
2025-04-14 19:32:29,947 - cephci - ceph:1606 - INFO - Execution of ceph fs ls --format json-pretty on 10.0.195.192 took 1.003901 seconds
2025-04-14 19:32:29,948 - cephci - cephfs_utilsV1:729 - INFO - Giving required permissions for clients:
2025-04-14 19:32:29,950 - cephci - ceph:1576 - INFO - Execute ceph auth get client.ceph-regression-rpbzec-wrs7oy-node8 on 10.0.195.192
2025-04-14 19:32:30,954 - cephci - ceph:1606 - INFO - Execution of ceph auth get client.ceph-regression-rpbzec-wrs7oy-node8 on 10.0.195.192 took 1.004159 seconds
2025-04-14 19:32:30,957 - cephci - ceph:1576 - INFO - Execute ceph auth del client.ceph-regression-rpbzec-wrs7oy-node8 on 10.0.195.192
2025-04-14 19:32:31,962 - cephci - ceph:1606 - INFO - Execution of ceph auth del client.ceph-regression-rpbzec-wrs7oy-node8 on 10.0.195.192 took 1.004038 seconds
2025-04-14 19:32:31,964 - cephci - ceph:1576 - INFO - Execute ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node8 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node8.keyring on 10.0.195.192
2025-04-14 19:32:32,969 - cephci - ceph:1606 - INFO - Execution of ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node8 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node8.keyring on 10.0.195.192 took 1.004348 seconds
2025-04-14 19:32:32,971 - cephci - ceph:1576 - INFO - Execute chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node8.keyring on 10.0.195.192
2025-04-14 19:32:33,976 - cephci - ceph:1606 - INFO - Execution of chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node8.keyring on 10.0.195.192 took 1.004149 seconds
2025-04-14 19:32:33,977 - cephci - cephfs_utilsV1:729 - INFO - Giving required permissions for clients:
2025-04-14 19:32:33,978 - cephci - ceph:1576 - INFO - Execute ceph auth get client.ceph-regression-rpbzec-wrs7oy-node9 on 10.0.195.94
2025-04-14 19:32:34,983 - cephci - ceph:1606 - INFO - Execution of ceph auth get client.ceph-regression-rpbzec-wrs7oy-node9 on 10.0.195.94 took 1.00381 seconds
2025-04-14 19:32:34,985 - cephci - ceph:1576 - INFO - Execute ceph auth del client.ceph-regression-rpbzec-wrs7oy-node9 on 10.0.195.94
2025-04-14 19:32:39,995 - cephci - ceph:1606 - INFO - Execution of ceph auth del client.ceph-regression-rpbzec-wrs7oy-node9 on 10.0.195.94 took 5.008884 seconds
2025-04-14 19:32:39,998 - cephci - ceph:1576 - INFO - Execute ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node9 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node9.keyring on 10.0.195.94
2025-04-14 19:32:41,003 - cephci - ceph:1606 - INFO - Execution of ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node9 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node9.keyring on 10.0.195.94 took 1.004544 seconds
2025-04-14 19:32:41,006 - cephci - ceph:1576 - INFO - Execute chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node9.keyring on 10.0.195.94
2025-04-14 19:32:42,011 - cephci - ceph:1606 - INFO - Execution of chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node9.keyring on 10.0.195.94 took 1.003939 seconds
2025-04-14 19:32:42,012 - cephci - cephfs_utilsV1:729 - INFO - Giving required permissions for clients:
2025-04-14 19:32:42,014 - cephci - ceph:1576 - INFO - Execute ceph auth get client.ceph-regression-rpbzec-wrs7oy-node10 on 10.0.195.5
2025-04-14 19:32:43,018 - cephci - ceph:1606 - INFO - Execution of ceph auth get client.ceph-regression-rpbzec-wrs7oy-node10 on 10.0.195.5 took 1.003917 seconds
2025-04-14 19:32:43,021 - cephci - ceph:1576 - INFO - Execute ceph auth del client.ceph-regression-rpbzec-wrs7oy-node10 on 10.0.195.5
2025-04-14 19:32:44,025 - cephci - ceph:1606 - INFO - Execution of ceph auth del client.ceph-regression-rpbzec-wrs7oy-node10 on 10.0.195.5 took 1.003852 seconds
2025-04-14 19:32:44,027 - cephci - ceph:1576 - INFO - Execute ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node10 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node10.keyring on 10.0.195.5
2025-04-14 19:32:45,032 - cephci - ceph:1606 - INFO - Execution of ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node10 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node10.keyring on 10.0.195.5 took 1.003603 seconds
2025-04-14 19:32:45,034 - cephci - ceph:1576 - INFO - Execute chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node10.keyring on 10.0.195.5
2025-04-14 19:32:46,038 - cephci - ceph:1606 - INFO - Execution of chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node10.keyring on 10.0.195.5 took 1.003389 seconds
2025-04-14 19:32:46,039 - cephci - cephfs_utilsV1:729 - INFO - Giving required permissions for clients:
2025-04-14 19:32:46,041 - cephci - ceph:1576 - INFO - Execute ceph auth get client.ceph-regression-rpbzec-wrs7oy-node11 on 10.0.195.217
2025-04-14 19:32:47,045 - cephci - ceph:1606 - INFO - Execution of ceph auth get client.ceph-regression-rpbzec-wrs7oy-node11 on 10.0.195.217 took 1.003944 seconds
2025-04-14 19:32:47,047 - cephci - ceph:1576 - INFO - Execute ceph auth del client.ceph-regression-rpbzec-wrs7oy-node11 on 10.0.195.217
2025-04-14 19:32:48,052 - cephci - ceph:1606 - INFO - Execution of ceph auth del client.ceph-regression-rpbzec-wrs7oy-node11 on 10.0.195.217 took 1.003948 seconds
2025-04-14 19:32:48,053 - cephci - ceph:1576 - INFO - Execute ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node11 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node11.keyring on 10.0.195.217
2025-04-14 19:32:49,058 - cephci - ceph:1606 - INFO - Execution of ceph auth get-or-create client.ceph-regression-rpbzec-wrs7oy-node11 mon 'allow *' mds 'allow *, allow * path=/' osd 'allow *' -o /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node11.keyring on 10.0.195.217 took 1.003616 seconds
2025-04-14 19:32:49,060 - cephci - ceph:1576 - INFO - Execute chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node11.keyring on 10.0.195.217
2025-04-14 19:32:50,064 - cephci - ceph:1606 - INFO - Execution of chmod 644 /etc/ceph/ceph.client.ceph-regression-rpbzec-wrs7oy-node11.keyring on 10.0.195.217 took 1.003825 seconds
2025-04-14 19:32:50,067 - cephci - ceph:1576 - INFO - Execute ceph crash ls-new -f json on 10.0.195.192
2025-04-14 19:32:51,072 - cephci - ceph:1606 - INFO - Execution of ceph crash ls-new -f json on 10.0.195.192 took 1.004099 seconds
2025-04-14 19:32:51,072 - cephci - client_caps_update_validation:378 - INFO - Crash status before Test: []
2025-04-14 19:32:51,072 - cephci - client_caps_update_validation:380 - INFO - Setup test configuration
2025-04-14 19:32:51,073 - cephci - client_caps_update_validation:23 - INFO - Create fs volume if doesn't exist
2025-04-14 19:32:51,074 - cephci - ceph:1576 - INFO - Execute ceph fs volume create cephfs on 10.0.195.192
2025-04-14 19:32:53,080 - cephci - ceph:1606 - INFO - Execution of ceph fs volume create cephfs on 10.0.195.192 took 2.004869 seconds
2025-04-14 19:32:53,082 - cephci - ceph:1576 - INFO - Execute ceph fs ls --format json on 10.0.195.192
2025-04-14 19:32:54,087 - cephci - ceph:1606 - INFO - Execution of ceph fs ls --format json on 10.0.195.192 took 1.004351 seconds
2025-04-14 19:32:54,087 - cephci - cephfs_utilsV1:1518 - INFO - Validating the creation of pools
2025-04-14 19:32:54,089 - cephci - ceph:1576 - INFO - Execute ceph osd lspools | grep -E 'cephfs.cephfs.data|cephfs.cephfs.meta' | wc -l on 10.0.195.192
2025-04-14 19:32:55,095 - cephci - ceph:1606 - INFO - Execution of ceph osd lspools | grep -E 'cephfs.cephfs.data|cephfs.cephfs.meta' | wc -l on 10.0.195.192 took 1.004473 seconds
2025-04-14 19:32:55,095 - cephci - cephfs_utilsV1:1527 - INFO - Creation of pools for cephfs is successful
2025-04-14 19:32:55,096 - cephci - client_caps_update_validation:29 - INFO - Configure/Verify 5 MDS for FS Volume, 2 Active, 3 Standyby
2025-04-14 19:32:55,099 - cephci - ceph:1576 - INFO - Execute ceph fs get cephfs on 10.0.195.192
2025-04-14 19:32:56,105 - cephci - ceph:1606 - INFO - Execution of ceph fs get cephfs on 10.0.195.192 took 1.005472 seconds
2025-04-14 19:32:56,105 - cephci - client_caps_update_validation:35 - INFO - max_mds before test 2
2025-04-14 19:32:56,107 - cephci - ceph:1576 - INFO - Execute ceph fs status cephfs --format json on 10.0.195.192
2025-04-14 19:32:57,113 - cephci - ceph:1606 - INFO - Execution of ceph fs status cephfs --format json on 10.0.195.192 took 1.00491 seconds
2025-04-14 19:32:57,115 - cephci - ceph:1576 - INFO - Execute ceph fs set cephfs max_mds 2 on 10.0.195.192
2025-04-14 19:33:03,126 - cephci - ceph:1606 - INFO - Execution of ceph fs set cephfs max_mds 2 on 10.0.195.192 took 6.010182 seconds
2025-04-14 19:33:03,128 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:33:04,133 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004139 seconds
2025-04-14 19:33:04,133 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11455,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186112,"bytes_used":8602198016,"bytes_avail":249028730880,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":481,"modified":"2025-04-14T19:32:40.636389+0000","services":{"mds":{"daemons":{"summary":"","cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}},"osd":{"daemons":{"summary":"","13":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}},"15":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}}}},"progress_events":{}}

2025-04-14 19:33:04,135 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 30 seconds...
2025-04-14 19:33:34,166 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:33:35,172 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.005283 seconds
2025-04-14 19:33:35,173 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11486,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"write_bytes_sec":341,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":482,"modified":"2025-04-14T19:32:42.637962+0000","services":{"osd":{"daemons":{"summary":"","13":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}},"15":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}}}},"progress_events":{}}

2025-04-14 19:33:35,174 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 60 seconds...
2025-04-14 19:34:35,227 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:34:36,232 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004907 seconds
2025-04-14 19:34:36,233 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11547,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":484,"modified":"2025-04-14T19:33:30.670053+0000","services":{}},"progress_events":{}}

2025-04-14 19:34:36,234 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 120 seconds...
2025-04-14 19:36:36,330 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:36:37,334 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004328 seconds
2025-04-14 19:36:37,335 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11668,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":486,"modified":"2025-04-14T19:35:06.738020+0000","services":{}},"progress_events":{}}

2025-04-14 19:36:37,336 - cephci - client_caps_update_validation:325 - INFO - Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:36:37,336 - cephci - client_caps_update_validation:326 - INFO - Wait for sometime to check if Cluster health can be OK, current state : Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:36:42,343 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:36:43,348 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004456 seconds
2025-04-14 19:36:43,349 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11674,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":486,"modified":"2025-04-14T19:35:06.738020+0000","services":{}},"progress_events":{}}

2025-04-14 19:36:43,350 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 30 seconds...
2025-04-14 19:37:13,377 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:37:14,382 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004361 seconds
2025-04-14 19:37:14,382 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11705,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":486,"modified":"2025-04-14T19:35:06.738020+0000","services":{}},"progress_events":{}}

2025-04-14 19:37:14,383 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 60 seconds...
2025-04-14 19:38:14,443 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:38:15,448 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004485 seconds
2025-04-14 19:38:15,448 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11766,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":487,"modified":"2025-04-14T19:37:00.822156+0000","services":{}},"progress_events":{}}

2025-04-14 19:38:15,449 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 120 seconds...
2025-04-14 19:40:15,551 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:40:16,556 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.003983 seconds
2025-04-14 19:40:16,556 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11887,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":487,"modified":"2025-04-14T19:37:00.822156+0000","services":{}},"progress_events":{}}

2025-04-14 19:40:16,557 - cephci - client_caps_update_validation:325 - INFO - Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:40:16,557 - cephci - client_caps_update_validation:326 - INFO - Wait for sometime to check if Cluster health can be OK, current state : Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:40:21,562 - cephci - client_caps_update_validation:473 - ERROR - 'int' object is not subscriptable
2025-04-14 19:40:21,563 - cephci - client_caps_update_validation:474 - ERROR - Traceback (most recent call last):
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/cephfs/99/cephci/tests/cephfs/clients/client_caps_update_validation.py", line 382, in run
    fs_name = setup_params["default_fs"]
TypeError: 'int' object is not subscriptable

2025-04-14 19:40:21,563 - cephci - client_caps_update_validation:477 - INFO - Cleaning up the system
2025-04-14 19:40:21,565 - cephci - ceph:1576 - INFO - Execute ceph crash ls-new -f json on 10.0.195.192
2025-04-14 19:40:22,569 - cephci - ceph:1606 - INFO - Execution of ceph crash ls-new -f json on 10.0.195.192 took 1.003831 seconds
2025-04-14 19:40:22,570 - cephci - client_caps_update_validation:480 - INFO - Crash status after Test: []
2025-04-14 19:40:22,571 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:40:23,575 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.003185 seconds
2025-04-14 19:40:23,575 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11894,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":487,"modified":"2025-04-14T19:37:00.822156+0000","services":{}},"progress_events":{}}

2025-04-14 19:40:23,576 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 30 seconds...
2025-04-14 19:40:53,607 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:40:54,611 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.003649 seconds
2025-04-14 19:40:54,612 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11925,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":487,"modified":"2025-04-14T19:37:00.822156+0000","services":{}},"progress_events":{}}

2025-04-14 19:40:54,613 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 60 seconds...
2025-04-14 19:41:54,673 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:41:55,678 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.003837 seconds
2025-04-14 19:41:55,678 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":11986,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":488,"modified":"2025-04-14T19:41:31.016395+0000","services":{}},"progress_events":{}}

2025-04-14 19:41:55,678 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 120 seconds...
2025-04-14 19:43:55,779 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:43:56,784 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.00478 seconds
2025-04-14 19:43:56,784 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":12107,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":492,"modified":"2025-04-14T19:43:31.101730+0000","services":{}},"progress_events":{}}

2025-04-14 19:43:56,785 - cephci - client_caps_update_validation:325 - INFO - Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:43:56,785 - cephci - client_caps_update_validation:326 - INFO - Wait for sometime to check if Cluster health can be OK, current state : Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:44:01,790 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:44:02,795 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.003723 seconds
2025-04-14 19:44:02,795 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":12113,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":492,"modified":"2025-04-14T19:43:31.101730+0000","services":{}},"progress_events":{}}

2025-04-14 19:44:02,796 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 30 seconds...
2025-04-14 19:44:32,824 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:44:33,829 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004056 seconds
2025-04-14 19:44:33,830 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":12144,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":492,"modified":"2025-04-14T19:43:31.101730+0000","services":{}},"progress_events":{}}

2025-04-14 19:44:33,831 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 60 seconds...
2025-04-14 19:45:33,882 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:45:34,889 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004978 seconds
2025-04-14 19:45:34,889 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":12206,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":493,"modified":"2025-04-14T19:44:59.165874+0000","services":{"osd":{"daemons":{"summary":"","5":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}}}},"progress_events":{}}

2025-04-14 19:45:34,890 - cephci - retry:28 - WARNING - Ceph Cluster is in HEALTH_WARN State, Retrying in 120 seconds...
2025-04-14 19:47:34,988 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.192
2025-04-14 19:47:35,994 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.192 took 1.004843 seconds
2025-04-14 19:47:35,994 - cephci - cephfs_utilsV1:3692 - INFO - 
{"fsid":"f69a23f6-1930-11f0-b91e-fa163ef8a5c7","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"12 OSD(s) experiencing slow operations in BlueStore","count":12},"muted":false}},"mutes":[]},"election_epoch":26,"quorum":[0,1,2],"quorum_names":["ceph-regression-rpbzec-wrs7oy-node1-installer","ceph-regression-rpbzec-wrs7oy-node2","ceph-regression-rpbzec-wrs7oy-node3"],"quorum_age":12327,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1095,"num_osds":16,"num_up_osds":16,"osd_up_since":1744657781,"num_in_osds":16,"osd_in_since":1744647058,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":609}],"num_pgs":609,"num_pools":7,"num_objects":96,"data_bytes":107186394,"bytes_used":8602181632,"bytes_avail":249028747264,"bytes_total":257630928896,"read_bytes_sec":85,"read_op_per_sec":0,"write_op_per_sec":0},"fsmap":{"epoch":793,"btime":"2025-04-14T19:32:41:892386+0000","id":5,"up":2,"in":2,"max":2,"id":6,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":6,"rank":0,"name":"cephfs-ec.ceph-regression-rpbzec-wrs7oy-node6.qgonvq","status":"up:active","gid":70425},{"filesystem_id":5,"rank":1,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq","status":"up:active","gid":70846},{"filesystem_id":5,"rank":0,"name":"cephfs.ceph-regression-rpbzec-wrs7oy-node3.dpunct","status":"up:active","gid":80165}],"up:standby":3},"mgrmap":{"available":true,"num_standbys":1,"modules":["cephadm","dashboard","iostat","nfs","smb"],"services":{"dashboard":"https://10.0.195.83:8443/"}},"servicemap":{"epoch":496,"modified":"2025-04-14T19:46:21.223993+0000","services":{"mds":{"daemons":{"summary":"","cephfs-ec.ceph-regression-rpbzec-wrs7oy-node7.vmoikj":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}},"cephfs.ceph-regression-rpbzec-wrs7oy-node4.gjpggq":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}}}},"progress_events":{}}

2025-04-14 19:47:35,995 - cephci - client_caps_update_validation:325 - INFO - Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:47:35,995 - cephci - client_caps_update_validation:326 - INFO - Wait for sometime to check if Cluster health can be OK, current state : Ceph Cluster is in HEALTH_WARN State
2025-04-14 19:47:41,000 - cephci - run:854 - ERROR - Cluster health is not OK even after waiting for 300secs
Traceback (most recent call last):
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/cephfs/99/cephci/run.py", line 839, in run
    rc = test_mod.run(
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/cephfs/99/cephci/tests/cephfs/clients/client_caps_update_validation.py", line 482, in run
    assert False, "Cluster health is not OK even after waiting for 300secs"
AssertionError: Cluster health is not OK even after waiting for 300secs
2025-04-14 19:47:41,002 - cephci - ceph:1576 - INFO - Execute podman --version | awk {'print $3'} on 10.0.195.83
2025-04-14 19:47:42,006 - cephci - ceph:1606 - INFO - Execution of podman --version | awk {'print $3'} on 10.0.195.83 took 1.003722 seconds
2025-04-14 19:47:42,007 - cephci - run:1065 - INFO - Podman Version 5.2.2
2025-04-14 19:47:42,008 - cephci - ceph:1576 - INFO - Execute docker --version | awk {'print $3'} on 10.0.195.83
2025-04-14 19:47:43,013 - cephci - ceph:1606 - INFO - Execution of docker --version | awk {'print $3'} on 10.0.195.83 took 1.00415 seconds
2025-04-14 19:47:43,015 - cephci - ceph:1576 - INFO - Execute ceph --version | awk '{print $3}' on 10.0.195.192
2025-04-14 19:47:44,018 - cephci - ceph:1606 - INFO - Execution of ceph --version | awk '{print $3}' on 10.0.195.192 took 1.002909 seconds
2025-04-14 19:47:44,019 - cephci - run:1081 - INFO - ceph Version 19.2.1-126.el9cp
2025-04-14 19:47:44,020 - cephci - run:1040 - INFO - ceph_clusters_file rerun/regression-RPBZEc-WRS7OY
2025-04-14 19:47:44,021 - cephci - run:927 - INFO - Test <module 'clients.client_caps_update_validation' from '/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/cephfs/99/cephci/tests/cephfs/clients/client_caps_update_validation.py'> failed
