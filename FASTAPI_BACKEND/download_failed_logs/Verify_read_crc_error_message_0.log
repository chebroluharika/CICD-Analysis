2025-04-15 12:47:42,333 - cephci - log:161 - DEBUG - Completed log configuration
2025-04-15 12:47:42,336 - cephci - run:722 - INFO - Running test test_crc_check_logs.py
2025-04-15 12:47:42,336 - cephci - test_crc_check_logs:31 - INFO - 
    CEPH-83612766 - Verification of read crc error messages in the OSD logs
    Bug Id: https://bugzilla.redhat.com/show_bug.cgi?id=2253735
    1. Create replicated and ec pool with a single PG
    2. set the debug_osd to 20
    3. Put the object into the pool with the different sizes
    4. Get the object after write the file with different sizes
    5. Get the  acting set of the PG
    6. Check the log contain "read crc" error message
    
2025-04-15 12:47:42,338 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set global log_to_file true on 10.0.195.79
2025-04-15 12:47:43,996 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set global log_to_file true on 10.0.195.79 took 1.658382 seconds
2025-04-15 12:47:43,997 - cephci - shell:64 - DEBUG - 
2025-04-15 12:47:44,038 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set global mon_cluster_log_to_file true on 10.0.195.79
2025-04-15 12:47:45,674 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set global mon_cluster_log_to_file true on 10.0.195.79 took 1.635646 seconds
2025-04-15 12:47:45,675 - cephci - shell:64 - DEBUG - 
2025-04-15 12:47:45,675 - cephci - core_workflows:563 - DEBUG - creating pool_name replicated_pool
2025-04-15 12:47:45,715 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool create replicated_pool 1 1 --pg_num_max 1 on 10.0.195.79
2025-04-15 12:47:48,644 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool create replicated_pool 1 1 --pg_num_max 1 on 10.0.195.79 took 2.928535 seconds
2025-04-15 12:47:48,645 - cephci - shell:64 - DEBUG - 
2025-04-15 12:47:48,646 - cephci - ceph:1576 - INFO - Execute cephadm shell -- sudo ceph osd pool application enable replicated_pool rados on 10.0.195.79
2025-04-15 12:47:51,773 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- sudo ceph osd pool application enable replicated_pool rados on 10.0.195.79 took 3.127312 seconds
2025-04-15 12:47:51,774 - cephci - shell:64 - DEBUG - 
2025-04-15 12:47:56,779 - cephci - core_workflows:620 - INFO - Created pool replicated_pool successfully
2025-04-15 12:47:56,780 - cephci - utils:1858 - DEBUG - The <bound method RadosOrchestrator.create_pool of <ceph.rados.core_workflows.RadosOrchestrator object at 0x7fbdecd8d850>> return status is True
2025-04-15 12:47:56,780 - cephci - core_workflows:1521 - DEBUG - Final command to create EC Profile : ceph osd erasure-code-profile set ecp_ec_pool crush-failure-domain=osd k=3 m=2 plugin=jerasure
2025-04-15 12:47:56,781 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd erasure-code-profile set ecp_ec_pool crush-failure-domain=osd k=3 m=2 plugin=jerasure -f json on 10.0.195.79
2025-04-15 12:48:00,074 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd erasure-code-profile set ecp_ec_pool crush-failure-domain=osd k=3 m=2 plugin=jerasure -f json on 10.0.195.79 took 3.292112 seconds
2025-04-15 12:48:05,081 - cephci - ceph:1576 - INFO - Execute ceph osd erasure-code-profile ls -f json on 10.0.195.240
2025-04-15 12:48:06,085 - cephci - ceph:1606 - INFO - Execution of ceph osd erasure-code-profile ls -f json on 10.0.195.240 took 1.003898 seconds
2025-04-15 12:48:06,086 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd erasure-code-profile get ecp_ec_pool -f json on 10.0.195.79
2025-04-15 12:48:07,624 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd erasure-code-profile get ecp_ec_pool -f json on 10.0.195.79 took 1.537274 seconds
2025-04-15 12:48:07,624 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:48:07,625 - cephci - ceph:1186 - DEBUG - {"crush-device-class":"","crush-failure-domain":"osd","crush-num-failure-domains":"0","crush-osds-per-failure-domain":"0","crush-root":"default","jerasure-per-chunk-alignment":"false","k":"3","m":"2","plugin":"jerasure","technique":"reed_sol_van","w":"8"}
2025-04-15 12:48:07,625 - cephci - core_workflows:1537 - INFO - Profile created : 
 {'crush-device-class': '', 'crush-failure-domain': 'osd', 'crush-num-failure-domains': '0', 'crush-osds-per-failure-domain': '0', 'crush-root': 'default', 'jerasure-per-chunk-alignment': 'false', 'k': '3', 'm': '2', 'plugin': 'jerasure', 'technique': 'reed_sol_van', 'w': '8'}
2025-04-15 12:48:07,625 - cephci - core_workflows:1538 - DEBUG - EC profile created. Proceeding to create pool
2025-04-15 12:48:07,625 - cephci - core_workflows:563 - DEBUG - creating pool_name ec_pool
2025-04-15 12:48:07,666 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool create ec_pool 1 1 --pg_num_max 1 erasure ecp_ec_pool on 10.0.195.79
2025-04-15 12:48:11,207 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool create ec_pool 1 1 --pg_num_max 1 erasure ecp_ec_pool on 10.0.195.79 took 3.540946 seconds
2025-04-15 12:48:11,207 - cephci - shell:64 - DEBUG - 
2025-04-15 12:48:11,249 - cephci - ceph:1576 - INFO - Execute cephadm shell -- sudo ceph osd pool application enable ec_pool rados on 10.0.195.79
2025-04-15 12:48:14,264 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- sudo ceph osd pool application enable ec_pool rados on 10.0.195.79 took 3.015413 seconds
2025-04-15 12:48:14,265 - cephci - shell:64 - DEBUG - 
2025-04-15 12:48:19,270 - cephci - core_workflows:620 - INFO - Created pool ec_pool successfully
2025-04-15 12:48:19,271 - cephci - core_workflows:1569 - INFO - Created the ec profile : ecp_ec_pool and pool : ec_pool
2025-04-15 12:48:19,272 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd crush rule dump ec_pool -f json on 10.0.195.79
2025-04-15 12:48:21,172 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd crush rule dump ec_pool -f json on 10.0.195.79 took 1.899822 seconds
2025-04-15 12:48:21,173 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:48:21,173 - cephci - ceph:1186 - DEBUG - {"rule_id":1,"rule_name":"ec_pool","type":3,"steps":[{"op":"set_chooseleaf_tries","num":5},{"op":"set_choose_tries","num":100},{"op":"take","item":-1,"item_name":"default"},{"op":"choose_indep","num":0,"type":"osd"},{"op":"emit"}]}
2025-04-15 12:48:21,173 - cephci - core_workflows:1571 - DEBUG - Printing the crush rule used : 
{'rule_id': 1, 'rule_name': 'ec_pool', 'type': 3, 'steps': [{'op': 'set_chooseleaf_tries', 'num': 5}, {'op': 'set_choose_tries', 'num': 100}, {'op': 'take', 'item': -1, 'item_name': 'default'}, {'op': 'choose_indep', 'num': 0, 'type': 'osd'}, {'op': 'emit'}]}

2025-04-15 12:48:21,174 - cephci - utils:1858 - DEBUG - The <bound method RadosOrchestrator.create_erasure_pool of <ceph.rados.core_workflows.RadosOrchestrator object at 0x7fbdecd8d850>> return status is True
2025-04-15 12:48:21,214 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd debug_osd 20/20 on 10.0.195.79
2025-04-15 12:48:22,801 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd debug_osd 20/20 on 10.0.195.79 took 1.586786 seconds
2025-04-15 12:48:22,802 - cephci - shell:64 - DEBUG - 
2025-04-15 12:48:32,812 - cephci - monitor_configurations:207 - DEBUG - verifying the value set
2025-04-15 12:48:32,813 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:48:35,358 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 2.544007 seconds
2025-04-15 12:48:35,358 - cephci - monitor_configurations:349 - INFO - Verified the value set for the config: debug_osd
2025-04-15 12:48:35,359 - cephci - monitor_configurations:212 - INFO - Value for config: debug_osd was set
2025-04-15 12:48:35,360 - cephci - ceph:1576 - INFO - Execute dd if=/dev/zero bs=1k count=0 > /tmp/ZeroKb_file.txt on 10.0.195.240
2025-04-15 12:48:36,364 - cephci - ceph:1606 - INFO - Execution of dd if=/dev/zero bs=1k count=0 > /tmp/ZeroKb_file.txt on 10.0.195.240 took 1.003993 seconds
2025-04-15 12:48:36,366 - cephci - ceph:1576 - INFO - Execute dd if=/dev/zero bs=1k count=100 > /tmp/100Kb_file.txt on 10.0.195.240
2025-04-15 12:48:37,371 - cephci - ceph:1606 - INFO - Execution of dd if=/dev/zero bs=1k count=100 > /tmp/100Kb_file.txt on 10.0.195.240 took 1.004139 seconds
2025-04-15 12:48:37,372 - cephci - ceph:1576 - INFO - Execute dd if=/dev/zero bs=1M count=120 > /tmp/120MB_file.txt on 10.0.195.240
2025-04-15 12:48:38,377 - cephci - ceph:1606 - INFO - Execution of dd if=/dev/zero bs=1M count=120 > /tmp/120MB_file.txt on 10.0.195.240 took 1.004032 seconds
2025-04-15 12:48:38,379 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%dT%H:%M:%S.%3N+0000' on 10.0.195.79
2025-04-15 12:48:39,383 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%dT%H:%M:%S.%3N+0000' on 10.0.195.79 took 1.003986 seconds
2025-04-15 12:48:39,384 - cephci - test_crc_check_logs:73 - INFO - Performing the tests on the--- replicated_pool pool
2025-04-15 12:48:39,385 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool put obj-1 /tmp/ZeroKb_file.txt on 10.0.195.240
2025-04-15 12:48:40,389 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool put obj-1 /tmp/ZeroKb_file.txt on 10.0.195.240 took 1.003503 seconds
2025-04-15 12:48:40,390 - cephci - test_crc_check_logs:77 - INFO - Writing the Object with zero size file
2025-04-15 12:48:40,391 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool get obj-1 /tmp/zero_output.txt on 10.0.195.240
2025-04-15 12:48:41,396 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool get obj-1 /tmp/zero_output.txt on 10.0.195.240 took 1.003893 seconds
2025-04-15 12:48:41,396 - cephci - test_crc_check_logs:81 - INFO - Reading the Object that contains the zero size file
2025-04-15 12:48:41,397 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool put obj-1 /tmp/100Kb_file.txt on 10.0.195.240
2025-04-15 12:48:42,401 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool put obj-1 /tmp/100Kb_file.txt on 10.0.195.240 took 1.003896 seconds
2025-04-15 12:48:42,402 - cephci - test_crc_check_logs:85 - INFO - Writing the same object with 100kb  file size
2025-04-15 12:48:42,403 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool get obj-1 /tmp/100Kb_output.txt on 10.0.195.240
2025-04-15 12:48:43,407 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool get obj-1 /tmp/100Kb_output.txt on 10.0.195.240 took 1.003684 seconds
2025-04-15 12:48:43,408 - cephci - test_crc_check_logs:89 - INFO - Reading the same object Object that contains the  100kb file size
2025-04-15 12:48:43,409 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool put obj-1 /tmp/120MB_file.txt on 10.0.195.240
2025-04-15 12:48:51,420 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool put obj-1 /tmp/120MB_file.txt on 10.0.195.240 took 8.010588 seconds
2025-04-15 12:48:51,421 - cephci - test_crc_check_logs:95 - INFO - Writing the same object with 120mb  file size
2025-04-15 12:48:51,422 - cephci - ceph:1576 - INFO - Execute rados -p replicated_pool get obj-1 /tmp/120MB_output.txt on 10.0.195.240
2025-04-15 12:49:37,475 - cephci - ceph:1606 - INFO - Execution of rados -p replicated_pool get obj-1 /tmp/120MB_output.txt on 10.0.195.240 took 46.053458 seconds
2025-04-15 12:49:37,476 - cephci - test_crc_check_logs:99 - INFO - Reading the same object Object that contains the  120mb file size
2025-04-15 12:49:37,476 - cephci - test_crc_check_logs:105 - INFO - Writing and reading completed on the--- replicated_pool pool
2025-04-15 12:49:37,476 - cephci - test_crc_check_logs:73 - INFO - Performing the tests on the--- ec_pool pool
2025-04-15 12:49:37,477 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool put obj-1 /tmp/ZeroKb_file.txt on 10.0.195.240
2025-04-15 12:49:38,482 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool put obj-1 /tmp/ZeroKb_file.txt on 10.0.195.240 took 1.004025 seconds
2025-04-15 12:49:38,482 - cephci - test_crc_check_logs:77 - INFO - Writing the Object with zero size file
2025-04-15 12:49:38,483 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool get obj-1 /tmp/zero_output.txt on 10.0.195.240
2025-04-15 12:49:39,487 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool get obj-1 /tmp/zero_output.txt on 10.0.195.240 took 1.003649 seconds
2025-04-15 12:49:39,488 - cephci - test_crc_check_logs:81 - INFO - Reading the Object that contains the zero size file
2025-04-15 12:49:39,489 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool put obj-1 /tmp/100Kb_file.txt on 10.0.195.240
2025-04-15 12:49:40,494 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool put obj-1 /tmp/100Kb_file.txt on 10.0.195.240 took 1.003851 seconds
2025-04-15 12:49:40,494 - cephci - test_crc_check_logs:85 - INFO - Writing the same object with 100kb  file size
2025-04-15 12:49:40,496 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool get obj-1 /tmp/100Kb_output.txt on 10.0.195.240
2025-04-15 12:49:41,500 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool get obj-1 /tmp/100Kb_output.txt on 10.0.195.240 took 1.003958 seconds
2025-04-15 12:49:41,501 - cephci - test_crc_check_logs:89 - INFO - Reading the same object Object that contains the  100kb file size
2025-04-15 12:49:41,502 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool put obj-1 /tmp/120MB_file.txt on 10.0.195.240
2025-04-15 12:49:49,514 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool put obj-1 /tmp/120MB_file.txt on 10.0.195.240 took 8.011779 seconds
2025-04-15 12:49:49,515 - cephci - test_crc_check_logs:95 - INFO - Writing the same object with 120mb  file size
2025-04-15 12:49:49,516 - cephci - ceph:1576 - INFO - Execute rados -p ec_pool get obj-1 /tmp/120MB_output.txt on 10.0.195.240
2025-04-15 12:50:13,546 - cephci - ceph:1606 - INFO - Execution of rados -p ec_pool get obj-1 /tmp/120MB_output.txt on 10.0.195.240 took 24.029196 seconds
2025-04-15 12:50:13,546 - cephci - test_crc_check_logs:99 - INFO - Reading the same object Object that contains the  120mb file size
2025-04-15 12:50:13,546 - cephci - test_crc_check_logs:105 - INFO - Writing and reading completed on the--- ec_pool pool
2025-04-15 12:50:33,568 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%dT%H:%M:%S.%3N+0000' on 10.0.195.79
2025-04-15 12:50:34,572 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%dT%H:%M:%S.%3N+0000' on 10.0.195.79 took 1.003411 seconds
2025-04-15 12:50:34,574 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph df -f json on 10.0.195.79
2025-04-15 12:50:36,424 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph df -f json on 10.0.195.79 took 1.849685 seconds
2025-04-15 12:50:36,424 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:50:36,425 - cephci - ceph:1186 - DEBUG - {"stats":{"total_bytes":375750918144,"total_avail_bytes":361441234944,"total_used_bytes":14309683200,"total_used_raw_bytes":14309683200,"total_used_raw_ratio":0.038082897663116455,"num_osds":14,"num_per_pool_osds":14,"num_per_pool_omap_osds":14},"stats_by_class":{"hdd":{"total_bytes":375750918144,"total_avail_bytes":361441234944,"total_used_bytes":14309683200,"total_used_raw_bytes":14309683200,"total_used_raw_ratio":0.038082897663116455}},"pools":[{"name":".smb","id":1,"stats":{"stored":0,"objects":0,"kb_used":0,"bytes_used":0,"percent_used":0,"max_avail":120385822720}},{"name":".mgr","id":2,"stats":{"stored":765466,"objects":2,"kb_used":2260,"bytes_used":2314240,"percent_used":6.4078008108481299e-06,"max_avail":120385822720}},{"name":"cephfs.cephfs.meta","id":3,"stats":{"stored":15755,"objects":22,"kb_used":136,"bytes_used":138279,"percent_used":3.8287717529783549e-07,"max_avail":120385822720}},{"name":"cephfs.cephfs.data","id":4,"stats":{"stored":0,"objects":0,"kb_used":0,"bytes_used":0,"percent_used":0,"max_avail":120385822720}},{"name":".rgw.root","id":5,"stats":{"stored":1613,"objects":6,"kb_used":72,"bytes_used":73728,"percent_used":2.0414360335507808e-07,"max_avail":120385822720}},{"name":"default.rgw.log","id":6,"stats":{"stored":3931,"objects":178,"kb_used":436,"bytes_used":446464,"percent_used":1.236201569554396e-06,"max_avail":120385822720}},{"name":"default.rgw.control","id":7,"stats":{"stored":0,"objects":8,"kb_used":0,"bytes_used":0,"percent_used":0,"max_avail":120385822720}},{"name":"default.rgw.meta","id":8,"stats":{"stored":3050,"objects":3,"kb_used":40,"bytes_used":40258,"percent_used":1.1146937595185591e-07,"max_avail":120385822720}},{"name":"replicated_pool","id":15,"stats":{"stored":125829120,"objects":1,"kb_used":368640,"bytes_used":377487360,"percent_used":0.0010441241320222616,"max_avail":120385822720}},{"name":"ec_pool","id":16,"stats":{"stored":125829120,"objects":1,"kb_used":204800,"bytes_used":209715200,"percent_used":0.00058033829554915428,"max_avail":216694489088}}]}
2025-04-15 12:50:36,425 - cephci - test_crc_check_logs:118 - INFO - The replicated_pool pool pg id is -15.0
2025-04-15 12:50:36,425 - cephci - test_crc_check_logs:197 - INFO - The log line to check - full-object read crc
2025-04-15 12:50:36,425 - cephci - core_workflows:699 - DEBUG - Collecting the acting set for the PG : 15.0
2025-04-15 12:50:36,466 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph pg map 15.0 -f json on 10.0.195.79
2025-04-15 12:50:38,018 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph pg map 15.0 -f json on 10.0.195.79 took 1.552291 seconds
2025-04-15 12:50:38,019 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:50:38,019 - cephci - ceph:1186 - DEBUG - {"epoch":6010,"raw_pgid":"15.0","pgid":"15.0","up":[13,1,8],"acting":[13,1,8]}
2025-04-15 12:50:38,019 - cephci - core_workflows:702 - DEBUG - collected the acting set for the PG : 15.0. details of PG map : {'epoch': 6010, 'raw_pgid': '15.0', 'pgid': '15.0', 'up': [13, 1, 8], 'acting': [13, 1, 8]}
2025-04-15 12:50:38,020 - cephci - test_crc_check_logs:200 - INFO - The acting osd set of the pg 15.0 is [13, 1, 8]
2025-04-15 12:50:38,061 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 12:50:40,710 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 2.64904 seconds
2025-04-15 12:50:40,712 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 12:50:42,229 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.516864 seconds
2025-04-15 12:50:42,230 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:50:42,230 - cephci - ceph:1186 - DEBUG - [{"container_id": "640401bbb928", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "6.68%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T12:46:19.411438Z", "memory_request": 4294967296, "memory_usage": 229200000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T11:11:02.621332Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 12:50:42,232 - cephci - ceph:1576 - INFO - Execute awk '$1 >= "2025-04-15T12:48:17.416+0000" && $1 <= "2025-04-15T12:50:12.602+0000"' /var/log/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/ceph-osd.13.log on 10.0.195.191
2025-04-15 12:55:42,567 - cephci - ceph:1632 - ERROR - awk '$1 >= "2025-04-15T12:48:17.416+0000" && $1 <= "2025-04-15T12:50:12.602+0000"' /var/log/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/ceph-osd.13.log failed to execute within 300s.
2025-04-15 12:55:42,568 - cephci - test_crc_check_logs:125 - INFO - Command exceed the allocated execution time.
2025-04-15 12:55:42,568 - cephci - test_crc_check_logs:126 - INFO - Traceback (most recent call last):
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/ceph/ceph.py", line 1603, in long_running
    check_timeout(_end_time, timeout)
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/ceph/ceph.py", line 1157, in check_timeout
    raise TimeoutException("Command exceed the allocated execution time.")
ceph.ceph.TimeoutException: Command exceed the allocated execution time.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/tests/rados/test_crc_check_logs.py", line 119, in run
    if not check_read_crc_log(rados_object, init_time, end_time, pg_id):
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/tests/rados/test_crc_check_logs.py", line 208, in check_read_crc_log
    osd_logs, err = host.exec_command(
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/ceph/ceph.py", line 1668, in exec_command
    _out, _err, _exit, _time = self.long_running(**kw)
  File "/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/ceph/ceph.py", line 1633, in long_running
    raise CommandFailed(tex)
ceph.ceph.CommandFailed: Command exceed the allocated execution time.

2025-04-15 12:55:42,568 - cephci - test_crc_check_logs:129 - INFO - ============Execution of finally block==================
2025-04-15 12:55:42,570 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79
2025-04-15 12:55:44,227 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79 took 1.657086 seconds
2025-04-15 12:55:44,228 - cephci - shell:64 - DEBUG - 
2025-04-15 12:55:54,238 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:55:54,240 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:55:55,998 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.758442 seconds
2025-04-15 12:55:55,999 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:55:55,999 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:55:56,000 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:55:56,000 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ceph-regression-juxejq-zv217a-node1-installer/server_addr","value":"10.0.195.79","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_
2025-04-15 12:55:56,001 - cephci - ceph:1186 - DEBUG - update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:55:56,001 - cephci - monitor_configurations:351 - ERROR - The Config: debug_osd not listed under in the dump
2025-04-15 12:55:56,001 - cephci - monitor_configurations:277 - INFO - Value for config: debug_osd was removed
2025-04-15 12:55:56,003 - cephci - ceph:1576 - INFO - Execute ceph osd pool ls detail -f json on 10.0.195.240
2025-04-15 12:55:57,009 - cephci - ceph:1606 - INFO - Execution of ceph osd pool ls detail -f json on 10.0.195.240 took 1.005696 seconds
2025-04-15 12:55:57,011 - cephci - ceph:1576 - INFO - Execute ceph config set mon mon_allow_pool_delete true on 10.0.195.240
2025-04-15 12:55:58,016 - cephci - ceph:1606 - INFO - Execution of ceph config set mon mon_allow_pool_delete true on 10.0.195.240 took 1.003677 seconds
2025-04-15 12:55:58,017 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:55:59,021 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.003669 seconds
2025-04-15 12:55:59,023 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79
2025-04-15 12:56:00,620 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79 took 1.596436 seconds
2025-04-15 12:56:00,621 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:56:00,621 - cephci - ceph:1186 - DEBUG - [{"pool_id":1,"pool_name":".smb","create_time":"2025-04-15T06:40:40.133891+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":32,"pg_placement_num":32,"pg_placement_num_target":32,"pg_num_target":32,"pg_num_pending":32,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"63","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"47","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{},"application_metadata":{"smb":{}},"read_balance":{"score_type":"Fair distribution","score_acting":1.8799999952316284,"score_stable":1.8799999952316284,"optimal_score":0.93000000715255737,"raw_score_acting":1.75,"raw_score_stable":1.75,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":2,"pool_name":".mgr","create_time":"2025-04-15T06:53:02.356665+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"pee
2025-04-15 12:56:00,622 - cephci - ceph:1186 - DEBUG - ring_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":1,"pg_placement_num":1,"pg_placement_num_target":1,"pg_num_target":1,"pg_num_pending":1,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"34","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"0","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{"pg_num_max":32,"pg_num_min":1},"application_metadata":{"mgr":{}},"read_balance":{"score_type":"Fair distribution","score_acting":15,"score_stable":15,"optimal_score":0.20000000298023224,"raw_score_acting":3,"raw_score_stable":3,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":3,"pool_name":"cephfs.cephfs.meta","create_time":"2025-04-15T06:56:03.084124+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":16,"pg_placeme
2025-04-15 12:56:00,622 - cephci - ceph:1186 - DEBUG - nt_num":16,"pg_placement_num_target":16,"pg_num_target":16,"pg_num_pending":16,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"124","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"99","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{"pg_autoscale_bias":4,"pg_num_min":16,"recovery_priority":5},"application_metadata":{"cephfs":{"metadata":"cephfs"}},"read_balance":{"score_type":"Fair distribution","score_acting":3.7400000095367432,"score_stable":3.7400000095367432,"optimal_score":0.87000000476837158,"raw_score_acting":3.25,"raw_score_stable":3.25,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":4,"pool_name":"cephfs.cephfs.data","create_time":"2025-04-15T06:56:03.742967+0000","flags":131073,"flags_names":"hashpspool,bulk","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":512,"pg_placement_num":512,"pg_placement_num_target":512,"pg_num_target":512,"pg_num_pending":512,"last_pg_m
2025-04-15 12:56:00,623 - cephci - ceph:1186 - DEBUG - erge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"147","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"141","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{},"application_metadata":{"cephfs":{"data":"cephfs"}},"read_balance":{"score_type":"Fair distribution","score_acting":1.4700000286102295,"score_stable":1.4700000286102295,"optimal_score":0.93000000715255737,"raw_score_acting":1.3700000047683716,"raw_score_stable":1.3700000047683716,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":5,"pool_name":".rgw.root","create_time":"2025-04-15T06:56:15.007937+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":32,"pg_placement_num":32,"pg_placement_num_target":32,"pg_num_target":32,"pg_num_pending":32,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_ch
2025-04-15 12:56:00,623 - cephci - ceph:1186 - DEBUG - ange":"124","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"100","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{},"application_metadata":{"rgw":{}},"read_balance":{"score_type":"Fair distribution","score_acting":1.8799999952316284,"score_stable":1.8799999952316284,"optimal_score":0.93000000715255737,"raw_score_acting":1.75,"raw_score_stable":1.75,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":6,"pool_name":"default.rgw.log","create_time":"2025-04-15T06:56:17.217286+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":32,"pg_placement_num":32,"pg_placement_num_target":32,"pg_num_target":32,"pg_num_pending":32,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"236","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"102","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool
2025-04-15 12:56:00,624 - cephci - ceph:1186 - DEBUG - _snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{},"application_metadata":{"rgw":{}},"read_balance":{"score_type":"Fair distribution","score_acting":2.3499999046325684,"score_stable":2.3499999046325684,"optimal_score":0.93000000715255737,"raw_score_acting":2.190000057220459,"raw_score_stable":2.190000057220459,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":7,"pool_name":"default.rgw.control","create_time":"2025-04-15T06:56:18.931943+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":32,"pg_placement_num":32,"pg_placement_num_target":32,"pg_num_target":32,"pg_num_pending":32,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"124","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"102","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_
2025-04-15 12:56:00,624 - cephci - ceph:1186 - DEBUG - max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{},"application_metadata":{"rgw":{}},"read_balance":{"score_type":"Fair distribution","score_acting":1.8799999952316284,"score_stable":1.8799999952316284,"optimal_score":0.93000000715255737,"raw_score_acting":1.75,"raw_score_stable":1.75,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":8,"pool_name":"default.rgw.meta","create_time":"2025-04-15T06:56:21.218354+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":32,"pg_placement_num":32,"pg_placement_num_target":32,"pg_num_target":32,"pg_num_pending":32,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"124","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"104","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"c
2025-04-15 12:56:00,625 - cephci - ceph:1186 - DEBUG - ache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{"pg_autoscale_bias":4},"application_metadata":{"rgw":{}},"read_balance":{"score_type":"Fair distribution","score_acting":1.8799999952316284,"score_stable":1.8799999952316284,"optimal_score":0.93000000715255737,"raw_score_acting":1.75,"raw_score_stable":1.75,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":15,"pool_name":"replicated_pool","create_time":"2025-04-15T12:47:26.369151+0000","flags":1,"flags_names":"hashpspool","type":1,"size":3,"min_size":2,"crush_rule":0,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":1,"pg_placement_num":1,"pg_placement_num_target":1,"pg_num_target":1,"pg_num_pending":1,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"6005","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"0","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":
2025-04-15 12:56:00,625 - cephci - ceph:1186 - DEBUG - 0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":0,"expected_num_objects":0,"fast_read":false,"options":{"pg_num_max":1},"application_metadata":{"rados":{}},"read_balance":{"score_type":"Fair distribution","score_acting":15,"score_stable":15,"optimal_score":0.20000000298023224,"raw_score_acting":3,"raw_score_stable":3,"primary_affinity_weighted":1,"average_primary_affinity":1,"average_primary_affinity_weighted":1}},{"pool_id":16,"pool_name":"ec_pool","create_time":"2025-04-15T12:47:49.386474+0000","flags":1,"flags_names":"hashpspool","type":3,"size":5,"min_size":4,"crush_rule":1,"peering_crush_bucket_count":0,"peering_crush_bucket_target":0,"peering_crush_bucket_barrier":0,"peering_crush_bucket_mandatory_member":2147483647,"is_stretch_pool":false,"object_hash":2,"pg_autoscale_mode":"on","pg_num":1,"pg_placement_num":1,"pg_placement_num_target":1,"pg_num_target":1,"pg_num_pending":1,"last_pg_merge_meta":{"source_pgid":"0.0","ready_epoch":0,"last_epoch_started":0,"last_epoch_clean":0,"source_version":"0'0","target_version":"0'0"},"last_change":"6010","last_force_op_resend":"0","last_force_op_resend_prenautilus":"0","last_force_op_resend_preluminous":"0","auid":0,"snap_mode":"selfmanaged","snap_seq":0,"snap_epoch":0,"pool_snaps":[],"removed_snaps":"[]","quota_max_bytes":0,"quota_max_objects":0,"tiers":[],"tier_of":-1,"read_tier":-1,"write_tier":-1,"cache_mode":"none","target_max_bytes":0,"target_max_objects":0,"cache_target_dirty_ratio_micro":400000,"cache_target_dirty_high_ratio_micro":600000,"cache_target_full_ratio_micro":800000,"cache_min_flush_age":0,"cache_min_evict_age":0,"erasure_code_profile":"ecp_ec_pool","hit_set_params":{"type":"none"},"hit_set_period":0,"hit_set_count":0,"use_gmt_hitset":true,"min_read_recency_for_promote":0,"min_write_recency_for_promote":0,"hit_set_grade_decay_rate":0,"hit_set_search_last_n":0,"grade_table":[],"stripe_width":12288,"expected_num_objects":0,"fast_read":false,"options":{"pg_num_max":1},"application
2025-04-15 12:56:00,626 - cephci - ceph:1186 - DEBUG - _metadata":{"rados":{}}}]
2025-04-15 12:56:00,628 - cephci - ceph:1576 - INFO - Execute ceph osd pool delete replicated_pool replicated_pool --yes-i-really-really-mean-it on 10.0.195.240
2025-04-15 12:56:01,632 - cephci - ceph:1606 - INFO - Execution of ceph osd pool delete replicated_pool replicated_pool --yes-i-really-really-mean-it on 10.0.195.240 took 1.003612 seconds
2025-04-15 12:56:03,635 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:56:04,640 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.003831 seconds
2025-04-15 12:56:04,640 - cephci - core_workflows:1199 - INFO - Pool:replicated_pool deleted Successfully
2025-04-15 12:56:04,640 - cephci - core_workflows:4548 - INFO - Pool replicated_pool deleted successfully
2025-04-15 12:56:04,642 - cephci - ceph:1576 - INFO - Execute ceph config set mon mon_allow_pool_delete true on 10.0.195.240
2025-04-15 12:56:05,645 - cephci - ceph:1606 - INFO - Execution of ceph config set mon mon_allow_pool_delete true on 10.0.195.240 took 1.003457 seconds
2025-04-15 12:56:05,647 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:56:06,651 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.00358 seconds
2025-04-15 12:56:06,652 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79
2025-04-15 12:56:09,203 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79 took 2.550079 seconds
2025-04-15 12:56:09,205 - cephci - ceph:1576 - INFO - Execute ceph osd pool delete ec_pool ec_pool --yes-i-really-really-mean-it on 10.0.195.240
2025-04-15 12:56:11,210 - cephci - ceph:1606 - INFO - Execution of ceph osd pool delete ec_pool ec_pool --yes-i-really-really-mean-it on 10.0.195.240 took 2.004705 seconds
2025-04-15 12:56:13,213 - cephci - core_workflows:1177 - INFO - deleting the profile created for the EC pool
2025-04-15 12:56:13,215 - cephci - ceph:1576 - INFO - Execute ceph osd erasure-code-profile ls -f json on 10.0.195.240
2025-04-15 12:56:14,219 - cephci - ceph:1606 - INFO - Execution of ceph osd erasure-code-profile ls -f json on 10.0.195.240 took 1.004229 seconds
2025-04-15 12:56:14,221 - cephci - ceph:1576 - INFO - Execute ceph osd erasure-code-profile rm ecp_ec_pool -f json on 10.0.195.240
2025-04-15 12:56:15,225 - cephci - ceph:1606 - INFO - Execution of ceph osd erasure-code-profile rm ecp_ec_pool -f json on 10.0.195.240 took 1.00387 seconds
2025-04-15 12:56:15,227 - cephci - ceph:1576 - INFO - Execute ceph osd erasure-code-profile ls -f json on 10.0.195.240
2025-04-15 12:56:16,231 - cephci - ceph:1606 - INFO - Execution of ceph osd erasure-code-profile ls -f json on 10.0.195.240 took 1.003516 seconds
2025-04-15 12:56:16,231 - cephci - core_workflows:1412 - INFO - Profile : ecp_ec_pool  deleted
2025-04-15 12:56:16,231 - cephci - core_workflows:1186 - INFO - Deleting the crush rule used for the EC pool
2025-04-15 12:56:16,232 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd crush rule ls -f json on 10.0.195.79
2025-04-15 12:56:17,766 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd crush rule ls -f json on 10.0.195.79 took 1.533437 seconds
2025-04-15 12:56:17,767 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:56:17,767 - cephci - ceph:1186 - DEBUG - ["replicated_rule","ec_pool"]
2025-04-15 12:56:17,769 - cephci - ceph:1576 - INFO - Execute ceph osd crush rule rm ec_pool on 10.0.195.240
2025-04-15 12:56:19,774 - cephci - ceph:1606 - INFO - Execution of ceph osd crush rule rm ec_pool on 10.0.195.240 took 2.004819 seconds
2025-04-15 12:56:19,776 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:56:20,780 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.003965 seconds
2025-04-15 12:56:20,781 - cephci - core_workflows:1199 - INFO - Pool:ec_pool deleted Successfully
2025-04-15 12:56:20,781 - cephci - core_workflows:4548 - INFO - Pool ec_pool deleted successfully
2025-04-15 12:56:20,781 - cephci - test_crc_check_logs:140 - INFO - Removing the test files from tmp directory
2025-04-15 12:56:20,783 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/ZeroKb_file.txt on 10.0.195.240
2025-04-15 12:56:21,787 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/ZeroKb_file.txt on 10.0.195.240 took 1.004116 seconds
2025-04-15 12:56:21,789 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/zero_output.txt on 10.0.195.240
2025-04-15 12:56:22,793 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/zero_output.txt on 10.0.195.240 took 1.003786 seconds
2025-04-15 12:56:22,795 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/100Kb_file.txt on 10.0.195.240
2025-04-15 12:56:23,799 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/100Kb_file.txt on 10.0.195.240 took 1.003772 seconds
2025-04-15 12:56:23,801 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/100Kb_output.txt on 10.0.195.240
2025-04-15 12:56:24,805 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/100Kb_output.txt on 10.0.195.240 took 1.003571 seconds
2025-04-15 12:56:24,807 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/120MB_file.txt on 10.0.195.240
2025-04-15 12:56:25,812 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/120MB_file.txt on 10.0.195.240 took 1.003946 seconds
2025-04-15 12:56:25,813 - cephci - ceph:1576 - INFO - Execute rm -f /tmp/100Kb_output.txt on 10.0.195.240
2025-04-15 12:56:26,817 - cephci - ceph:1606 - INFO - Execution of rm -f /tmp/100Kb_output.txt on 10.0.195.240 took 1.003613 seconds
2025-04-15 12:56:36,828 - cephci - core_workflows:4558 - DEBUG - Printing cluster health and status
2025-04-15 12:56:36,829 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 12:56:38,570 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 1.739811 seconds
2025-04-15 12:56:38,570 - cephci - ceph:1186 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
2025-04-15 12:56:38,570 - cephci - ceph:1186 - DEBUG - [WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
2025-04-15 12:56:38,570 - cephci - ceph:1186 - DEBUG -      osd.6 observed slow operation indications in BlueStore
2025-04-15 12:56:38,570 - cephci - ceph:1186 - DEBUG - [WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
2025-04-15 12:56:38,571 - cephci - ceph:1186 - DEBUG -     daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
2025-04-15 12:56:38,571 - cephci - shell:64 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state

2025-04-15 12:56:38,571 - cephci - core_workflows:4560 - INFO - 
****
 Cluster health detail: 
 HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
 
****
2025-04-15 12:56:38,573 - cephci - ceph:1576 - INFO - Execute ceph -s on 10.0.195.240
2025-04-15 12:56:39,577 - cephci - ceph:1606 - INFO - Execution of ceph -s on 10.0.195.240 took 1.004128 seconds
2025-04-15 12:56:39,578 - cephci - core_workflows:4561 - INFO - 
****
 Cluster status: 
   cluster:
    id:     79ad3a88-19c4-11f0-bea5-fa163ea10f51
    health: HEALTH_WARN
            1 OSD(s) experiencing slow operations in BlueStore
            1 failed cephadm daemon(s)
 
  services:
    mon: 3 daemons, quorum ceph-regression-juxejq-zv217a-node1-installer,ceph-regression-juxejq-zv217a-node2,ceph-regression-juxejq-zv217a-node6 (age 6h)
    mgr: ceph-regression-juxejq-zv217a-node1-installer.odtafx(active, since 6h), standbys: ceph-regression-juxejq-zv217a-node2.oxadyy, ceph-regression-juxejq-zv217a-node6.agdlmk
    mds: 1/1 daemons up, 1 standby
    osd: 15 osds: 14 up (since 105m), 14 in (since 3h)
    rgw: 2 daemons active (2 hosts, 1 zones)
 
  data:
    volumes: 1/1 healthy
    pools:   8 pools, 689 pgs
    objects: 219 objects, 456 KiB
    usage:   13 GiB used, 337 GiB / 350 GiB avail
    pgs:     689 active+clean
 
 
****
2025-04-15 12:56:39,580 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph crash ls-new -f json on 10.0.195.79
2025-04-15 12:56:42,075 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph crash ls-new -f json on 10.0.195.79 took 2.494403 seconds
2025-04-15 12:56:42,076 - cephci - ceph:1576 - INFO - Execute podman --version | awk {'print $3'} on 10.0.195.79
2025-04-15 12:56:43,080 - cephci - ceph:1606 - INFO - Execution of podman --version | awk {'print $3'} on 10.0.195.79 took 1.00315 seconds
2025-04-15 12:56:43,080 - cephci - run:1065 - INFO - Podman Version 5.2.2
2025-04-15 12:56:43,081 - cephci - ceph:1576 - INFO - Execute docker --version | awk {'print $3'} on 10.0.195.79
2025-04-15 12:56:44,085 - cephci - ceph:1606 - INFO - Execution of docker --version | awk {'print $3'} on 10.0.195.79 took 1.003847 seconds
2025-04-15 12:56:44,087 - cephci - ceph:1576 - INFO - Execute ceph --version | awk '{print $3}' on 10.0.195.240
2025-04-15 12:56:45,091 - cephci - ceph:1606 - INFO - Execution of ceph --version | awk '{print $3}' on 10.0.195.240 took 1.003091 seconds
2025-04-15 12:56:45,091 - cephci - run:1081 - INFO - ceph Version 19.2.1-126.el9cp
2025-04-15 12:56:45,092 - cephci - run:1040 - INFO - ceph_clusters_file rerun/regression-JUxEjq-ZV217A
2025-04-15 12:56:45,092 - cephci - run:927 - INFO - Test <module 'test_crc_check_logs' from '/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/tests/rados/test_crc_check_logs.py'> failed
