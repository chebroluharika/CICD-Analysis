2025-04-15 09:19:48,710 - cephci - log:161 - DEBUG - Completed log configuration
2025-04-15 09:19:48,713 - cephci - run:722 - INFO - Running test test_osd_replicated_inconsistency_scenario.py
2025-04-15 09:19:48,713 - cephci - test_osd_replicated_inconsistency_scenario:51 - INFO - 
    Test to create an inconsistent object functionality during scrub/deep-scrub in replicated pool.
    Returns:
        1 -> Fail, 0 -> Pass
    
2025-04-15 09:19:48,714 - cephci - core_workflows:563 - DEBUG - creating pool_name replicated_pool
2025-04-15 09:19:48,715 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool create replicated_pool 1 1 on 10.0.195.79
2025-04-15 09:19:51,237 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool create replicated_pool 1 1 on 10.0.195.79 took 2.522348 seconds
2025-04-15 09:19:51,239 - cephci - shell:64 - DEBUG - 
2025-04-15 09:19:51,280 - cephci - ceph:1576 - INFO - Execute cephadm shell -- sudo ceph osd pool application enable replicated_pool rados on 10.0.195.79
2025-04-15 09:19:54,125 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- sudo ceph osd pool application enable replicated_pool rados on 10.0.195.79 took 2.844394 seconds
2025-04-15 09:19:54,125 - cephci - shell:64 - DEBUG - 
2025-04-15 09:19:54,127 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool set replicated_pool pg_autoscale_mode off on 10.0.195.79
2025-04-15 09:19:57,161 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool set replicated_pool pg_autoscale_mode off on 10.0.195.79 took 3.033763 seconds
2025-04-15 09:19:57,161 - cephci - shell:64 - DEBUG - 
2025-04-15 09:20:02,162 - cephci - core_workflows:620 - INFO - Created pool replicated_pool successfully
2025-04-15 09:20:02,165 - cephci - ceph:1576 - INFO - Execute ceph osd pool stats replicated_pool -f json on 10.0.195.240
2025-04-15 09:20:03,169 - cephci - ceph:1606 - INFO - Execution of ceph osd pool stats replicated_pool -f json on 10.0.195.240 took 1.003613 seconds
2025-04-15 09:20:03,170 - cephci - pool_workflows:123 - DEBUG - Writing 5000 Key pairs to increase the omap entries on pool replicated_pool
2025-04-15 09:20:03,171 - cephci - ceph:1576 - INFO - Execute curl -k https://raw.githubusercontent.com/red-hat-storage/cephci/refs/heads/main/utility/generate_omap_entries.py -O on 10.0.195.240
2025-04-15 09:20:04,175 - cephci - ceph:1606 - INFO - Execution of curl -k https://raw.githubusercontent.com/red-hat-storage/cephci/refs/heads/main/utility/generate_omap_entries.py -O on 10.0.195.240 took 1.003382 seconds
2025-04-15 09:20:04,177 - cephci - ceph:1576 - INFO - Execute pip3 install docopt on 10.0.195.240
2025-04-15 09:20:30,209 - cephci - ceph:1186 - DEBUG - Collecting docopt
2025-04-15 09:20:42,103 - cephci - ceph:1186 - DEBUG -   Downloading docopt-0.6.2.tar.gz (25 kB)
2025-04-15 09:20:42,123 - cephci - ceph:1186 - DEBUG -   Preparing metadata (setup.py): started
2025-04-15 09:20:42,310 - cephci - ceph:1186 - DEBUG -   Preparing metadata (setup.py): finished with status 'done'
2025-04-15 09:20:42,314 - cephci - ceph:1186 - DEBUG - Using legacy 'setup.py install' for docopt, since package 'wheel' is not installed.
2025-04-15 09:20:42,377 - cephci - ceph:1186 - DEBUG - Installing collected packages: docopt
2025-04-15 09:20:42,377 - cephci - ceph:1186 - DEBUG -     Running setup.py install for docopt: started
2025-04-15 09:20:42,604 - cephci - ceph:1186 - DEBUG -     Running setup.py install for docopt: finished with status 'done'
2025-04-15 09:20:42,607 - cephci - ceph:1186 - DEBUG - Successfully installed docopt-0.6.2
2025-04-15 09:20:42,680 - cephci - ceph:1186 - ERROR - WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
2025-04-15 09:20:42,681 - cephci - ceph:1606 - INFO - Execution of pip3 install docopt on 10.0.195.240 took 38.503469 seconds
2025-04-15 09:20:42,681 - cephci - ceph:1576 - INFO - Execute python3 generate_omap_entries.py --pool replicated_pool --start 0 --end 50 --key-count 100 on 10.0.195.240
2025-04-15 09:20:43,684 - cephci - ceph:1186 - DEBUG - wrote 100 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 200 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 300 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 400 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 500 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 600 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 700 of 5000 omap entries
2025-04-15 09:20:43,685 - cephci - ceph:1186 - DEBUG - wrote 800 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 900 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1000 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1100 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1200 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1300 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1400 of 5000 omap entries
2025-04-15 09:20:43,686 - cephci - ceph:1186 - DEBUG - wrote 1500 of 5000 omap entries
2025-04-15 09:20:43,687 - cephci - ceph:1186 - DEBUG - wrote 1600 of 5000 omap entries
2025-04-15 09:20:43,694 - cephci - ceph:1186 - DEBUG - wrote 1700 of 5000 omap entries
2025-04-15 09:20:43,927 - cephci - ceph:1186 - DEBUG - wrote 1800 of 5000 omap entries
2025-04-15 09:20:43,945 - cephci - ceph:1186 - DEBUG - wrote 1900 of 5000 omap entries
2025-04-15 09:20:43,954 - cephci - ceph:1186 - DEBUG - wrote 2000 of 5000 omap entries
2025-04-15 09:20:44,187 - cephci - ceph:1186 - DEBUG - wrote 2100 of 5000 omap entries
2025-04-15 09:20:44,196 - cephci - ceph:1186 - DEBUG - wrote 2200 of 5000 omap entries
2025-04-15 09:20:44,208 - cephci - ceph:1186 - DEBUG - wrote 2300 of 5000 omap entries
2025-04-15 09:20:44,218 - cephci - ceph:1186 - DEBUG - wrote 2400 of 5000 omap entries
2025-04-15 09:20:44,227 - cephci - ceph:1186 - DEBUG - wrote 2500 of 5000 omap entries
2025-04-15 09:20:44,235 - cephci - ceph:1186 - DEBUG - wrote 2600 of 5000 omap entries
2025-04-15 09:20:44,459 - cephci - ceph:1186 - DEBUG - wrote 2700 of 5000 omap entries
2025-04-15 09:20:44,491 - cephci - ceph:1186 - DEBUG - wrote 2800 of 5000 omap entries
2025-04-15 09:20:44,506 - cephci - ceph:1186 - DEBUG - wrote 2900 of 5000 omap entries
2025-04-15 09:20:44,522 - cephci - ceph:1186 - DEBUG - wrote 3000 of 5000 omap entries
2025-04-15 09:20:44,544 - cephci - ceph:1186 - DEBUG - wrote 3100 of 5000 omap entries
2025-04-15 09:20:44,555 - cephci - ceph:1186 - DEBUG - wrote 3200 of 5000 omap entries
2025-04-15 09:20:44,564 - cephci - ceph:1186 - DEBUG - wrote 3300 of 5000 omap entries
2025-04-15 09:20:44,573 - cephci - ceph:1186 - DEBUG - wrote 3400 of 5000 omap entries
2025-04-15 09:20:44,582 - cephci - ceph:1186 - DEBUG - wrote 3500 of 5000 omap entries
2025-04-15 09:20:44,591 - cephci - ceph:1186 - DEBUG - wrote 3600 of 5000 omap entries
2025-04-15 09:20:44,601 - cephci - ceph:1186 - DEBUG - wrote 3700 of 5000 omap entries
2025-04-15 09:20:44,610 - cephci - ceph:1186 - DEBUG - wrote 3800 of 5000 omap entries
2025-04-15 09:20:44,619 - cephci - ceph:1186 - DEBUG - wrote 3900 of 5000 omap entries
2025-04-15 09:20:44,629 - cephci - ceph:1186 - DEBUG - wrote 4000 of 5000 omap entries
2025-04-15 09:20:44,644 - cephci - ceph:1186 - DEBUG - wrote 4100 of 5000 omap entries
2025-04-15 09:20:44,654 - cephci - ceph:1186 - DEBUG - wrote 4200 of 5000 omap entries
2025-04-15 09:20:44,663 - cephci - ceph:1186 - DEBUG - wrote 4300 of 5000 omap entries
2025-04-15 09:20:44,671 - cephci - ceph:1186 - DEBUG - wrote 4400 of 5000 omap entries
2025-04-15 09:20:44,680 - cephci - ceph:1186 - DEBUG - wrote 4500 of 5000 omap entries
2025-04-15 09:20:44,689 - cephci - ceph:1186 - DEBUG - wrote 4600 of 5000 omap entries
2025-04-15 09:20:44,699 - cephci - ceph:1186 - DEBUG - wrote 4700 of 5000 omap entries
2025-04-15 09:20:44,748 - cephci - ceph:1186 - DEBUG - wrote 4800 of 5000 omap entries
2025-04-15 09:20:44,758 - cephci - ceph:1186 - DEBUG - wrote 4900 of 5000 omap entries
2025-04-15 09:20:44,768 - cephci - ceph:1186 - DEBUG - wrote 5000 of 5000 omap entries
2025-04-15 09:20:44,770 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:20:44,770 - cephci - ceph:1186 - DEBUG - Done!
2025-04-15 09:20:44,778 - cephci - ceph:1606 - INFO - Execution of python3 generate_omap_entries.py --pool replicated_pool --start 0 --end 50 --key-count 100 on 10.0.195.240 took 2.096288 seconds
2025-04-15 09:20:44,820 - cephci - ceph:1576 - INFO - Execute rm -rf generate_omap_entries.py on 10.0.195.240
2025-04-15 09:20:45,824 - cephci - ceph:1606 - INFO - Execution of rm -rf generate_omap_entries.py on 10.0.195.240 took 1.003496 seconds
2025-04-15 09:20:45,826 - cephci - ceph:1576 - INFO - Execute ceph osd pool deep-scrub replicated_pool on 10.0.195.240
2025-04-15 09:20:46,831 - cephci - ceph:1606 - INFO - Execution of ceph osd pool deep-scrub replicated_pool on 10.0.195.240 took 1.003875 seconds
2025-04-15 09:20:46,831 - cephci - pool_workflows:151 - DEBUG - Triggered deep-scrub on the pool, and checking for omap entries present
2025-04-15 09:21:26,849 - cephci - ceph:1576 - INFO - Execute ceph pg dump_pools_json on 10.0.195.240
2025-04-15 09:21:27,853 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_pools_json on 10.0.195.240 took 1.003748 seconds
2025-04-15 09:21:27,854 - cephci - pool_workflows:101 - INFO - Current value of OMAP keys: 5000
2025-04-15 09:21:27,854 - cephci - pool_workflows:156 - INFO - OMAP entries are available
2025-04-15 09:21:27,855 - cephci - ceph:1576 - INFO - Execute ceph pg dump_pools_json on 10.0.195.240
2025-04-15 09:21:28,859 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_pools_json on 10.0.195.240 took 1.00344 seconds
2025-04-15 09:21:28,860 - cephci - pool_workflows:163 - INFO - Wrote 5000 keys with 39500 bytes of OMAP data on the pool.
2025-04-15 09:21:28,862 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados -p replicated_pool ls -f json on 10.0.195.79
2025-04-15 09:21:31,051 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados -p replicated_pool ls -f json on 10.0.195.79 took 2.189578 seconds
2025-04-15 09:21:31,053 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_18 -f json on 10.0.195.79
2025-04-15 09:21:32,568 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_18 -f json on 10.0.195.79 took 1.515323 seconds
2025-04-15 09:21:32,569 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:21:32,569 - cephci - ceph:1186 - DEBUG - {"epoch":1492,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_18","raw_pgid":"14.a753eec0","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 09:21:32,570 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 09:21:32,570 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_18 is created in the pg-14.0
2025-04-15 09:21:32,610 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:21:34,151 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.539738 seconds
2025-04-15 09:21:34,151 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:21:34,151 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:21:34,192 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:21:35,849 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.65614 seconds
2025-04-15 09:21:35,850 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:21:35,850 - cephci - ceph:1186 - DEBUG - [{"container_id": "e41be70dd298", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.80%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:20:24.576948Z", "memory_request": 4294967296, "memory_usage": 612200000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T07:20:08.405138Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:21:35,850 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:21:35,851 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:21:36,855 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.00346 seconds
2025-04-15 09:21:36,857 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:21:38,380 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.522557 seconds
2025-04-15 09:21:38,381 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:21:38,381 - cephci - ceph:1186 - DEBUG - [{"container_id": "e41be70dd298", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.80%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:20:24.576948Z", "memory_request": 4294967296, "memory_usage": 612200000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T07:20:08.405138Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:21:38,381 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'e41be70dd298', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.80%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:20:24.576948Z', 'memory_request': 4294967296, 'memory_usage': 612200000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T07:20:08.405138Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:21:38,383 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:21:39,387 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003636 seconds
2025-04-15 09:21:44,390 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:21:44,392 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:21:49,400 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 5.007977 seconds
2025-04-15 09:21:54,405 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:21:55,410 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 09:21:55,412 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.006432 seconds
2025-04-15 09:21:55,414 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:21:57,023 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.607946 seconds
2025-04-15 09:21:57,023 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:21:57,023 - cephci - ceph:1186 - DEBUG - [{"container_id": "e41be70dd298", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.80%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:21:20.387565Z", "memory_request": 4294967296, "memory_usage": 611800000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T07:20:08.405138Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:21:57,024 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'e41be70dd298', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.80%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:21:20.387565Z', 'memory_request': 4294967296, 'memory_usage': 611800000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T07:20:08.405138Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:21:57,024 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:22:17,046 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:22:18,642 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.595889 seconds
2025-04-15 09:22:18,643 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:22:18,643 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:21:38.680908Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:22:18,643 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:21:38.680908Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:22:18,644 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:22:18,644 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 09:22:23,650 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:22:25,331 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.679933 seconds
2025-04-15 09:22:25,331 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:22:25,332 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:22:25,333 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191
2025-04-15 09:26:46,086 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191 took 260.752456 seconds
2025-04-15 09:26:46,087 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 09:26:46,087 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:26:46,087 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:26:46,087 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:26:46,088 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:26:46,089 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:26:46,090 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:26:46,091 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:26:46,092 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:26:46,093 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:26:46,094 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:26:46,095 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:26:46,096 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:26:46,097 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:26:46,098 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:26:46,098 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:26:46,098 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:26:46,098 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:26:46,098 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_18 object
2025-04-15 09:26:46,100 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:26:47,730 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.629665 seconds
2025-04-15 09:26:47,731 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:26:47,731 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:26:47,732 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  rm-omap key_0 on 10.0.195.191
2025-04-15 09:27:00,688 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  rm-omap key_0 on 10.0.195.191 took 12.955551 seconds
2025-04-15 09:27:00,689 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_18 object
2025-04-15 09:27:05,696 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:27:07,221 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.524602 seconds
2025-04-15 09:27:07,221 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:27:07,222 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:27:07,223 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191
2025-04-15 09:27:23,480 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191 took 16.256159 seconds
2025-04-15 09:27:23,480 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_18Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 09:27:23,480 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 09:27:23,482 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:27:24,921 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.438534 seconds
2025-04-15 09:27:24,922 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:27:24,922 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:27:24,924 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191
2025-04-15 09:27:39,026 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_18  list-omap on 10.0.195.191 took 14.101813 seconds
2025-04-15 09:27:39,026 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:27:39,028 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:27:40,588 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.560142 seconds
2025-04-15 09:27:40,589 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:27:40,589 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:27:40,630 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:27:42,211 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.58029 seconds
2025-04-15 09:27:42,212 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:27:42,212 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:27:42,212 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:27:42,213 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:27:43,217 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.00378 seconds
2025-04-15 09:27:43,219 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:27:44,824 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.604193 seconds
2025-04-15 09:27:44,825 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:27:44,825 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:22:00.797403Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:27:44,825 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:22:00.797403Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:27:44,827 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:27:45,832 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.004669 seconds
2025-04-15 09:27:50,838 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:27:50,840 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:28:04,858 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 14.018126 seconds
2025-04-15 09:28:09,865 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:28:10,873 - cephci - ceph:1186 - DEBUG - active
2025-04-15 09:28:10,873 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005738 seconds
2025-04-15 09:28:10,875 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:28:13,503 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.627697 seconds
2025-04-15 09:28:13,504 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:27:27.356128Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:28:13,504 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:28:33,518 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:28:36,130 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.612198 seconds
2025-04-15 09:28:36,131 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '112be47552b4', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.13%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:27:54.205867Z', 'memory_request': 4294967296, 'memory_usage': 21020000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:27:43.121529Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:28:36,131 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:28:36,131 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 09:28:36,132 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 09:28:36,133 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:28:37,166 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032519 seconds
2025-04-15 09:28:37,190 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 09:28:37,191 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 09:28:37,191 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:20:29.846002+0000
2025-04-15 09:28:37,193 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 09:28:37,194 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 09:28:38,198 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003404 seconds
2025-04-15 09:28:40,202 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:28:41,236 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033361 seconds
2025-04-15 09:28:41,258 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:28:41,258 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:20:29.846002+0000
2025-04-15 09:28:41,258 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:28:41,258 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:29:11,290 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:29:12,325 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033841 seconds
2025-04-15 09:29:12,349 - cephci - core_workflows:3506 - INFO - Scrubbing complete on the PG: 14.0
2025-04-15 09:29:12,349 - cephci - core_workflows:3507 - DEBUG - Final last_deep_scrub: 1492'100
2025-04-15 09:29:12,349 - cephci - core_workflows:3508 - DEBUG - Final last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:29:12,349 - cephci - core_workflows:3511 - DEBUG - Total time taken for scrubbing to complete on the pg : 14.0 is 0:07:53.310780
2025-04-15 09:29:12,350 - cephci - core_workflows:3404 - DEBUG - Completed deep-scrubbing the pg : 14.0
2025-04-15 09:29:22,361 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 09:29:25,008 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 2.646201 seconds
2025-04-15 09:29:25,008 - cephci - shell:64 - DEBUG - HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 2 scrub errors; Possible data damage: 1 pg inconsistent
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
[ERR] OSD_SCRUB_ERRORS: 2 scrub errors
[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent
    pg 14.0 is active+clean+inconsistent, acting [13,8,3]

2025-04-15 09:29:25,008 - cephci - core_workflows:3317 - INFO - Health warning on cluster: 
 ('HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 2 scrub errors; Possible data damage: 1 pg inconsistent\n[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore\n     osd.6 observed slow operation indications in BlueStore\n[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)\n    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state\n[ERR] OSD_SCRUB_ERRORS: 2 scrub errors\n[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent\n    pg 14.0 is active+clean+inconsistent, acting [13,8,3]\n', "Inferring fsid 79ad3a88-19c4-11f0-bea5-fa163ea10f51\nInferring config /var/lib/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/mon.ceph-regression-juxejq-zv217a-node1-installer/config\nUsing ceph image with id '3b1483211cc8' and tag '8-110' created on 2025-04-11 13:52:17 +0000 UTC\ncp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6\n") 


2025-04-15 09:29:25,011 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 09:29:26,016 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.004688 seconds
2025-04-15 09:29:26,019 - cephci - core_workflows:3325 - INFO - pg inconsistent generated in the cluster
2025-04-15 09:29:26,019 - cephci - core_workflows:3408 - INFO - The inconsistent object is created in the pg: 14.0
2025-04-15 09:29:26,019 - cephci - test_osd_replicated_inconsistency_scenario:301 - INFO - The 1 objects are converted into inconsistent objects
2025-04-15 09:29:26,021 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_24 -f json on 10.0.195.79
2025-04-15 09:29:28,485 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_24 -f json on 10.0.195.79 took 2.464296 seconds
2025-04-15 09:29:28,486 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 09:29:28,486 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_24 is created in the pg-14.0
2025-04-15 09:29:28,487 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:29:30,120 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.632218 seconds
2025-04-15 09:29:30,121 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:29:30,121 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:29:30,163 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:29:31,724 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.560773 seconds
2025-04-15 09:29:31,725 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:29:31,725 - cephci - ceph:1186 - DEBUG - [{"container_id": "112be47552b4", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.90%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:28:17.247944Z", "memory_request": 4294967296, "memory_usage": 67390000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:27:43.121529Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:29:31,725 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:29:31,726 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:29:32,731 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.0037 seconds
2025-04-15 09:29:32,732 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:29:34,338 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.605627 seconds
2025-04-15 09:29:34,339 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:29:34,348 - cephci - ceph:1186 - DEBUG - [{"container_id": "112be47552b4", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.90%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:28:17.247944Z", "memory_request": 4294967296, "memory_usage": 67390000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:27:43.121529Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:29:34,349 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '112be47552b4', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.90%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:28:17.247944Z', 'memory_request': 4294967296, 'memory_usage': 67390000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:27:43.121529Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:29:34,350 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:29:35,354 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003445 seconds
2025-04-15 09:29:40,360 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:29:40,361 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:29:45,371 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 5.009265 seconds
2025-04-15 09:29:50,374 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:29:51,378 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 09:29:51,378 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004183 seconds
2025-04-15 09:29:51,380 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:29:54,010 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.629889 seconds
2025-04-15 09:29:54,011 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '112be47552b4', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.98%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:29:16.159661Z', 'memory_request': 4294967296, 'memory_usage': 76300000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:27:43.121529Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:29:54,011 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:30:14,033 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:30:15,615 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.581703 seconds
2025-04-15 09:30:15,616 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:30:15,616 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:35.139511Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:30:15,616 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:29:35.139511Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:30:15,617 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:30:15,617 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 09:30:20,623 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:30:23,222 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.59917 seconds
2025-04-15 09:30:23,224 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191
2025-04-15 09:30:45,296 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191 took 22.071146 seconds
2025-04-15 09:30:45,296 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:30:45,297 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:30:45,298 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:30:45,299 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:30:45,300 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:30:45,301 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:30:45,302 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:30:45,303 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:30:45,304 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:30:45,305 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:30:45,306 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:30:45,306 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_24 object
2025-04-15 09:30:45,307 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:30:47,010 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.702338 seconds
2025-04-15 09:30:47,011 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:30:47,011 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:58.271312Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:30:47,012 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  rm-omap key_0 on 10.0.195.191
2025-04-15 09:30:59,906 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  rm-omap key_0 on 10.0.195.191 took 12.893324 seconds
2025-04-15 09:30:59,906 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_24 object
2025-04-15 09:31:04,913 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:31:06,521 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.607547 seconds
2025-04-15 09:31:06,522 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:31:06,522 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:58.271312Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:31:06,524 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191
2025-04-15 09:31:19,053 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191 took 12.529359 seconds
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:31:19,054 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:31:19,055 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:31:19,056 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:31:19,057 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:31:19,058 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:31:19,059 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:31:19,060 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:31:19,061 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:31:19,062 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_24Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 09:31:19,062 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 09:31:19,064 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:31:20,710 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.64547 seconds
2025-04-15 09:31:20,711 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:31:20,711 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:58.271312Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:31:20,713 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191
2025-04-15 09:31:34,212 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_24  list-omap on 10.0.195.191 took 13.498541 seconds
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:31:34,213 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:31:34,214 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:31:34,215 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:31:34,216 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:31:34,217 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:31:34,218 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:31:34,219 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:31:34,220 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:31:34,221 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:31:34,221 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:31:34,222 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:31:36,836 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 2.613778 seconds
2025-04-15 09:31:36,841 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:31:38,456 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.614988 seconds
2025-04-15 09:31:38,457 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:31:38,457 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:58.271312Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:31:38,458 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:31:38,459 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:31:39,462 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.002678 seconds
2025-04-15 09:31:39,464 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:31:41,109 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.644661 seconds
2025-04-15 09:31:41,110 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:31:41,110 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:29:58.271312Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:31:41,111 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:29:58.271312Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:31:41,112 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:31:42,116 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003577 seconds
2025-04-15 09:31:47,122 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:31:47,124 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:32:01,143 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 14.018381 seconds
2025-04-15 09:32:06,150 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:32:07,154 - cephci - ceph:1186 - DEBUG - active
2025-04-15 09:32:07,155 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004304 seconds
2025-04-15 09:32:07,157 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:32:08,921 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.763144 seconds
2025-04-15 09:32:08,921 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:32:08,922 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:31:23.500744Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:32:08,922 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:31:23.500744Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:32:08,922 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:32:28,944 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:32:30,526 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.581242 seconds
2025-04-15 09:32:30,527 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:32:30,527 - cephci - ceph:1186 - DEBUG - [{"container_id": "2c97511ba399", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.19%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:31:51.024481Z", "memory_request": 4294967296, "memory_usage": 21120000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:31:39.474009Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:32:30,527 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '2c97511ba399', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.19%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:31:51.024481Z', 'memory_request': 4294967296, 'memory_usage': 21120000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:31:39.474009Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:32:30,528 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:32:30,528 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 09:32:30,528 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 09:32:30,529 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:32:31,566 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036502 seconds
2025-04-15 09:32:31,592 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 09:32:31,592 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 09:32:31,592 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:32:31,593 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 09:32:31,594 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 09:32:32,598 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.00391 seconds
2025-04-15 09:32:34,603 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:32:35,638 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035062 seconds
2025-04-15 09:32:35,661 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:32:35,662 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:32:35,662 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:32:35,662 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+undersized+degraded. Sleeping for 30 secs
2025-04-15 09:33:05,685 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:33:06,718 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033071 seconds
2025-04-15 09:33:06,740 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:33:06,741 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:33:06,741 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:33:06,741 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:33:36,752 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:33:37,785 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032495 seconds
2025-04-15 09:33:37,813 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:33:37,813 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:33:37,813 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:33:37,813 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:34:07,845 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:34:08,878 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032525 seconds
2025-04-15 09:34:08,902 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:34:08,903 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:34:08,903 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:34:08,903 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:34:38,935 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:34:39,966 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031393 seconds
2025-04-15 09:34:39,993 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:34:39,994 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:34:39,994 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:34:39,994 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:35:10,020 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:35:11,051 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031133 seconds
2025-04-15 09:35:11,074 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:35:11,074 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:35:11,074 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:35:11,075 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:35:41,084 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:35:42,115 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030134 seconds
2025-04-15 09:35:42,137 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:35:42,138 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:35:42,138 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:35:42,138 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:36:12,150 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:36:13,181 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030805 seconds
2025-04-15 09:36:13,204 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:36:13,204 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:36:13,204 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:36:13,205 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:36:43,219 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:36:44,257 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036681 seconds
2025-04-15 09:36:44,368 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:36:44,369 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:36:44,370 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:36:44,370 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:37:14,391 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:37:15,425 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033686 seconds
2025-04-15 09:37:15,455 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:37:15,455 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:37:15,455 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:37:15,456 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:37:45,487 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:37:46,519 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031709 seconds
2025-04-15 09:37:46,544 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:37:46,545 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:37:46,545 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:37:46,545 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:38:16,569 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:38:17,604 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034026 seconds
2025-04-15 09:38:17,626 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:38:17,627 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:38:17,627 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:38:17,627 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:38:47,657 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:38:48,690 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032895 seconds
2025-04-15 09:38:48,713 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:38:48,713 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:38:48,714 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:38:48,714 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:39:18,745 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:39:19,778 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032382 seconds
2025-04-15 09:39:19,804 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:39:19,805 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:39:19,805 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:39:19,805 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:39:49,834 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:39:50,866 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031291 seconds
2025-04-15 09:39:50,889 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:39:50,890 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:39:50,890 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:39:50,890 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:40:20,905 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:40:21,939 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033261 seconds
2025-04-15 09:40:21,962 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:40:21,963 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:40:21,964 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:40:21,964 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:40:51,974 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:40:53,006 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031273 seconds
2025-04-15 09:40:53,029 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:40:53,029 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:40:53,029 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:40:53,029 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:41:23,043 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:41:24,076 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032279 seconds
2025-04-15 09:41:24,099 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:41:24,099 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:41:24,100 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:41:24,100 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:41:54,119 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:41:55,151 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031994 seconds
2025-04-15 09:41:55,173 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:41:55,174 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:41:55,174 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:41:55,174 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:42:25,181 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:42:26,216 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034223 seconds
2025-04-15 09:42:26,240 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:42:26,240 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:42:26,240 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:42:26,240 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:42:56,272 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:42:57,304 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031293 seconds
2025-04-15 09:42:57,326 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:42:57,327 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:42:57,328 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:42:57,328 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:43:27,346 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:43:28,381 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03472 seconds
2025-04-15 09:43:28,418 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:43:28,418 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:43:28,418 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:43:28,419 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:43:58,427 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:43:59,461 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033535 seconds
2025-04-15 09:43:59,485 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:43:59,485 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:43:59,485 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:43:59,485 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:44:29,488 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:44:30,521 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033151 seconds
2025-04-15 09:44:30,544 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:44:30,545 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:44:30,545 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:44:30,545 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:45:00,559 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:45:01,592 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031987 seconds
2025-04-15 09:45:01,618 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:45:01,619 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:45:01,619 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:45:01,619 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:45:31,631 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:45:32,664 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032949 seconds
2025-04-15 09:45:32,767 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:45:32,768 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:45:32,768 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:45:32,768 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:46:02,800 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:46:03,835 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034433 seconds
2025-04-15 09:46:03,859 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:46:03,859 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:46:03,859 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:46:03,859 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:46:33,875 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:46:34,911 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034984 seconds
2025-04-15 09:46:34,934 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:46:34,935 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:46:34,935 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:46:34,935 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:47:04,961 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:47:05,997 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035348 seconds
2025-04-15 09:47:06,021 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:47:06,022 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:47:06,022 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:47:06,022 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:47:36,052 - cephci - core_workflows:3530 - ERROR - PG : 14.0 could not be deep-scrubbed in time
2025-04-15 09:47:36,053 - cephci - test_osd_replicated_inconsistency_scenario:294 - INFO - Cannot able to convert the object-omap_obj_15671_24 into inconsistent object.Picking another object to convert-Objects not scrubbed error.Currently the 1 objects converted into inconsistent objects
2025-04-15 09:47:36,054 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_19 -f json on 10.0.195.79
2025-04-15 09:47:38,645 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_19 -f json on 10.0.195.79 took 2.590685 seconds
2025-04-15 09:47:38,646 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 09:47:38,646 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_19 is created in the pg-14.0
2025-04-15 09:47:38,647 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:47:40,312 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.664721 seconds
2025-04-15 09:47:40,312 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:47:40,312 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:47:40,353 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:47:41,880 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.526261 seconds
2025-04-15 09:47:41,880 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:47:41,880 - cephci - ceph:1186 - DEBUG - [{"container_id": "2c97511ba399", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.01%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:42:58.536092Z", "memory_request": 4294967296, "memory_usage": 78010000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:31:39.474009Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:47:41,881 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:47:41,882 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:47:42,887 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003856 seconds
2025-04-15 09:47:42,888 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:47:44,497 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.608871 seconds
2025-04-15 09:47:44,498 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:47:44,498 - cephci - ceph:1186 - DEBUG - [{"container_id": "2c97511ba399", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.01%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:42:58.536092Z", "memory_request": 4294967296, "memory_usage": 78010000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:31:39.474009Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:47:44,499 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '2c97511ba399', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.01%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:42:58.536092Z', 'memory_request': 4294967296, 'memory_usage': 78010000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:31:39.474009Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:47:44,500 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:47:45,504 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003239 seconds
2025-04-15 09:47:50,509 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:47:50,511 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:47:54,519 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 4.007138 seconds
2025-04-15 09:47:59,525 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:48:00,530 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 09:48:00,531 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005359 seconds
2025-04-15 09:48:00,532 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:48:02,117 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.584017 seconds
2025-04-15 09:48:02,118 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:48:02,118 - cephci - ceph:1186 - DEBUG - [{"container_id": "2c97511ba399", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.00%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:47:26.670707Z", "memory_request": 4294967296, "memory_usage": 78900000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:31:39.474009Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:48:02,118 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '2c97511ba399', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.00%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:47:26.670707Z', 'memory_request': 4294967296, 'memory_usage': 78900000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:31:39.474009Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:48:02,119 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:48:22,121 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:48:23,675 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.553684 seconds
2025-04-15 09:48:23,676 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:48:23,676 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:47:44.023346Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:48:23,676 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:47:44.023346Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:48:23,677 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:48:23,677 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 09:48:28,683 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:48:30,214 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.531357 seconds
2025-04-15 09:48:30,215 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:48:30,215 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:48:05.977019Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:48:30,217 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191
2025-04-15 09:48:53,409 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191 took 23.192302 seconds
2025-04-15 09:48:53,410 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 09:48:53,410 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:48:53,411 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:48:53,412 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:48:53,413 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:48:53,414 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:48:53,415 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:48:53,416 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:48:53,417 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:48:53,418 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:48:53,419 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:48:53,419 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:48:53,420 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_19 object
2025-04-15 09:48:53,422 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:48:55,938 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.515907 seconds
2025-04-15 09:48:55,939 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  rm-omap key_0 on 10.0.195.191
2025-04-15 09:49:10,567 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  rm-omap key_0 on 10.0.195.191 took 14.627222 seconds
2025-04-15 09:49:10,568 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_19 object
2025-04-15 09:49:15,574 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:49:18,107 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.532568 seconds
2025-04-15 09:49:18,109 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191
2025-04-15 09:49:35,908 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191 took 17.797754 seconds
2025-04-15 09:49:35,908 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:49:35,909 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:49:35,910 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:49:35,911 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:49:35,912 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:49:35,913 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:49:35,914 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:49:35,915 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:49:35,916 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:49:35,916 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_19Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 09:49:35,916 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 09:49:35,917 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:49:37,520 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.602033 seconds
2025-04-15 09:49:37,520 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:49:37,521 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:48:05.977019Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:49:37,522 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191
2025-04-15 09:49:55,515 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_19  list-omap on 10.0.195.191 took 17.992694 seconds
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:49:55,516 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:49:55,517 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:49:55,518 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:49:55,519 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:49:55,520 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:49:55,521 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:49:55,522 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:49:55,523 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:49:55,524 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:49:55,525 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:49:55,525 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:49:55,527 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:49:57,135 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.607436 seconds
2025-04-15 09:49:57,135 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:49:57,136 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:49:57,177 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:49:58,661 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.483095 seconds
2025-04-15 09:49:58,661 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:49:58,662 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:48:05.977019Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:49:58,662 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:49:58,663 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:49:59,668 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.004012 seconds
2025-04-15 09:49:59,670 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:50:01,274 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.60384 seconds
2025-04-15 09:50:01,275 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:50:01,275 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:48:05.977019Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:50:01,275 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:48:05.977019Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:50:01,276 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:50:02,280 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003459 seconds
2025-04-15 09:50:07,285 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:50:07,287 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:50:22,305 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 15.017603 seconds
2025-04-15 09:50:27,311 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:50:28,317 - cephci - ceph:1186 - DEBUG - active
2025-04-15 09:50:28,318 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005829 seconds
2025-04-15 09:50:28,325 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:50:29,885 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.558912 seconds
2025-04-15 09:50:29,885 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:50:29,885 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:49:43.190493Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:50:29,886 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:49:43.190493Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:50:29,886 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:50:49,904 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:50:52,425 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.521223 seconds
2025-04-15 09:50:52,426 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '5e5ed6dcaa55', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.31%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:50:12.527365Z', 'memory_request': 4294967296, 'memory_usage': 21070000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:49:59.174288Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:50:52,427 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:50:52,427 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 09:50:52,427 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 09:50:52,428 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:50:53,461 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032639 seconds
2025-04-15 09:50:53,483 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 09:50:53,483 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 09:50:53,483 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:50:53,483 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 09:50:53,485 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 09:50:54,488 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.00323 seconds
2025-04-15 09:50:56,492 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:50:57,527 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034282 seconds
2025-04-15 09:50:57,552 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:50:57,552 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:28:23.156782+0000
2025-04-15 09:50:57,552 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:50:57,552 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean+scrubbing+deep. Sleeping for 30 secs
2025-04-15 09:51:27,583 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:51:28,617 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033747 seconds
2025-04-15 09:51:28,639 - cephci - core_workflows:3506 - INFO - Scrubbing complete on the PG: 14.0
2025-04-15 09:51:28,640 - cephci - core_workflows:3507 - DEBUG - Final last_deep_scrub: 1492'100
2025-04-15 09:51:28,640 - cephci - core_workflows:3508 - DEBUG - Final last_deep_scrub_stamp: 2025-04-15T09:50:33.796036+0000
2025-04-15 09:51:28,640 - cephci - core_workflows:3511 - DEBUG - Total time taken for scrubbing to complete on the pg : 14.0 is 0:22:10.639254
2025-04-15 09:51:28,640 - cephci - core_workflows:3404 - DEBUG - Completed deep-scrubbing the pg : 14.0
2025-04-15 09:51:38,651 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 09:51:41,233 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 2.580784 seconds
2025-04-15 09:51:41,233 - cephci - shell:64 - DEBUG - HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 6 scrub errors; Possible data damage: 1 pg inconsistent
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
[ERR] OSD_SCRUB_ERRORS: 6 scrub errors
[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent
    pg 14.0 is active+clean+inconsistent, acting [13,8,3]

2025-04-15 09:51:41,233 - cephci - core_workflows:3317 - INFO - Health warning on cluster: 
 ('HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 6 scrub errors; Possible data damage: 1 pg inconsistent\n[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore\n     osd.6 observed slow operation indications in BlueStore\n[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)\n    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state\n[ERR] OSD_SCRUB_ERRORS: 6 scrub errors\n[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent\n    pg 14.0 is active+clean+inconsistent, acting [13,8,3]\n', "Inferring fsid 79ad3a88-19c4-11f0-bea5-fa163ea10f51\nInferring config /var/lib/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/mon.ceph-regression-juxejq-zv217a-node1-installer/config\nUsing ceph image with id '3b1483211cc8' and tag '8-110' created on 2025-04-11 13:52:17 +0000 UTC\ncp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6\n") 


2025-04-15 09:51:41,235 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 09:51:42,240 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.003875 seconds
2025-04-15 09:51:42,242 - cephci - core_workflows:3325 - INFO - pg inconsistent generated in the cluster
2025-04-15 09:51:42,243 - cephci - core_workflows:3408 - INFO - The inconsistent object is created in the pg: 14.0
2025-04-15 09:51:42,243 - cephci - test_osd_replicated_inconsistency_scenario:301 - INFO - The 2 objects are converted into inconsistent objects
2025-04-15 09:51:42,244 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_39 -f json on 10.0.195.79
2025-04-15 09:51:43,728 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_39 -f json on 10.0.195.79 took 1.482992 seconds
2025-04-15 09:51:43,728 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:51:43,728 - cephci - ceph:1186 - DEBUG - {"epoch":1507,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_39","raw_pgid":"14.67f804a8","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 09:51:43,729 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 09:51:43,729 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_39 is created in the pg-14.0
2025-04-15 09:51:43,770 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:51:45,285 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.515045 seconds
2025-04-15 09:51:45,286 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:51:45,286 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:51:45,327 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:51:47,813 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.485752 seconds
2025-04-15 09:51:47,814 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:51:47,815 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:51:48,820 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.004189 seconds
2025-04-15 09:51:48,822 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:51:50,270 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.448418 seconds
2025-04-15 09:51:50,271 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:51:50,271 - cephci - ceph:1186 - DEBUG - [{"container_id": "5e5ed6dcaa55", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "3.01%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:50:33.658833Z", "memory_request": 4294967296, "memory_usage": 64760000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:49:59.174288Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:51:50,272 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '5e5ed6dcaa55', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '3.01%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:50:33.658833Z', 'memory_request': 4294967296, 'memory_usage': 64760000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:49:59.174288Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:51:50,273 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:51:51,276 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003163 seconds
2025-04-15 09:51:56,278 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:51:56,279 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:52:02,288 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 6.007965 seconds
2025-04-15 09:52:07,294 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:52:08,300 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 09:52:08,300 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005634 seconds
2025-04-15 09:52:08,302 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:52:09,857 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.554657 seconds
2025-04-15 09:52:09,858 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:52:09,858 - cephci - ceph:1186 - DEBUG - [{"container_id": "5e5ed6dcaa55", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.94%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:51:32.499579Z", "memory_request": 4294967296, "memory_usage": 72540000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:49:59.174288Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:52:09,858 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '5e5ed6dcaa55', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.94%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:51:32.499579Z', 'memory_request': 4294967296, 'memory_usage': 72540000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:49:59.174288Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:52:09,859 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:52:29,880 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:52:31,480 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.599415 seconds
2025-04-15 09:52:31,481 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:52:31,481 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:51:53.119373Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:52:31,481 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:51:53.119373Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:52:31,481 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:52:31,482 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 09:52:36,487 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:52:37,977 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.490082 seconds
2025-04-15 09:52:37,978 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:52:37,978 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:52:14.381362Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:52:37,980 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191
2025-04-15 09:53:06,454 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191 took 28.474434 seconds
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:53:06,455 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:53:06,456 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:53:06,457 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:53:06,458 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:53:06,459 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:53:06,460 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:53:06,461 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:53:06,462 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:53:06,463 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:53:06,463 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:53:06,463 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_39 object
2025-04-15 09:53:06,466 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:53:09,060 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.593778 seconds
2025-04-15 09:53:09,062 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  rm-omap key_0 on 10.0.195.191
2025-04-15 09:53:25,519 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  rm-omap key_0 on 10.0.195.191 took 16.457337 seconds
2025-04-15 09:53:25,520 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_39 object
2025-04-15 09:53:30,527 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:53:33,170 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.643032 seconds
2025-04-15 09:53:33,173 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191
2025-04-15 09:53:48,506 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191 took 15.333217 seconds
2025-04-15 09:53:48,507 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:53:48,507 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:53:48,507 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:53:48,508 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:53:48,509 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:53:48,510 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:53:48,511 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:53:48,512 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:53:48,513 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:53:48,514 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:53:48,515 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:53:48,516 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:53:48,517 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:53:48,518 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:53:48,518 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:53:48,518 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:53:48,518 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_39Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 09:53:48,518 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 09:53:48,519 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:53:51,041 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.521373 seconds
2025-04-15 09:53:51,042 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191
2025-04-15 09:54:05,581 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_39  list-omap on 10.0.195.191 took 14.538764 seconds
2025-04-15 09:54:05,582 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:54:05,582 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:54:05,582 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:54:05,582 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:54:05,582 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:54:05,583 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:54:05,584 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:54:05,585 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:54:05,586 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:54:05,587 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:54:05,588 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:54:05,589 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:54:05,590 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:54:05,590 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:54:05,591 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:54:07,243 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.65116 seconds
2025-04-15 09:54:07,244 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:54:07,244 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 09:54:07,285 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:54:08,811 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.525913 seconds
2025-04-15 09:54:08,814 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:54:08,814 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:52:14.381362Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:54:08,814 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:54:08,815 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:54:09,820 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.004349 seconds
2025-04-15 09:54:09,821 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:54:11,307 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.485274 seconds
2025-04-15 09:54:11,308 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:54:11,308 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:52:14.381362Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:54:11,308 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:52:14.381362Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:54:11,310 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:54:12,314 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003657 seconds
2025-04-15 09:54:17,317 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:54:17,318 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:54:31,337 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 14.017722 seconds
2025-04-15 09:54:36,343 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:54:37,348 - cephci - ceph:1186 - DEBUG - active
2025-04-15 09:54:37,348 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004669 seconds
2025-04-15 09:54:37,351 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:54:39,842 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.491002 seconds
2025-04-15 09:54:39,843 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:53:53.791551Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:54:39,843 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:54:59,853 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:55:02,324 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.47008 seconds
2025-04-15 09:55:02,325 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'cd152753ae9e', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.25%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:54:19.896996Z', 'memory_request': 4294967296, 'memory_usage': 23480000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:54:08.758859Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:55:02,325 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:55:02,326 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 09:55:02,326 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 09:55:02,327 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:55:03,359 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032184 seconds
2025-04-15 09:55:03,381 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 09:55:03,382 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 09:55:03,382 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:50:33.796036+0000
2025-04-15 09:55:03,382 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 09:55:03,384 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 09:55:04,388 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003574 seconds
2025-04-15 09:55:06,391 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:55:07,427 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035464 seconds
2025-04-15 09:55:07,449 - cephci - core_workflows:3506 - INFO - Scrubbing complete on the PG: 14.0
2025-04-15 09:55:07,450 - cephci - core_workflows:3507 - DEBUG - Final last_deep_scrub: 1492'100
2025-04-15 09:55:07,450 - cephci - core_workflows:3508 - DEBUG - Final last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 09:55:07,450 - cephci - core_workflows:3511 - DEBUG - Total time taken for scrubbing to complete on the pg : 14.0 is 0:04:10.296281
2025-04-15 09:55:07,450 - cephci - core_workflows:3404 - DEBUG - Completed deep-scrubbing the pg : 14.0
2025-04-15 09:55:17,462 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 09:55:20,152 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 2.689624 seconds
2025-04-15 09:55:20,152 - cephci - shell:64 - DEBUG - HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 8 scrub errors; Possible data damage: 1 pg inconsistent
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
[ERR] OSD_SCRUB_ERRORS: 8 scrub errors
[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent
    pg 14.0 is active+clean+inconsistent, acting [13,8,3]

2025-04-15 09:55:20,153 - cephci - core_workflows:3317 - INFO - Health warning on cluster: 
 ('HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 8 scrub errors; Possible data damage: 1 pg inconsistent\n[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore\n     osd.6 observed slow operation indications in BlueStore\n[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)\n    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state\n[ERR] OSD_SCRUB_ERRORS: 8 scrub errors\n[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent\n    pg 14.0 is active+clean+inconsistent, acting [13,8,3]\n', "Inferring fsid 79ad3a88-19c4-11f0-bea5-fa163ea10f51\nInferring config /var/lib/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/mon.ceph-regression-juxejq-zv217a-node1-installer/config\nUsing ceph image with id '3b1483211cc8' and tag '8-110' created on 2025-04-11 13:52:17 +0000 UTC\ncp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6\n") 


2025-04-15 09:55:20,160 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 09:55:21,165 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.005096 seconds
2025-04-15 09:55:21,167 - cephci - core_workflows:3325 - INFO - pg inconsistent generated in the cluster
2025-04-15 09:55:21,168 - cephci - core_workflows:3408 - INFO - The inconsistent object is created in the pg: 14.0
2025-04-15 09:55:21,168 - cephci - test_osd_replicated_inconsistency_scenario:301 - INFO - The 3 objects are converted into inconsistent objects
2025-04-15 09:55:21,170 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_16 -f json on 10.0.195.79
2025-04-15 09:55:22,766 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_16 -f json on 10.0.195.79 took 1.596311 seconds
2025-04-15 09:55:22,767 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:55:22,768 - cephci - ceph:1186 - DEBUG - {"epoch":1513,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_16","raw_pgid":"14.2012ad04","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 09:55:22,768 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 09:55:22,768 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_16 is created in the pg-14.0
2025-04-15 09:55:22,810 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:55:25,430 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 2.620225 seconds
2025-04-15 09:55:25,432 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:55:26,969 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.536596 seconds
2025-04-15 09:55:26,970 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:55:26,970 - cephci - ceph:1186 - DEBUG - [{"container_id": "cd152753ae9e", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.67%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:54:43.563284Z", "memory_request": 4294967296, "memory_usage": 66120000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:54:08.758859Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:55:26,971 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:55:26,972 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:55:27,976 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003467 seconds
2025-04-15 09:55:27,978 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:55:29,509 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.531105 seconds
2025-04-15 09:55:29,510 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:55:29,510 - cephci - ceph:1186 - DEBUG - [{"container_id": "cd152753ae9e", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.67%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:54:43.563284Z", "memory_request": 4294967296, "memory_usage": 66120000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:54:08.758859Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:55:29,510 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'cd152753ae9e', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.67%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:54:43.563284Z', 'memory_request': 4294967296, 'memory_usage': 66120000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:54:08.758859Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:55:29,512 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:55:30,517 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.004559 seconds
2025-04-15 09:55:35,523 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:55:35,525 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:55:40,534 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 5.008689 seconds
2025-04-15 09:55:45,541 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:55:46,548 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 09:55:46,548 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.006057 seconds
2025-04-15 09:55:46,549 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:55:48,084 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.53496 seconds
2025-04-15 09:55:48,085 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:55:48,085 - cephci - ceph:1186 - DEBUG - [{"container_id": "cd152753ae9e", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "2.37%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:11.492672Z", "memory_request": 4294967296, "memory_usage": 72590000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:54:08.758859Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:55:48,086 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'cd152753ae9e', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '2.37%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:55:11.492672Z', 'memory_request': 4294967296, 'memory_usage': 72590000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:54:08.758859Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:55:48,087 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:56:08,108 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:56:09,634 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.52489 seconds
2025-04-15 09:56:09,634 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:56:09,636 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:30.356002Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:56:09,637 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:55:30.356002Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:56:09,637 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:56:09,637 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 09:56:14,647 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:56:17,121 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.474099 seconds
2025-04-15 09:56:17,123 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191
2025-04-15 09:56:45,422 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191 took 28.298033 seconds
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:56:45,423 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:56:45,424 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:56:45,425 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:56:45,426 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:56:45,427 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:56:45,428 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:56:45,429 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:56:45,430 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:56:45,431 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:56:45,431 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:56:45,431 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:56:45,431 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:56:45,432 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:56:45,433 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:56:45,433 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:56:45,433 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:56:45,433 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:56:45,433 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_16 object
2025-04-15 09:56:45,435 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:56:47,924 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.48939 seconds
2025-04-15 09:56:47,926 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  rm-omap key_0 on 10.0.195.191
2025-04-15 09:57:03,875 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  rm-omap key_0 on 10.0.195.191 took 15.948783 seconds
2025-04-15 09:57:03,876 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_16 object
2025-04-15 09:57:08,883 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:57:10,476 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.591873 seconds
2025-04-15 09:57:10,477 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:57:10,477 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:51.686998Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:57:10,478 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191
2025-04-15 09:57:29,999 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191 took 19.520251 seconds
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:57:30,000 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:57:30,001 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:57:30,002 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:57:30,003 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:57:30,004 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:57:30,005 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:57:30,006 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:57:30,007 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:57:30,008 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:57:30,008 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_16Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 09:57:30,008 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 09:57:30,009 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:57:31,532 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.522429 seconds
2025-04-15 09:57:31,532 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:57:31,533 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:51.686998Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:57:31,534 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191
2025-04-15 09:57:50,531 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_16  list-omap on 10.0.195.191 took 18.996286 seconds
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 09:57:50,532 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 09:57:50,533 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 09:57:50,534 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 09:57:50,535 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 09:57:50,536 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 09:57:50,537 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 09:57:50,538 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 09:57:50,539 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 09:57:50,540 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 09:57:50,541 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 09:57:50,542 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 09:57:53,134 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 2.591798 seconds
2025-04-15 09:57:53,136 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 09:57:54,644 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.508234 seconds
2025-04-15 09:57:54,645 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:57:54,645 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:51.686998Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:57:54,646 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 09:57:54,647 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 09:57:55,652 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.005207 seconds
2025-04-15 09:57:55,654 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:57:57,175 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.521186 seconds
2025-04-15 09:57:57,176 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:57:57,176 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:55:51.686998Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 09:57:57,176 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:55:51.686998Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:57:57,178 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 09:57:58,183 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.005014 seconds
2025-04-15 09:58:03,189 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 09:58:03,191 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:58:17,209 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 14.017698 seconds
2025-04-15 09:58:22,216 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 09:58:23,221 - cephci - ceph:1186 - DEBUG - active
2025-04-15 09:58:23,221 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.00499 seconds
2025-04-15 09:58:23,223 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:58:25,719 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.494786 seconds
2025-04-15 09:58:25,719 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:57:40.638887Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 09:58:25,720 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 09:58:45,742 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 09:58:47,294 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.551655 seconds
2025-04-15 09:58:47,296 - cephci - ceph:1186 - DEBUG - 
2025-04-15 09:58:47,296 - cephci - ceph:1186 - DEBUG - [{"container_id": "ed1e78e3a2a6", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.61%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T09:58:06.591557Z", "memory_request": 4294967296, "memory_usage": 17340000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T09:57:54.796972Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 09:58:47,297 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'ed1e78e3a2a6', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.61%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T09:58:06.591557Z', 'memory_request': 4294967296, 'memory_usage': 17340000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:57:54.796972Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 09:58:47,298 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 09:58:47,298 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 09:58:47,298 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 09:58:47,300 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:58:48,335 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034979 seconds
2025-04-15 09:58:48,359 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 09:58:48,359 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 09:58:48,359 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 09:58:48,359 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 09:58:48,361 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 09:58:49,365 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003923 seconds
2025-04-15 09:58:51,369 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:58:52,407 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.038121 seconds
2025-04-15 09:58:52,430 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:58:52,430 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 09:58:52,431 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:58:52,431 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+undersized+degraded. Sleeping for 30 secs
2025-04-15 09:59:22,456 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:59:23,492 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035318 seconds
2025-04-15 09:59:23,524 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:59:23,525 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 09:59:23,525 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:59:23,525 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 09:59:53,556 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 09:59:54,590 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033192 seconds
2025-04-15 09:59:54,614 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 09:59:54,614 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 09:59:54,614 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 09:59:54,614 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:00:24,646 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:00:25,679 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032187 seconds
2025-04-15 10:00:25,702 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:00:25,702 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:00:25,702 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:00:25,702 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:00:55,733 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:00:56,766 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03263 seconds
2025-04-15 10:00:56,787 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:00:56,787 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:00:56,787 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:00:56,787 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:01:26,819 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:01:27,850 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03086 seconds
2025-04-15 10:01:27,872 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:01:27,872 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:01:27,872 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:01:27,872 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:01:57,889 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:01:58,922 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032066 seconds
2025-04-15 10:01:58,944 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:01:58,945 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:01:58,945 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:01:58,945 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:02:28,955 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:02:29,986 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030663 seconds
2025-04-15 10:02:30,008 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:02:30,008 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:02:30,008 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:02:30,008 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:03:00,026 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:03:01,058 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031994 seconds
2025-04-15 10:03:01,081 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:03:01,082 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:03:01,082 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:03:01,082 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:03:31,100 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:03:32,133 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032491 seconds
2025-04-15 10:03:32,155 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:03:32,156 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:03:32,156 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:03:32,156 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:04:02,168 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:04:03,200 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031936 seconds
2025-04-15 10:04:03,222 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:04:03,222 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:04:03,222 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:04:03,222 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:04:33,240 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:04:34,272 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03195 seconds
2025-04-15 10:04:34,357 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:04:34,358 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:04:34,358 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:04:34,358 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:05:04,384 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:05:05,415 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030921 seconds
2025-04-15 10:05:05,438 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:05:05,438 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:05:05,438 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:05:05,438 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:05:35,453 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:05:36,486 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032792 seconds
2025-04-15 10:05:36,511 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:05:36,512 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:05:36,512 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:05:36,512 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:06:06,529 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:06:07,561 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031538 seconds
2025-04-15 10:06:07,584 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:06:07,585 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:06:07,585 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:06:07,585 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:06:37,617 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:06:38,651 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033997 seconds
2025-04-15 10:06:38,677 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:06:38,678 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:06:38,678 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:06:38,678 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:07:08,688 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:07:09,721 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032938 seconds
2025-04-15 10:07:09,744 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:07:09,745 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:07:09,745 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:07:09,745 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:07:39,758 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:07:40,790 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032051 seconds
2025-04-15 10:07:40,812 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:07:40,812 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:07:40,813 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:07:40,813 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:08:10,844 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:08:11,878 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031754 seconds
2025-04-15 10:08:11,899 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:08:11,899 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:08:11,900 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:08:11,900 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:08:41,932 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:08:42,964 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031886 seconds
2025-04-15 10:08:42,986 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:08:42,987 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:08:42,987 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:08:42,987 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:09:13,019 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:09:14,052 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033011 seconds
2025-04-15 10:09:14,083 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:09:14,083 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:09:14,083 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:09:14,083 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:09:44,115 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:09:45,147 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03241 seconds
2025-04-15 10:09:45,174 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:09:45,174 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:09:45,174 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:09:45,174 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:10:15,201 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:10:16,238 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036883 seconds
2025-04-15 10:10:16,268 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:10:16,269 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:10:16,269 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:10:16,270 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:10:46,301 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:10:47,340 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.037995 seconds
2025-04-15 10:10:47,377 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:10:47,378 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:10:47,378 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:10:47,378 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:11:17,410 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:11:18,448 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.037847 seconds
2025-04-15 10:11:18,486 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:11:18,486 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:11:18,486 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:11:18,486 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:11:48,508 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:11:49,545 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036383 seconds
2025-04-15 10:11:49,567 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:11:49,567 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:11:49,567 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:11:49,567 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:12:19,578 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:12:20,613 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034223 seconds
2025-04-15 10:12:20,636 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:12:20,637 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:12:20,637 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:12:20,637 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:12:50,650 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:12:51,684 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033435 seconds
2025-04-15 10:12:51,707 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:12:51,708 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:12:51,708 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:12:51,708 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:13:21,719 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:13:22,751 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031963 seconds
2025-04-15 10:13:22,829 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:13:22,829 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:13:22,829 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:13:22,829 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:13:52,837 - cephci - core_workflows:3530 - ERROR - PG : 14.0 could not be deep-scrubbed in time
2025-04-15 10:13:52,837 - cephci - test_osd_replicated_inconsistency_scenario:294 - INFO - Cannot able to convert the object-omap_obj_15671_16 into inconsistent object.Picking another object to convert-Objects not scrubbed error.Currently the 3 objects converted into inconsistent objects
2025-04-15 10:13:52,839 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_27 -f json on 10.0.195.79
2025-04-15 10:13:54,368 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_27 -f json on 10.0.195.79 took 1.528873 seconds
2025-04-15 10:13:54,369 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:13:54,369 - cephci - ceph:1186 - DEBUG - {"epoch":1518,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_27","raw_pgid":"14.c1b02944","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 10:13:54,369 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 10:13:54,369 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_27 is created in the pg-14.0
2025-04-15 10:13:54,410 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:13:56,069 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.658763 seconds
2025-04-15 10:13:56,070 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:13:56,070 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 10:13:56,111 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:13:58,572 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.459884 seconds
2025-04-15 10:13:58,572 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:13:58,574 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:13:59,579 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.004911 seconds
2025-04-15 10:13:59,581 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:14:02,106 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.525598 seconds
2025-04-15 10:14:02,108 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'ed1e78e3a2a6', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.06%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:08:34.652434Z', 'memory_request': 4294967296, 'memory_usage': 80210000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:57:54.796972Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:14:02,109 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:14:03,113 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003113 seconds
2025-04-15 10:14:08,119 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:14:08,122 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:14:12,129 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 4.006706 seconds
2025-04-15 10:14:17,137 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:14:18,142 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 10:14:18,143 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005935 seconds
2025-04-15 10:14:18,145 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:14:20,664 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.519193 seconds
2025-04-15 10:14:20,665 - cephci - core_workflows:2089 - DEBUG - [{'container_id': 'ed1e78e3a2a6', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.00%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:13:43.224504Z', 'memory_request': 4294967296, 'memory_usage': 80610000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T09:57:54.796972Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:14:20,666 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:14:40,669 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:14:43,259 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.589164 seconds
2025-04-15 10:14:43,259 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:14:00.958964Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:14:43,260 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:14:43,260 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 10:14:48,266 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:14:50,761 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.494725 seconds
2025-04-15 10:14:50,763 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191
2025-04-15 10:15:21,109 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191 took 30.34543 seconds
2025-04-15 10:15:21,110 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:15:21,110 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_27 object
2025-04-15 10:15:21,111 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:15:22,698 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.585955 seconds
2025-04-15 10:15:22,699 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:15:22,699 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:14:24.762450Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:15:22,701 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  rm-omap key_0 on 10.0.195.191
2025-04-15 10:15:40,696 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  rm-omap key_0 on 10.0.195.191 took 17.994907 seconds
2025-04-15 10:15:40,697 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_27 object
2025-04-15 10:15:45,704 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:15:47,291 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.586297 seconds
2025-04-15 10:15:47,291 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:15:47,291 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:14:24.762450Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:15:47,293 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191
2025-04-15 10:16:10,975 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191 took 23.681391 seconds
2025-04-15 10:16:10,975 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_27Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 10:16:10,975 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 10:16:10,977 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:16:12,490 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.512764 seconds
2025-04-15 10:16:12,491 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:16:12,491 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:14:24.762450Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:16:12,492 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191
2025-04-15 10:16:34,007 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_27  list-omap on 10.0.195.191 took 21.514272 seconds
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 10:16:34,008 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 10:16:34,009 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 10:16:34,010 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 10:16:34,011 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 10:16:34,012 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 10:16:34,013 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 10:16:34,014 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 10:16:34,015 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 10:16:34,016 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 10:16:34,016 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:16:34,018 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:16:35,605 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.587148 seconds
2025-04-15 10:16:35,606 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:16:35,606 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 10:16:35,647 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:16:38,225 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.577426 seconds
2025-04-15 10:16:38,225 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:16:38,227 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:16:39,231 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003931 seconds
2025-04-15 10:16:39,232 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:16:40,735 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.502227 seconds
2025-04-15 10:16:40,735 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:16:40,735 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:14:24.762450Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:16:40,736 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:14:24.762450Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:16:40,737 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:16:41,740 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003097 seconds
2025-04-15 10:16:46,746 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:16:46,747 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:16:59,763 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 13.015568 seconds
2025-04-15 10:17:04,772 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:17:05,788 - cephci - ceph:1186 - DEBUG - active
2025-04-15 10:17:05,789 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.006106 seconds
2025-04-15 10:17:05,791 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:17:08,290 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.499221 seconds
2025-04-15 10:17:08,291 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:16:23.762808Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:17:08,291 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:17:28,312 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:17:30,789 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.475838 seconds
2025-04-15 10:17:30,789 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '38747e3b7501', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.71%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:16:49.393754Z', 'memory_request': 4294967296, 'memory_usage': 16790000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:16:38.037843Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:17:30,790 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:17:30,790 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 10:17:30,790 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 10:17:30,791 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:17:31,825 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033335 seconds
2025-04-15 10:17:31,847 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 10:17:31,848 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 10:17:31,848 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:17:31,848 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 10:17:31,849 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 10:17:32,853 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003592 seconds
2025-04-15 10:17:34,857 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:17:35,889 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031926 seconds
2025-04-15 10:17:35,911 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:17:35,911 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:17:35,911 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:17:35,911 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+undersized+degraded. Sleeping for 30 secs
2025-04-15 10:18:05,941 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:18:06,975 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033373 seconds
2025-04-15 10:18:06,999 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:18:07,000 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:18:07,000 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:18:07,000 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:18:37,026 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:18:38,061 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034579 seconds
2025-04-15 10:18:38,084 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:18:38,085 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:18:38,085 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:18:38,085 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:19:08,100 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:19:09,134 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032826 seconds
2025-04-15 10:19:09,155 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:19:09,155 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:19:09,155 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:19:09,155 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:19:39,171 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:19:40,203 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031258 seconds
2025-04-15 10:19:40,227 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:19:40,227 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:19:40,228 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:19:40,228 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:20:10,247 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:20:11,281 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033271 seconds
2025-04-15 10:20:11,305 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:20:11,305 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:20:11,305 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:20:11,306 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:20:41,337 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:20:42,375 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.037023 seconds
2025-04-15 10:20:42,400 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:20:42,400 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:20:42,401 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:20:42,401 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:21:12,426 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:21:13,461 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034699 seconds
2025-04-15 10:21:13,487 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:21:13,487 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:21:13,487 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:21:13,487 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:21:43,510 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:21:44,544 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033619 seconds
2025-04-15 10:21:44,569 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:21:44,570 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:21:44,570 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:21:44,570 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:22:14,575 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:22:15,607 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032123 seconds
2025-04-15 10:22:15,632 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:22:15,632 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:22:15,633 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:22:15,633 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:22:45,641 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:22:46,673 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031133 seconds
2025-04-15 10:22:46,698 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:22:46,699 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:22:46,699 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:22:46,699 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:23:16,730 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:23:17,763 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032266 seconds
2025-04-15 10:23:17,787 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:23:17,787 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:23:17,787 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:23:17,787 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:23:47,819 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:23:48,854 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034216 seconds
2025-04-15 10:23:48,880 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:23:48,881 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:23:48,881 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:23:48,881 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:24:18,912 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:24:19,946 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033491 seconds
2025-04-15 10:24:19,987 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:24:19,987 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:24:19,987 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:24:19,988 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:24:50,014 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:24:51,046 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031706 seconds
2025-04-15 10:24:51,130 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:24:51,130 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:24:51,131 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:24:51,131 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:25:21,162 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:25:22,194 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031652 seconds
2025-04-15 10:25:22,216 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:25:22,217 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:25:22,217 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:25:22,217 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:25:52,238 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:25:53,273 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034314 seconds
2025-04-15 10:25:53,299 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:25:53,299 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:25:53,299 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:25:53,299 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:26:23,309 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:26:24,342 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031472 seconds
2025-04-15 10:26:24,367 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:26:24,367 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:26:24,368 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:26:24,368 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:26:54,400 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:26:55,434 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033794 seconds
2025-04-15 10:26:55,462 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:26:55,463 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:26:55,463 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:26:55,463 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:27:25,495 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:27:26,527 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031521 seconds
2025-04-15 10:27:26,559 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:27:26,559 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:27:26,560 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:27:26,560 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:27:56,592 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:27:57,629 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036854 seconds
2025-04-15 10:27:57,669 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:27:57,670 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:27:57,670 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:27:57,670 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:28:27,694 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:28:28,728 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032955 seconds
2025-04-15 10:28:28,752 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:28:28,752 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:28:28,752 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:28:28,752 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:28:58,763 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:28:59,795 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031738 seconds
2025-04-15 10:28:59,819 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:28:59,819 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:28:59,819 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:28:59,820 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:29:29,846 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:29:30,880 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033627 seconds
2025-04-15 10:29:30,902 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:29:30,902 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:29:30,902 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:29:30,903 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:30:00,910 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:30:01,947 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.037173 seconds
2025-04-15 10:30:01,970 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:30:01,970 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:30:01,971 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:30:01,971 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:30:32,002 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:30:33,035 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032015 seconds
2025-04-15 10:30:33,058 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:30:33,059 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:30:33,059 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:30:33,059 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:31:03,091 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:31:04,127 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035613 seconds
2025-04-15 10:31:04,165 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:31:04,165 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:31:04,165 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:31:04,165 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:31:34,197 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:31:35,234 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036555 seconds
2025-04-15 10:31:35,257 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:31:35,257 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:31:35,257 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:31:35,258 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:32:05,285 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:32:06,316 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029993 seconds
2025-04-15 10:32:06,338 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:32:06,339 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:32:06,339 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:32:06,339 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:32:36,355 - cephci - core_workflows:3530 - ERROR - PG : 14.0 could not be deep-scrubbed in time
2025-04-15 10:32:36,356 - cephci - test_osd_replicated_inconsistency_scenario:294 - INFO - Cannot able to convert the object-omap_obj_15671_27 into inconsistent object.Picking another object to convert-Objects not scrubbed error.Currently the 3 objects converted into inconsistent objects
2025-04-15 10:32:36,357 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_38 -f json on 10.0.195.79
2025-04-15 10:32:38,836 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_38 -f json on 10.0.195.79 took 2.477893 seconds
2025-04-15 10:32:38,836 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 10:32:38,838 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_38 is created in the pg-14.0
2025-04-15 10:32:38,840 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:32:40,431 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.591474 seconds
2025-04-15 10:32:40,432 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:32:40,432 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 10:32:40,473 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:32:42,977 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.503286 seconds
2025-04-15 10:32:42,977 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:32:42,979 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:32:43,982 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003291 seconds
2025-04-15 10:32:43,984 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:32:45,438 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.453332 seconds
2025-04-15 10:32:45,438 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:32:45,439 - cephci - ceph:1186 - DEBUG - [{"container_id": "38747e3b7501", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.99%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:27:47.458975Z", "memory_request": 4294967296, "memory_usage": 79570000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:16:38.037843Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 10:32:45,439 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '38747e3b7501', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.99%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:27:47.458975Z', 'memory_request': 4294967296, 'memory_usage': 79570000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:16:38.037843Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:32:45,440 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:32:46,444 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003233 seconds
2025-04-15 10:32:51,449 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:32:51,451 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:32:56,460 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 5.007932 seconds
2025-04-15 10:33:01,467 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:33:02,472 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 10:33:02,473 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.00511 seconds
2025-04-15 10:33:02,474 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:33:04,030 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.555144 seconds
2025-04-15 10:33:04,031 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:33:04,031 - cephci - ceph:1186 - DEBUG - [{"container_id": "38747e3b7501", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.94%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:32:28.364349Z", "memory_request": 4294967296, "memory_usage": 79860000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:16:38.037843Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 10:33:04,031 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '38747e3b7501', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.94%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:32:28.364349Z', 'memory_request': 4294967296, 'memory_usage': 79860000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:16:38.037843Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:33:04,032 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:33:24,054 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:33:26,469 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.414822 seconds
2025-04-15 10:33:26,469 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:32:45.953305Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:33:26,470 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:33:26,470 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 10:33:31,476 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:33:32,911 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.432499 seconds
2025-04-15 10:33:32,912 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:33:32,912 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:33:32,913 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191
2025-04-15 10:34:01,291 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191 took 28.377055 seconds
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 10:34:01,292 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 10:34:01,293 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 10:34:01,294 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 10:34:01,295 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 10:34:01,296 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 10:34:01,297 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 10:34:01,298 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 10:34:01,299 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 10:34:01,300 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 10:34:01,301 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 10:34:01,302 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:34:01,302 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_38 object
2025-04-15 10:34:01,304 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:34:02,820 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.515052 seconds
2025-04-15 10:34:02,820 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:34:02,820 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:34:02,822 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  rm-omap key_0 on 10.0.195.191
2025-04-15 10:34:22,987 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  rm-omap key_0 on 10.0.195.191 took 20.164737 seconds
2025-04-15 10:34:22,988 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_38 object
2025-04-15 10:34:27,994 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:34:29,500 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.505368 seconds
2025-04-15 10:34:29,501 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:34:29,501 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:34:29,502 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191
2025-04-15 10:34:50,513 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191 took 21.010431 seconds
2025-04-15 10:34:50,513 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 10:34:50,513 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 10:34:50,514 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 10:34:50,515 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 10:34:50,516 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 10:34:50,517 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 10:34:50,518 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 10:34:50,519 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 10:34:50,520 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 10:34:50,521 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 10:34:50,521 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_38Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 10:34:50,521 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 10:34:50,522 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:34:51,977 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.454084 seconds
2025-04-15 10:34:51,977 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:34:51,977 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:34:51,979 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191
2025-04-15 10:35:13,954 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_38  list-omap on 10.0.195.191 took 21.974828 seconds
2025-04-15 10:35:13,954 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:35:13,956 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:35:15,547 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.591437 seconds
2025-04-15 10:35:15,548 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:35:15,548 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 10:35:15,589 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:35:17,096 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.5068 seconds
2025-04-15 10:35:17,097 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:35:17,097 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:35:17,098 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:35:17,099 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:35:18,102 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.002917 seconds
2025-04-15 10:35:18,104 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:35:19,622 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.516953 seconds
2025-04-15 10:35:19,622 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:35:19,623 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:33:06.867710Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:35:19,623 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:33:06.867710Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:35:19,624 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:35:20,629 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.004099 seconds
2025-04-15 10:35:25,634 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:35:25,636 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:35:38,653 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 13.016977 seconds
2025-04-15 10:35:43,659 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:35:44,664 - cephci - ceph:1186 - DEBUG - active
2025-04-15 10:35:44,665 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004974 seconds
2025-04-15 10:35:44,666 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:35:47,129 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.462133 seconds
2025-04-15 10:35:47,129 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:35:01.372600Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:35:47,130 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:36:07,151 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:36:08,728 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.576526 seconds
2025-04-15 10:36:08,729 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:36:08,729 - cephci - ceph:1186 - DEBUG - [{"container_id": "97600a8edfa1", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.75%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:35:27.403017Z", "memory_request": 4294967296, "memory_usage": 17500000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:35:17.114544Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 10:36:08,730 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '97600a8edfa1', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.75%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:35:27.403017Z', 'memory_request': 4294967296, 'memory_usage': 17500000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:35:17.114544Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:36:08,731 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:36:08,731 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 10:36:08,731 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 10:36:08,732 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:36:09,765 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03281 seconds
2025-04-15 10:36:09,788 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 10:36:09,788 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 10:36:09,788 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:36:09,788 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 10:36:09,790 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 10:36:10,793 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003458 seconds
2025-04-15 10:36:12,797 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:36:13,829 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031398 seconds
2025-04-15 10:36:13,852 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:36:13,852 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:36:13,852 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:36:13,852 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+undersized+degraded. Sleeping for 30 secs
2025-04-15 10:36:43,857 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:36:44,889 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030728 seconds
2025-04-15 10:36:44,965 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:36:44,966 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:36:44,966 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:36:44,966 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:37:14,988 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:37:16,022 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033258 seconds
2025-04-15 10:37:16,044 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:37:16,044 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:37:16,044 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:37:16,044 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:37:46,046 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:37:47,079 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032228 seconds
2025-04-15 10:37:47,101 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:37:47,101 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:37:47,101 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:37:47,101 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:38:17,133 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:38:18,165 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031276 seconds
2025-04-15 10:38:18,188 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:38:18,189 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:38:18,189 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:38:18,189 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:38:48,221 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:38:49,256 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034192 seconds
2025-04-15 10:38:49,278 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:38:49,278 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:38:49,278 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:38:49,278 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:39:19,310 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:39:20,346 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03529 seconds
2025-04-15 10:39:20,382 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:39:20,382 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:39:20,382 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:39:20,382 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:39:50,413 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:39:51,445 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031538 seconds
2025-04-15 10:39:51,469 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:39:51,469 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:39:51,469 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:39:51,469 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:40:21,501 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:40:22,536 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033327 seconds
2025-04-15 10:40:22,559 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:40:22,559 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:40:22,559 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:40:22,559 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:40:52,592 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:40:53,625 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03263 seconds
2025-04-15 10:40:53,649 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:40:53,649 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:40:53,649 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:40:53,649 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:41:23,682 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:41:24,720 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.038385 seconds
2025-04-15 10:41:24,754 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:41:24,755 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:41:24,755 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:41:24,755 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:41:54,773 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:41:55,809 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03504 seconds
2025-04-15 10:41:55,830 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:41:55,831 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:41:55,831 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:41:55,831 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:42:25,860 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:42:26,894 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03268 seconds
2025-04-15 10:42:26,917 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:42:26,917 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:42:26,917 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:42:26,917 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:42:56,949 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:42:57,983 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033357 seconds
2025-04-15 10:42:58,006 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:42:58,006 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:42:58,006 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:42:58,006 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:43:28,028 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:43:29,061 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.0323 seconds
2025-04-15 10:43:29,084 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:43:29,084 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:43:29,084 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:43:29,084 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:43:59,101 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:44:00,134 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033409 seconds
2025-04-15 10:44:00,160 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:44:00,161 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:44:00,161 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:44:00,161 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:44:30,170 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:44:31,203 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032598 seconds
2025-04-15 10:44:31,229 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:44:31,230 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:44:31,230 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:44:31,230 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:45:01,256 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:45:02,289 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032075 seconds
2025-04-15 10:45:02,311 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:45:02,312 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:45:02,312 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:45:02,312 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:45:32,324 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:45:33,356 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031957 seconds
2025-04-15 10:45:33,381 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:45:33,381 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:45:33,381 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:45:33,381 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:46:03,390 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:46:04,420 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03034 seconds
2025-04-15 10:46:04,442 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:46:04,443 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:46:04,443 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:46:04,443 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:46:34,450 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:46:35,481 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030054 seconds
2025-04-15 10:46:35,502 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:46:35,502 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:46:35,502 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:46:35,502 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:47:05,516 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:47:06,549 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032887 seconds
2025-04-15 10:47:06,572 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:47:06,572 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:47:06,572 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:47:06,572 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:47:36,580 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:47:37,615 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034487 seconds
2025-04-15 10:47:37,697 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:47:37,698 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:47:37,698 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:47:37,698 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:48:07,730 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:48:08,762 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032219 seconds
2025-04-15 10:48:08,786 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:48:08,786 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:48:08,786 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:48:08,786 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:48:38,818 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:48:39,850 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031604 seconds
2025-04-15 10:48:39,872 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:48:39,873 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:48:39,873 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:48:39,873 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:49:09,884 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:49:10,917 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033073 seconds
2025-04-15 10:49:10,940 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:49:10,941 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:49:10,941 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:49:10,941 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:49:40,957 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:49:41,994 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034849 seconds
2025-04-15 10:49:42,031 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:49:42,031 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:49:42,032 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:49:42,032 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:50:12,063 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:50:13,095 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031265 seconds
2025-04-15 10:50:13,119 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:50:13,119 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:50:13,119 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:50:13,120 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:50:43,147 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:50:44,179 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031396 seconds
2025-04-15 10:50:44,202 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:50:44,202 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:50:44,203 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:50:44,203 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:51:14,224 - cephci - core_workflows:3530 - ERROR - PG : 14.0 could not be deep-scrubbed in time
2025-04-15 10:51:14,224 - cephci - test_osd_replicated_inconsistency_scenario:294 - INFO - Cannot able to convert the object-omap_obj_15671_38 into inconsistent object.Picking another object to convert-Objects not scrubbed error.Currently the 3 objects converted into inconsistent objects
2025-04-15 10:51:14,226 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_48 -f json on 10.0.195.79
2025-04-15 10:51:15,697 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_48 -f json on 10.0.195.79 took 1.470514 seconds
2025-04-15 10:51:15,697 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:51:15,697 - cephci - ceph:1186 - DEBUG - {"epoch":1528,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_48","raw_pgid":"14.45f25954","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 10:51:15,697 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 10:51:15,698 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_48 is created in the pg-14.0
2025-04-15 10:51:15,738 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:51:18,495 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 2.756561 seconds
2025-04-15 10:51:18,497 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:51:20,013 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.514794 seconds
2025-04-15 10:51:20,013 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:51:20,013 - cephci - ceph:1186 - DEBUG - [{"container_id": "97600a8edfa1", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.96%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:46:23.059733Z", "memory_request": 4294967296, "memory_usage": 80230000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:35:17.114544Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 10:51:20,014 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:51:20,015 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:51:21,018 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003268 seconds
2025-04-15 10:51:21,020 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:51:23,553 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.532691 seconds
2025-04-15 10:51:23,553 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '97600a8edfa1', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.96%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:46:23.059733Z', 'memory_request': 4294967296, 'memory_usage': 80230000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:35:17.114544Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:51:23,555 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:51:24,558 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003492 seconds
2025-04-15 10:51:29,564 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:51:29,566 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:51:35,576 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 6.010138 seconds
2025-04-15 10:51:40,581 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:51:41,587 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 10:51:41,588 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.005879 seconds
2025-04-15 10:51:41,589 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:51:44,080 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.490956 seconds
2025-04-15 10:51:44,081 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '97600a8edfa1', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.93%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:51:04.846042Z', 'memory_request': 4294967296, 'memory_usage': 80620000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:35:17.114544Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:51:44,081 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:52:04,093 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:52:05,505 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.411727 seconds
2025-04-15 10:52:05,506 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:52:05,506 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:51:25.067304Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:52:05,507 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:51:25.067304Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:52:05,507 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:52:05,507 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 10:52:10,513 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:52:12,001 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.485371 seconds
2025-04-15 10:52:12,002 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:52:12,002 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:51:47.610168Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:52:12,004 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191
2025-04-15 10:52:32,893 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191 took 20.888446 seconds
2025-04-15 10:52:32,895 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:52:32,895 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_48 object
2025-04-15 10:52:32,897 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:52:35,454 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.556869 seconds
2025-04-15 10:52:35,455 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  rm-omap key_0 on 10.0.195.191
2025-04-15 10:52:40,953 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  rm-omap key_0 on 10.0.195.191 took 5.497567 seconds
2025-04-15 10:52:40,954 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_48 object
2025-04-15 10:52:45,961 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:52:48,403 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.441832 seconds
2025-04-15 10:52:48,405 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191
2025-04-15 10:52:55,995 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191 took 7.589016 seconds
2025-04-15 10:52:55,996 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_48Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 10:52:55,996 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 10:52:55,997 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:52:57,480 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.482442 seconds
2025-04-15 10:52:57,481 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:52:57,481 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:51:47.610168Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:52:57,482 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191
2025-04-15 10:53:03,769 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_48  list-omap on 10.0.195.191 took 6.286464 seconds
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 10:53:03,770 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 10:53:03,771 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 10:53:03,772 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 10:53:03,773 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 10:53:03,774 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 10:53:03,775 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 10:53:03,776 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 10:53:03,777 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 10:53:03,778 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 10:53:03,778 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 10:53:03,779 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 10:53:05,331 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.551684 seconds
2025-04-15 10:53:05,332 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:53:05,332 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 10:53:05,373 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 10:53:07,800 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.426269 seconds
2025-04-15 10:53:07,801 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 10:53:07,802 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 10:53:08,807 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003963 seconds
2025-04-15 10:53:08,808 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:53:10,221 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.412714 seconds
2025-04-15 10:53:10,222 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:53:10,223 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:51:47.610168Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:53:10,223 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:51:47.610168Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:53:10,224 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 10:53:11,228 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.00313 seconds
2025-04-15 10:53:16,233 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 10:53:16,236 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:53:29,252 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 13.01512 seconds
2025-04-15 10:53:34,259 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 10:53:35,264 - cephci - ceph:1186 - DEBUG - active
2025-04-15 10:53:35,264 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004743 seconds
2025-04-15 10:53:35,266 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:53:36,811 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.545652 seconds
2025-04-15 10:53:36,812 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:53:36,812 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:52:52.915703Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 10:53:36,812 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:52:52.915703Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 10:53:36,813 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 10:53:56,829 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 10:53:58,334 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.505228 seconds
2025-04-15 10:53:58,335 - cephci - ceph:1186 - DEBUG - 
2025-04-15 10:53:58,335 - cephci - ceph:1186 - DEBUG - [{"container_id": "168c34bf44da", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "4.07%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T10:53:18.062929Z", "memory_request": 4294967296, "memory_usage": 41810000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:53:07.039955Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 10:53:58,336 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '168c34bf44da', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '4.07%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T10:53:18.062929Z', 'memory_request': 4294967296, 'memory_usage': 41810000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:53:07.039955Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 10:53:58,336 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 10:53:58,337 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 10:53:58,337 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 10:53:58,339 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:53:59,372 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033043 seconds
2025-04-15 10:53:59,394 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 10:53:59,394 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 10:53:59,394 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:53:59,394 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 10:53:59,396 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 10:54:00,400 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.003719 seconds
2025-04-15 10:54:02,404 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:54:03,435 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031104 seconds
2025-04-15 10:54:03,457 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:54:03,457 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:54:03,457 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:54:03,457 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+undersized+degraded. Sleeping for 30 secs
2025-04-15 10:54:33,477 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:54:34,509 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031574 seconds
2025-04-15 10:54:34,531 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:54:34,532 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:54:34,532 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:54:34,532 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:55:04,561 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:55:05,593 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030893 seconds
2025-04-15 10:55:05,615 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:55:05,615 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:55:05,616 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:55:05,616 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:55:35,636 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:55:36,669 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032431 seconds
2025-04-15 10:55:36,691 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:55:36,691 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:55:36,691 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:55:36,691 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:56:06,723 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:56:07,758 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033966 seconds
2025-04-15 10:56:07,783 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:56:07,784 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:56:07,784 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:56:07,784 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:56:37,816 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:56:38,851 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033978 seconds
2025-04-15 10:56:38,876 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:56:38,876 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:56:38,876 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:56:38,877 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:57:08,908 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:57:09,943 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033112 seconds
2025-04-15 10:57:09,965 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:57:09,966 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:57:09,967 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:57:09,967 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:57:39,998 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:57:41,031 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031719 seconds
2025-04-15 10:57:41,054 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:57:41,055 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:57:41,055 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:57:41,055 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:58:11,087 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:58:12,119 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031679 seconds
2025-04-15 10:58:12,140 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:58:12,140 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:58:12,141 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:58:12,141 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:58:42,166 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:58:43,200 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033018 seconds
2025-04-15 10:58:43,276 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:58:43,276 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:58:43,276 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:58:43,276 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:59:13,278 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:59:14,309 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03054 seconds
2025-04-15 10:59:14,330 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:59:14,330 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:59:14,330 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:59:14,331 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 10:59:44,362 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 10:59:45,393 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03035 seconds
2025-04-15 10:59:45,414 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 10:59:45,414 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 10:59:45,414 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 10:59:45,414 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:00:15,437 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:00:16,470 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032362 seconds
2025-04-15 11:00:16,493 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:00:16,493 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:00:16,493 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:00:16,493 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:00:46,525 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:00:47,559 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033703 seconds
2025-04-15 11:00:47,596 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:00:47,597 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:00:47,597 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:00:47,597 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:01:17,628 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:01:18,660 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031087 seconds
2025-04-15 11:01:18,682 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:01:18,682 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:01:18,683 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:01:18,683 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:01:48,714 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:01:49,746 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031205 seconds
2025-04-15 11:01:49,769 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:01:49,769 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:01:49,769 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:01:49,769 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:02:19,800 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:02:20,834 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033648 seconds
2025-04-15 11:02:20,857 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:02:20,857 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:02:20,857 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:02:20,858 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:02:50,870 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:02:51,904 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033847 seconds
2025-04-15 11:02:51,940 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:02:51,941 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:02:51,941 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:02:51,941 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:03:21,953 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:03:22,987 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033335 seconds
2025-04-15 11:03:23,009 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:03:23,009 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:03:23,009 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:03:23,010 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:03:53,041 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:03:54,076 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034826 seconds
2025-04-15 11:03:54,099 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:03:54,100 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:03:54,100 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:03:54,100 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:04:24,129 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:04:25,161 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031582 seconds
2025-04-15 11:04:25,183 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:04:25,183 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:04:25,184 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:04:25,184 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:04:55,204 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:04:56,237 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033155 seconds
2025-04-15 11:04:56,260 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:04:56,260 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:04:56,261 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:04:56,261 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:05:26,285 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:05:27,316 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031033 seconds
2025-04-15 11:05:27,340 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:05:27,340 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:05:27,340 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:05:27,340 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:05:57,364 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:05:58,397 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032056 seconds
2025-04-15 11:05:58,419 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:05:58,419 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:05:58,419 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:05:58,419 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:06:28,449 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:06:29,481 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032334 seconds
2025-04-15 11:06:29,503 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:06:29,504 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:06:29,505 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:06:29,505 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:06:59,520 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:07:00,553 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031874 seconds
2025-04-15 11:07:00,635 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:07:00,635 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:07:00,635 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:07:00,636 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:07:30,667 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:07:31,702 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034526 seconds
2025-04-15 11:07:31,734 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:07:31,734 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:07:31,736 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:07:31,736 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:08:01,766 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:08:02,796 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.02958 seconds
2025-04-15 11:08:02,818 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:08:02,818 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:08:02,818 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:08:02,818 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:08:32,820 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:08:33,854 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033244 seconds
2025-04-15 11:08:33,877 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:08:33,878 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:08:33,878 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:08:33,878 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:09:03,908 - cephci - core_workflows:3530 - ERROR - PG : 14.0 could not be deep-scrubbed in time
2025-04-15 11:09:03,909 - cephci - test_osd_replicated_inconsistency_scenario:294 - INFO - Cannot able to convert the object-omap_obj_15671_48 into inconsistent object.Picking another object to convert-Objects not scrubbed error.Currently the 3 objects converted into inconsistent objects
2025-04-15 11:09:03,910 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd map replicated_pool omap_obj_15671_44 -f json on 10.0.195.79
2025-04-15 11:09:05,365 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd map replicated_pool omap_obj_15671_44 -f json on 10.0.195.79 took 1.455236 seconds
2025-04-15 11:09:05,366 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:05,366 - cephci - ceph:1186 - DEBUG - {"epoch":1533,"pool":"replicated_pool","pool_id":14,"objname":"omap_obj_15671_44","raw_pgid":"14.df6a5674","pgid":"14.0","up":[13,8,3],"up_primary":13,"acting":[13,8,3],"acting_primary":13}
2025-04-15 11:09:05,366 - cephci - core_workflows:3355 - INFO - The object stored in the primary osd number-13
2025-04-15 11:09:05,367 - cephci - core_workflows:3357 - INFO - The object omap_obj_15671_44 is created in the pg-14.0
2025-04-15 11:09:05,407 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 11:09:07,005 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.596944 seconds
2025-04-15 11:09:07,005 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:07,005 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 11:09:07,046 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:09:08,524 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.477112 seconds
2025-04-15 11:09:08,525 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:08,525 - cephci - ceph:1186 - DEBUG - [{"container_id": "168c34bf44da", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.96%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:04:34.324514Z", "memory_request": 4294967296, "memory_usage": 81100000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:53:07.039955Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 11:09:08,525 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 11:09:08,526 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 11:09:09,530 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003153 seconds
2025-04-15 11:09:09,531 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:09:11,076 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.543998 seconds
2025-04-15 11:09:11,076 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:11,076 - cephci - ceph:1186 - DEBUG - [{"container_id": "168c34bf44da", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.96%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:04:34.324514Z", "memory_request": 4294967296, "memory_usage": 81100000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:53:07.039955Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 11:09:11,077 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '168c34bf44da', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.96%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:04:34.324514Z', 'memory_request': 4294967296, 'memory_usage': 81100000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:53:07.039955Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 11:09:11,078 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 11:09:12,081 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.002855 seconds
2025-04-15 11:09:17,086 - cephci - core_workflows:1616 - INFO - Performing stop on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 11:09:17,088 - cephci - ceph:1576 - INFO - Execute systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 11:09:21,095 - cephci - ceph:1606 - INFO - Execution of systemctl stop ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 4.006396 seconds
2025-04-15 11:09:26,102 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 11:09:27,106 - cephci - ceph:1186 - DEBUG - inactive
2025-04-15 11:09:27,106 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.00453 seconds
2025-04-15 11:09:27,108 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:09:28,727 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.619386 seconds
2025-04-15 11:09:28,728 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:28,728 - cephci - ceph:1186 - DEBUG - [{"container_id": "168c34bf44da", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "0.93%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:08:52.488955Z", "memory_request": 4294967296, "memory_usage": 81260000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T10:53:07.039955Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 11:09:28,728 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '168c34bf44da', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '0.93%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:08:52.488955Z', 'memory_request': 4294967296, 'memory_usage': 81260000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T10:53:07.039955Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 11:09:28,729 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 11:09:48,740 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:09:51,450 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.709913 seconds
2025-04-15 11:09:51,451 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:09:10.931721Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 11:09:51,451 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 11:09:51,451 - cephci - core_workflows:3363 - DEBUG - Stopped OSD : 13 to create inconsistent object
2025-04-15 11:09:56,458 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:09:57,982 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.523599 seconds
2025-04-15 11:09:57,983 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:09:57,983 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:09:32.007316Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 11:09:57,984 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191
2025-04-15 11:10:18,472 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191 took 20.488089 seconds
2025-04-15 11:10:18,473 - cephci - ceph:1186 - DEBUG - key_0
2025-04-15 11:10:18,473 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 11:10:18,473 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 11:10:18,474 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 11:10:18,475 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 11:10:18,476 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 11:10:18,477 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 11:10:18,478 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 11:10:18,479 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 11:10:18,480 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 11:10:18,481 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 11:10:18,482 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 11:10:18,483 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 11:10:18,484 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 11:10:18,485 - cephci - core_workflows:3367 - DEBUG - Key list before deletion : ['key_0', 'key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 11:10:18,485 - cephci - core_workflows:3371 - INFO -  Deleting the key :key_0 in the omap_obj_15671_44 object
2025-04-15 11:10:18,486 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:10:20,009 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.522559 seconds
2025-04-15 11:10:20,010 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:10:20,010 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:09:32.007316Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 11:10:20,011 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  rm-omap key_0 on 10.0.195.191
2025-04-15 11:10:30,738 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  rm-omap key_0 on 10.0.195.191 took 10.727064 seconds
2025-04-15 11:10:30,739 - cephci - core_workflows:3272 - INFO - key_0 object key is removed for the omap_obj_15671_44 object
2025-04-15 11:10:35,745 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:10:37,326 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.580286 seconds
2025-04-15 11:10:37,327 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:10:37,327 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:09:32.007316Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 11:10:37,328 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191
2025-04-15 11:10:45,987 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191 took 8.658068 seconds
2025-04-15 11:10:45,987 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 11:10:45,987 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 11:10:45,987 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 11:10:45,987 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 11:10:45,987 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 11:10:45,988 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 11:10:45,989 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 11:10:45,990 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 11:10:45,991 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 11:10:45,992 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 11:10:45,993 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 11:10:45,994 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 11:10:45,995 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 11:10:45,996 - cephci - core_workflows:3382 - DEBUG - Successfully Deleted the key_0 from object: omap_obj_15671_44Total keys deleted on the object : 1Keys removed : ['key_0']
2025-04-15 11:10:45,996 - cephci - core_workflows:3389 - DEBUG - Done with deleting KW pairs on the OSD : 13 for Obj : object_name
2025-04-15 11:10:45,997 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:10:48,503 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 2.505661 seconds
2025-04-15 11:10:48,504 - cephci - ceph:1576 - INFO - Execute cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191
2025-04-15 11:10:58,954 - cephci - ceph:1606 - INFO - Execution of cephadm shell --name osd.13 -- ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --pgid 14.0 omap_obj_15671_44  list-omap on 10.0.195.191 took 10.449654 seconds
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_1
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_10
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_11
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_12
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_13
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_14
2025-04-15 11:10:58,955 - cephci - ceph:1186 - DEBUG - key_15
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_16
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_17
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_18
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_19
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_2
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_20
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_21
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_22
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_23
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_24
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_25
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_26
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_27
2025-04-15 11:10:58,956 - cephci - ceph:1186 - DEBUG - key_28
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_29
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_3
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_30
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_31
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_32
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_33
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_34
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_35
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_36
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_37
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_38
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_39
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_4
2025-04-15 11:10:58,957 - cephci - ceph:1186 - DEBUG - key_40
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_41
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_42
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_43
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_44
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_45
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_46
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_47
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_48
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_49
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_5
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_50
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_51
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_52
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_53
2025-04-15 11:10:58,958 - cephci - ceph:1186 - DEBUG - key_54
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_55
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_56
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_57
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_58
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_59
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_6
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_60
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_61
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_62
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_63
2025-04-15 11:10:58,959 - cephci - ceph:1186 - DEBUG - key_64
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_65
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_66
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_67
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_68
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_69
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_7
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_70
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_71
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_72
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_73
2025-04-15 11:10:58,960 - cephci - ceph:1186 - DEBUG - key_74
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_75
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_76
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_77
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_78
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_79
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_8
2025-04-15 11:10:58,961 - cephci - ceph:1186 - DEBUG - key_80
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_81
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_82
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_83
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_84
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_85
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_86
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_87
2025-04-15 11:10:58,962 - cephci - ceph:1186 - DEBUG - key_88
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_89
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_9
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_90
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_91
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_92
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_93
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_94
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_95
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_96
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_97
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_98
2025-04-15 11:10:58,963 - cephci - ceph:1186 - DEBUG - key_99
2025-04-15 11:10:58,964 - cephci - core_workflows:3393 - DEBUG - Key list After deletion : ['key_1', 'key_10', 'key_11', 'key_12', 'key_13', 'key_14', 'key_15', 'key_16', 'key_17', 'key_18', 'key_19', 'key_2', 'key_20', 'key_21', 'key_22', 'key_23', 'key_24', 'key_25', 'key_26', 'key_27', 'key_28', 'key_29', 'key_3', 'key_30', 'key_31', 'key_32', 'key_33', 'key_34', 'key_35', 'key_36', 'key_37', 'key_38', 'key_39', 'key_4', 'key_40', 'key_41', 'key_42', 'key_43', 'key_44', 'key_45', 'key_46', 'key_47', 'key_48', 'key_49', 'key_5', 'key_50', 'key_51', 'key_52', 'key_53', 'key_54', 'key_55', 'key_56', 'key_57', 'key_58', 'key_59', 'key_6', 'key_60', 'key_61', 'key_62', 'key_63', 'key_64', 'key_65', 'key_66', 'key_67', 'key_68', 'key_69', 'key_7', 'key_70', 'key_71', 'key_72', 'key_73', 'key_74', 'key_75', 'key_76', 'key_77', 'key_78', 'key_79', 'key_8', 'key_80', 'key_81', 'key_82', 'key_83', 'key_84', 'key_85', 'key_86', 'key_87', 'key_88', 'key_89', 'key_9', 'key_90', 'key_91', 'key_92', 'key_93', 'key_94', 'key_95', 'key_96', 'key_97', 'key_98', 'key_99']
2025-04-15 11:10:58,965 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph fsid -f json on 10.0.195.79
2025-04-15 11:11:00,594 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph fsid -f json on 10.0.195.79 took 1.62826 seconds
2025-04-15 11:11:00,595 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:11:00,595 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51"}
2025-04-15 11:11:00,636 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79
2025-04-15 11:11:02,222 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 -f json on 10.0.195.79 took 1.585827 seconds
2025-04-15 11:11:02,223 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:11:02,223 - cephci - ceph:1186 - DEBUG - [{"container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:09:32.007316Z", "memory_request": 4294967296, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "status": 0, "status_desc": "stopped", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13"}]
2025-04-15 11:11:02,223 - cephci - core_workflows:1593 - DEBUG - Hostname of target host : ceph-regression-juxejq-zv217a-node3
2025-04-15 11:11:02,224 - cephci - ceph:1576 - INFO - Execute sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191
2025-04-15 11:11:03,228 - cephci - ceph:1606 - INFO - Execution of sudo date '+%Y-%m-%d %H:%M:%S' on 10.0.195.191 took 1.003337 seconds
2025-04-15 11:11:03,229 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:11:05,722 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.49217 seconds
2025-04-15 11:11:05,722 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:09:32.007316Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 11:11:05,724 - cephci - ceph:1576 - INFO - Execute systemctl reset-failed on 10.0.195.191
2025-04-15 11:11:06,728 - cephci - ceph:1606 - INFO - Execution of systemctl reset-failed on 10.0.195.191 took 1.003298 seconds
2025-04-15 11:11:11,733 - cephci - core_workflows:1616 - INFO - Performing start on osd-13 on host ceph-regression-juxejq-zv217a-node3. Command systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service
2025-04-15 11:11:11,735 - cephci - ceph:1576 - INFO - Execute systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 11:11:24,750 - cephci - ceph:1606 - INFO - Execution of systemctl start ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 13.015422 seconds
2025-04-15 11:11:29,757 - cephci - ceph:1576 - INFO - Execute systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191
2025-04-15 11:11:30,761 - cephci - ceph:1186 - DEBUG - active
2025-04-15 11:11:30,761 - cephci - ceph:1606 - INFO - Execution of systemctl is-active ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13.service on 10.0.195.191 took 1.004237 seconds
2025-04-15 11:11:30,764 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:11:33,319 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 2.555001 seconds
2025-04-15 11:11:33,320 - cephci - core_workflows:2089 - DEBUG - [{'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:10:46.133640Z', 'memory_request': 4294967296, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'status': 0, 'status_desc': 'stopped', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13'}]
2025-04-15 11:11:33,320 - cephci - core_workflows:1655 - INFO - osd_status: 0, status_desc: stopped
2025-04-15 11:11:53,331 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79
2025-04-15 11:11:54,792 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph orch ps --daemon_type osd --daemon_id 13 --refresh -f json on 10.0.195.79 took 1.460238 seconds
2025-04-15 11:11:54,793 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:11:54,793 - cephci - ceph:1186 - DEBUG - [{"container_id": "640401bbb928", "container_image_digests": ["cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee"], "container_image_id": "3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef", "container_image_name": "cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6", "cpu_percentage": "1.62%", "created": "2025-04-15T06:55:19.474682Z", "daemon_id": "13", "daemon_name": "osd.13", "daemon_type": "osd", "events": ["2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] \"Deployed osd.13 on host 'ceph-regression-juxejq-zv217a-node3'\"", "2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] \"restart osd.13 from host 'ceph-regression-juxejq-zv217a-node3'\""], "hostname": "ceph-regression-juxejq-zv217a-node3", "is_active": false, "last_refresh": "2025-04-15T11:11:13.569794Z", "memory_request": 4294967296, "memory_usage": 21450000, "pending_daemon_config": false, "ports": [], "service_name": "osd.all-available-devices", "started": "2025-04-15T11:11:02.621332Z", "status": 1, "status_desc": "running", "systemd_unit": "ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13", "version": "19.2.1-126.el9cp"}]
2025-04-15 11:11:54,793 - cephci - core_workflows:2089 - DEBUG - [{'container_id': '640401bbb928', 'container_image_digests': ['cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:ef6bebc203b9e271330f00b51d0616aa3de5cff643d4c6bf13e526801a5c18ee'], 'container_image_id': '3b1483211cc850965b5b32c6080476272f17b6b386903025fa3d9ab5f6e784ef', 'container_image_name': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu_percentage': '1.62%', 'created': '2025-04-15T06:55:19.474682Z', 'daemon_id': '13', 'daemon_name': 'osd.13', 'daemon_type': 'osd', 'events': ['2025-04-15T06:55:19.624379Z daemon:osd.13 [INFO] "Deployed osd.13 on host \'ceph-regression-juxejq-zv217a-node3\'"', '2025-04-15T07:20:09.542917Z daemon:osd.13 [INFO] "restart osd.13 from host \'ceph-regression-juxejq-zv217a-node3\'"'], 'hostname': 'ceph-regression-juxejq-zv217a-node3', 'is_active': False, 'last_refresh': '2025-04-15T11:11:13.569794Z', 'memory_request': 4294967296, 'memory_usage': 21450000, 'pending_daemon_config': False, 'ports': [], 'service_name': 'osd.all-available-devices', 'started': '2025-04-15T11:11:02.621332Z', 'status': 1, 'status_desc': 'running', 'systemd_unit': 'ceph-79ad3a88-19c4-11f0-bea5-fa163ea10f51@osd.13', 'version': '19.2.1-126.el9cp'}]
2025-04-15 11:11:54,793 - cephci - core_workflows:1655 - INFO - osd_status: 1, status_desc: running
2025-04-15 11:11:54,794 - cephci - core_workflows:3397 - DEBUG - Started the OSD: 13 and performing deep scrubs on the PG
2025-04-15 11:11:54,794 - cephci - core_workflows:3400 - INFO - Performing the deep-scrub on the pg-14.0
2025-04-15 11:11:54,795 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:11:55,828 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03239 seconds
2025-04-15 11:11:55,852 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 11:11:55,852 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 11:11:55,852 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:11:55,852 - cephci - core_workflows:3490 - DEBUG - Initiating deep-scrubbing on pg : 14.0
2025-04-15 11:11:55,853 - cephci - ceph:1576 - INFO - Execute ceph pg deep-scrub 14.0 on 10.0.195.240
2025-04-15 11:11:56,856 - cephci - ceph:1606 - INFO - Execution of ceph pg deep-scrub 14.0 on 10.0.195.240 took 1.002414 seconds
2025-04-15 11:11:58,860 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:11:59,892 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031297 seconds
2025-04-15 11:11:59,913 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:11:59,914 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T09:54:44.092317+0000
2025-04-15 11:11:59,914 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:11:59,914 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:12:29,944 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:12:30,977 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032219 seconds
2025-04-15 11:12:31,002 - cephci - core_workflows:3506 - INFO - Scrubbing complete on the PG: 14.0
2025-04-15 11:12:31,003 - cephci - core_workflows:3507 - DEBUG - Final last_deep_scrub: 1492'100
2025-04-15 11:12:31,003 - cephci - core_workflows:3508 - DEBUG - Final last_deep_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:12:31,003 - cephci - core_workflows:3511 - DEBUG - Total time taken for scrubbing to complete on the pg : 14.0 is 1:16:52.015866
2025-04-15 11:12:31,003 - cephci - core_workflows:3404 - DEBUG - Completed deep-scrubbing the pg : 14.0
2025-04-15 11:12:41,013 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 11:12:42,585 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 1.571024 seconds
2025-04-15 11:12:42,585 - cephci - ceph:1186 - DEBUG - HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 18 scrub errors; Possible data damage: 1 pg inconsistent
2025-04-15 11:12:42,585 - cephci - ceph:1186 - DEBUG - [WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
2025-04-15 11:12:42,586 - cephci - ceph:1186 - DEBUG -      osd.6 observed slow operation indications in BlueStore
2025-04-15 11:12:42,586 - cephci - ceph:1186 - DEBUG - [WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
2025-04-15 11:12:42,586 - cephci - ceph:1186 - DEBUG -     daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
2025-04-15 11:12:42,586 - cephci - ceph:1186 - DEBUG - [ERR] OSD_SCRUB_ERRORS: 18 scrub errors
2025-04-15 11:12:42,586 - cephci - ceph:1186 - DEBUG - [ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent
2025-04-15 11:12:42,587 - cephci - ceph:1186 - DEBUG -     pg 14.0 is active+clean+inconsistent, acting [13,8,3]
2025-04-15 11:12:42,587 - cephci - shell:64 - DEBUG - HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 18 scrub errors; Possible data damage: 1 pg inconsistent
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state
[ERR] OSD_SCRUB_ERRORS: 18 scrub errors
[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent
    pg 14.0 is active+clean+inconsistent, acting [13,8,3]

2025-04-15 11:12:42,587 - cephci - core_workflows:3317 - INFO - Health warning on cluster: 
 ('HEALTH_ERR 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s); 18 scrub errors; Possible data damage: 1 pg inconsistent\n[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore\n     osd.6 observed slow operation indications in BlueStore\n[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)\n    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in unknown state\n[ERR] OSD_SCRUB_ERRORS: 18 scrub errors\n[ERR] PG_DAMAGED: Possible data damage: 1 pg inconsistent\n    pg 14.0 is active+clean+inconsistent, acting [13,8,3]\n', "Inferring fsid 79ad3a88-19c4-11f0-bea5-fa163ea10f51\nInferring config /var/lib/ceph/79ad3a88-19c4-11f0-bea5-fa163ea10f51/mon.ceph-regression-juxejq-zv217a-node1-installer/config\nUsing ceph image with id '3b1483211cc8' and tag '8-110' created on 2025-04-11 13:52:17 +0000 UTC\ncp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6\n") 


2025-04-15 11:12:42,595 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 11:12:43,599 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.004119 seconds
2025-04-15 11:12:43,601 - cephci - core_workflows:3325 - INFO - pg inconsistent generated in the cluster
2025-04-15 11:12:43,602 - cephci - core_workflows:3408 - INFO - The inconsistent object is created in the pg: 14.0
2025-04-15 11:12:43,602 - cephci - test_osd_replicated_inconsistency_scenario:301 - INFO - The 4 objects are converted into inconsistent objects
2025-04-15 11:12:43,602 - cephci - test_osd_replicated_inconsistency_scenario:303 - INFO - The inconsistent count on the replicated pool is - 4
2025-04-15 11:12:43,603 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd debug_osd 20/20 on 10.0.195.79
2025-04-15 11:12:45,081 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd debug_osd 20/20 on 10.0.195.79 took 1.477575 seconds
2025-04-15 11:12:45,081 - cephci - shell:64 - DEBUG - 
2025-04-15 11:12:55,091 - cephci - monitor_configurations:207 - DEBUG - verifying the value set
2025-04-15 11:12:55,094 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 11:12:56,530 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.436014 seconds
2025-04-15 11:12:56,531 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:12:56,531 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 11:12:56,532 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 11:12:56,532 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"debug_osd","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mc
2025-04-15 11:12:56,533 - cephci - ceph:1186 - DEBUG - lock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 11:12:56,533 - cephci - monitor_configurations:349 - INFO - Verified the value set for the config: debug_osd
2025-04-15 11:12:56,533 - cephci - monitor_configurations:212 - INFO - Value for config: debug_osd was set
2025-04-15 11:12:56,572 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set mgr debug_mgr 20/20 on 10.0.195.79
2025-04-15 11:12:58,089 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set mgr debug_mgr 20/20 on 10.0.195.79 took 1.516282 seconds
2025-04-15 11:12:58,090 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:08,100 - cephci - monitor_configurations:207 - DEBUG - verifying the value set
2025-04-15 11:13:08,102 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 11:13:09,601 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.49858 seconds
2025-04-15 11:13:09,602 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:13:09,602 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"debug_mgr","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false
2025-04-15 11:13:09,602 - cephci - ceph:1186 - DEBUG - ,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/nginx-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advan
2025-04-15 11:13:09,602 - cephci - ceph:1186 - DEBUG - ced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"debug_osd","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd",
2025-04-15 11:13:09,603 - cephci - ceph:1186 - DEBUG - "value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 11:13:09,603 - cephci - monitor_configurations:349 - INFO - Verified the value set for the config: debug_mgr
2025-04-15 11:13:09,603 - cephci - monitor_configurations:212 - INFO - Value for config: debug_mgr was set
2025-04-15 11:13:09,603 - cephci - test_osd_replicated_inconsistency_scenario:77 - INFO - The inconsistent object count is- 4 on pg -14.0
2025-04-15 11:13:09,643 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph -s -f json on 10.0.195.79
2025-04-15 11:13:11,239 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph -s -f json on 10.0.195.79 took 1.596026 seconds
2025-04-15 11:13:11,240 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:13:11,240 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51","health":{"status":"HEALTH_ERR","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"1 OSD(s) experiencing slow operations in BlueStore","count":1},"muted":false},"CEPHADM_FAILED_DAEMON":{"severity":"HEALTH_WARN","summary":{"message":"1 failed cephadm daemon(s)","count":1},"muted":false},"OSD_SCRUB_ERRORS":{"severity":"HEALTH_ERR","summary":{"message":"18 scrub errors","count":18},"muted":false},"PG_DAMAGED":{"severity":"HEALTH_ERR","summary":{"message":"Possible data damage: 1 pg inconsistent","count":1},"muted":false}},"mutes":[]},"election_epoch":14,"quorum":[0,1,2],"quorum_names":["ceph-regression-juxejq-zv217a-node1-installer","ceph-regression-juxejq-zv217a-node2","ceph-regression-juxejq-zv217a-node6"],"quorum_age":15870,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":1538,"num_osds":15,"num_up_osds":14,"osd_up_since":1744715474,"num_in_osds":14,"osd_in_since":1744708780,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":689},{"state_name":"active+clean+inconsistent","count":1}],"num_pgs":690,"num_pools":9,"num_objects":269,"data_bytes":467441,"bytes_used":10042277888,"bytes_avail":365708640256,"bytes_total":375750918144},"fsmap":{"epoch":15,"btime":"2025-04-15T06:58:09:825897+0000","id":1,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":1,"rank":0,"name":"cephfs.ceph-regression-juxejq-zv217a-node2.piqpns","status":"up:active","gid":24460}],"up:standby":1},"mgrmap":{"available":true,"num_standbys":2,"modules":["cephadm","dashboard","iostat","nfs","prometheus","smb"],"services":{"dashboard":"https://10.0.195.79:8443/","prometheus":"http://10.0.195.79:9283/"}},"servicemap":{"epoch":281,"modified":"2025-04-15T11:12:15.720122+0000","services":{"mgr":{"daemons":{"summary":"","ceph-regression-juxejq-zv217a-node1-installer.odtafx":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}},"mon":{"daemon
2025-04-15 11:13:11,240 - cephci - ceph:1186 - DEBUG - s":{"summary":"","ceph-regression-juxejq-zv217a-node1-installer":{"start_epoch":0,"start_stamp":"0.000000","gid":0,"addr":"(unrecognized address family 0)/0","metadata":{},"task_status":{}}}},"rgw":{"daemons":{"summary":"","24478":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.061555+0000","gid":24478,"addr":"10.0.195.138:0/3807925065","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node2","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)","distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node2","id":"rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868476","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}},"34526":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.103709+0000","gid":34526,"addr":"10.0.195.171:0/2816701951","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node6","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)"
2025-04-15 11:13:11,240 - cephci - ceph:1186 - DEBUG - ,"distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node6","id":"rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868484","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}}}}}},"progress_events":{}}
2025-04-15 11:13:11,281 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_auto_repair_num_errors on 10.0.195.79
2025-04-15 11:13:12,765 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_auto_repair_num_errors on 10.0.195.79 took 1.483901 seconds
2025-04-15 11:13:12,766 - cephci - ceph:1186 - DEBUG - 5
2025-04-15 11:13:12,766 - cephci - shell:64 - DEBUG - 5

2025-04-15 11:13:12,766 - cephci - test_osd_replicated_inconsistency_scenario:88 - INFO - Original value of osd_scrub_auto_repair_num_errors is 5
2025-04-15 11:13:12,807 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_auto_repair on 10.0.195.79
2025-04-15 11:13:14,268 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_auto_repair on 10.0.195.79 took 1.461288 seconds
2025-04-15 11:13:14,269 - cephci - ceph:1186 - DEBUG - false
2025-04-15 11:13:14,269 - cephci - shell:64 - DEBUG - false

2025-04-15 11:13:14,269 - cephci - test_osd_replicated_inconsistency_scenario:100 - INFO - Original value of osd_scrub_auto_repair is false
2025-04-15 11:13:14,269 - cephci - test_osd_replicated_inconsistency_scenario:106 - INFO - Test scenario1: scrub error count greater than the osd_scrub_auto_repair_num_errors count
2025-04-15 11:13:14,310 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_scrub_auto_repair_num_errors 17 on 10.0.195.79
2025-04-15 11:13:15,787 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_scrub_auto_repair_num_errors 17 on 10.0.195.79 took 1.476498 seconds
2025-04-15 11:13:15,788 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:25,798 - cephci - monitor_configurations:207 - DEBUG - verifying the value set
2025-04-15 11:13:25,799 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 11:13:28,264 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 2.464442 seconds
2025-04-15 11:13:28,265 - cephci - monitor_configurations:349 - INFO - Verified the value set for the config: osd_scrub_auto_repair_num_errors
2025-04-15 11:13:28,265 - cephci - monitor_configurations:212 - INFO - Value for config: osd_scrub_auto_repair_num_errors was set
2025-04-15 11:13:28,265 - cephci - test_osd_replicated_inconsistency_scenario:114 - INFO - The osd_scrub_auto_repair_num_errors value is set to 17
2025-04-15 11:13:28,266 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_scrub_auto_repair true on 10.0.195.79
2025-04-15 11:13:30,850 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_scrub_auto_repair true on 10.0.195.79 took 2.583153 seconds
2025-04-15 11:13:30,850 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:40,860 - cephci - monitor_configurations:207 - DEBUG - verifying the value set
2025-04-15 11:13:40,863 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 11:13:42,390 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.527279 seconds
2025-04-15 11:13:42,391 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:13:42,391 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"debug_mgr","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false
2025-04-15 11:13:42,391 - cephci - ceph:1186 - DEBUG - ,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/nginx-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advan
2025-04-15 11:13:42,392 - cephci - ceph:1186 - DEBUG - ced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"debug_osd","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair_num_errors","value":"17","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","lev
2025-04-15 11:13:42,392 - cephci - ceph:1186 - DEBUG - el":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 11:13:42,392 - cephci - monitor_configurations:349 - INFO - Verified the value set for the config: osd_scrub_auto_repair
2025-04-15 11:13:42,393 - cephci - monitor_configurations:212 - INFO - Value for config: osd_scrub_auto_repair was set
2025-04-15 11:13:42,393 - cephci - test_osd_replicated_inconsistency_scenario:119 - INFO - The osd_scrub_auto_repair value is set to true
2025-04-15 11:13:42,393 - cephci - stretch_cluster:373 - DEBUG - Updating recovery thread and osd_op_queue to assist faster recovery
2025-04-15 11:13:42,432 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79
2025-04-15 11:13:43,841 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79 took 1.408553 seconds
2025-04-15 11:13:43,842 - cephci - ceph:1186 - DEBUG - mclock_scheduler
2025-04-15 11:13:43,842 - cephci - shell:64 - DEBUG - mclock_scheduler

2025-04-15 11:13:43,883 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79
2025-04-15 11:13:45,326 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79 took 1.442818 seconds
2025-04-15 11:13:45,327 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:45,369 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_max_backfills 18 on 10.0.195.79
2025-04-15 11:13:46,913 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_max_backfills 18 on 10.0.195.79 took 1.543375 seconds
2025-04-15 11:13:46,914 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:46,955 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_recovery_max_active 18 on 10.0.195.79
2025-04-15 11:13:49,523 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_recovery_max_active 18 on 10.0.195.79 took 2.567723 seconds
2025-04-15 11:13:49,524 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:49,526 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 11:13:50,530 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.004269 seconds
2025-04-15 11:13:50,532 - cephci - stretch_cluster:425 - INFO - Waiting for active + clean. Active alerts: dict_keys(['BLUESTORE_SLOW_OP_ALERT', 'CEPHADM_FAILED_DAEMON', 'OSD_SCRUB_ERRORS', 'PG_DAMAGED']), PG States: [{'state': 'active+clean+inconsistent', 'num': 1}, {'state': 'active+clean', 'num': 689}]. Checking status again in 120 seconds
2025-04-15 11:13:50,533 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.240
2025-04-15 11:13:51,537 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.240 took 1.003409 seconds
2025-04-15 11:13:51,537 - cephci - stretch_cluster:430 - INFO - 
ceph status : {'fsid': '79ad3a88-19c4-11f0-bea5-fa163ea10f51', 'health': {'status': 'HEALTH_ERR', 'checks': {'BLUESTORE_SLOW_OP_ALERT': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 OSD(s) experiencing slow operations in BlueStore', 'count': 1}, 'muted': False}, 'CEPHADM_FAILED_DAEMON': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 failed cephadm daemon(s)', 'count': 1}, 'muted': False}, 'OSD_SCRUB_ERRORS': {'severity': 'HEALTH_ERR', 'summary': {'message': '18 scrub errors', 'count': 18}, 'muted': False}, 'PG_DAMAGED': {'severity': 'HEALTH_ERR', 'summary': {'message': 'Possible data damage: 1 pg inconsistent', 'count': 1}, 'muted': False}}, 'mutes': []}, 'election_epoch': 14, 'quorum': [0, 1, 2], 'quorum_names': ['ceph-regression-juxejq-zv217a-node1-installer', 'ceph-regression-juxejq-zv217a-node2', 'ceph-regression-juxejq-zv217a-node6'], 'quorum_age': 15910, 'monmap': {'epoch': 3, 'min_mon_release_name': 'squid', 'num_mons': 3}, 'osdmap': {'epoch': 1538, 'num_osds': 15, 'num_up_osds': 14, 'osd_up_since': 1744715474, 'num_in_osds': 14, 'osd_in_since': 1744708780, 'num_remapped_pgs': 0}, 'pgmap': {'pgs_by_state': [{'state_name': 'active+clean', 'count': 689}, {'state_name': 'active+clean+inconsistent', 'count': 1}], 'num_pgs': 690, 'num_pools': 9, 'num_objects': 269, 'data_bytes': 467441, 'bytes_used': 10042277888, 'bytes_avail': 365708640256, 'bytes_total': 375750918144}, 'fsmap': {'epoch': 15, 'btime': '2025-04-15T06:58:09:825897+0000', 'id': 1, 'up': 1, 'in': 1, 'max': 1, 'by_rank': [{'filesystem_id': 1, 'rank': 0, 'name': 'cephfs.ceph-regression-juxejq-zv217a-node2.piqpns', 'status': 'up:active', 'gid': 24460}], 'up:standby': 1}, 'mgrmap': {'available': True, 'num_standbys': 2, 'modules': ['cephadm', 'dashboard', 'iostat', 'nfs', 'prometheus', 'smb'], 'services': {'dashboard': 'https://10.0.195.79:8443/', 'prometheus': 'http://10.0.195.79:9283/'}}, 'servicemap': {'epoch': 282, 'modified': '2025-04-15T11:12:51.764249+0000', 'services': {'rgw': {'daemons': {'summary': '', '24478': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.061555+0000', 'gid': 24478, 'addr': '10.0.195.138:0/3807925065', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node2', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node2', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868476', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}, '34526': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.103709+0000', 'gid': 34526, 'addr': '10.0.195.171:0/2816701951', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node6', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node6', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868484', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}}}}}, 'progress_events': {}}

2025-04-15 11:13:51,538 - cephci - stretch_cluster:425 - INFO - Waiting for active + clean. Active alerts: dict_keys(['BLUESTORE_SLOW_OP_ALERT', 'CEPHADM_FAILED_DAEMON', 'OSD_SCRUB_ERRORS', 'PG_DAMAGED']), PG States: [{'state': 'active+clean+inconsistent', 'num': 1}, {'state': 'active+clean', 'num': 689}]. Checking status again in 120 seconds
2025-04-15 11:13:51,539 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.240
2025-04-15 11:13:52,543 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.240 took 1.00343 seconds
2025-04-15 11:13:52,543 - cephci - stretch_cluster:430 - INFO - 
ceph status : {'fsid': '79ad3a88-19c4-11f0-bea5-fa163ea10f51', 'health': {'status': 'HEALTH_ERR', 'checks': {'BLUESTORE_SLOW_OP_ALERT': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 OSD(s) experiencing slow operations in BlueStore', 'count': 1}, 'muted': False}, 'CEPHADM_FAILED_DAEMON': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 failed cephadm daemon(s)', 'count': 1}, 'muted': False}, 'OSD_SCRUB_ERRORS': {'severity': 'HEALTH_ERR', 'summary': {'message': '18 scrub errors', 'count': 18}, 'muted': False}, 'PG_DAMAGED': {'severity': 'HEALTH_ERR', 'summary': {'message': 'Possible data damage: 1 pg inconsistent', 'count': 1}, 'muted': False}}, 'mutes': []}, 'election_epoch': 14, 'quorum': [0, 1, 2], 'quorum_names': ['ceph-regression-juxejq-zv217a-node1-installer', 'ceph-regression-juxejq-zv217a-node2', 'ceph-regression-juxejq-zv217a-node6'], 'quorum_age': 15911, 'monmap': {'epoch': 3, 'min_mon_release_name': 'squid', 'num_mons': 3}, 'osdmap': {'epoch': 1538, 'num_osds': 15, 'num_up_osds': 14, 'osd_up_since': 1744715474, 'num_in_osds': 14, 'osd_in_since': 1744708780, 'num_remapped_pgs': 0}, 'pgmap': {'pgs_by_state': [{'state_name': 'active+clean', 'count': 689}, {'state_name': 'active+clean+inconsistent', 'count': 1}], 'num_pgs': 690, 'num_pools': 9, 'num_objects': 269, 'data_bytes': 467441, 'bytes_used': 10042277888, 'bytes_avail': 365708640256, 'bytes_total': 375750918144}, 'fsmap': {'epoch': 15, 'btime': '2025-04-15T06:58:09:825897+0000', 'id': 1, 'up': 1, 'in': 1, 'max': 1, 'by_rank': [{'filesystem_id': 1, 'rank': 0, 'name': 'cephfs.ceph-regression-juxejq-zv217a-node2.piqpns', 'status': 'up:active', 'gid': 24460}], 'up:standby': 1}, 'mgrmap': {'available': True, 'num_standbys': 2, 'modules': ['cephadm', 'dashboard', 'iostat', 'nfs', 'prometheus', 'smb'], 'services': {'dashboard': 'https://10.0.195.79:8443/', 'prometheus': 'http://10.0.195.79:9283/'}}, 'servicemap': {'epoch': 282, 'modified': '2025-04-15T11:12:51.764249+0000', 'services': {'rgw': {'daemons': {'summary': '', '24478': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.061555+0000', 'gid': 24478, 'addr': '10.0.195.138:0/3807925065', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node2', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node2', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868476', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}, '34526': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.103709+0000', 'gid': 34526, 'addr': '10.0.195.171:0/2816701951', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node6', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node6', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868484', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}}}}}, 'progress_events': {}}

2025-04-15 11:13:52,544 - cephci - stretch_cluster:438 - DEBUG - Removing recovery thread settings
2025-04-15 11:13:52,545 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79
2025-04-15 11:13:54,024 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79 took 1.478613 seconds
2025-04-15 11:13:54,025 - cephci - ceph:1186 - DEBUG - mclock_scheduler
2025-04-15 11:13:54,025 - cephci - shell:64 - DEBUG - mclock_scheduler

2025-04-15 11:13:54,066 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79
2025-04-15 11:13:55,511 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79 took 1.444444 seconds
2025-04-15 11:13:55,512 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:55,553 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_max_backfills on 10.0.195.79
2025-04-15 11:13:57,032 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_max_backfills on 10.0.195.79 took 1.47834 seconds
2025-04-15 11:13:57,032 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:57,073 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_recovery_max_active on 10.0.195.79
2025-04-15 11:13:58,524 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_recovery_max_active on 10.0.195.79 took 1.450417 seconds
2025-04-15 11:13:58,525 - cephci - shell:64 - DEBUG - 
2025-04-15 11:13:58,525 - cephci - stretch_cluster:440 - INFO - The recovery and back-filling of the OSDs/Pool is completed
2025-04-15 11:13:58,525 - cephci - utils:1858 - DEBUG - The <function wait_for_clean_pg_sets at 0x7fbdecd8c1f0> return status is True
2025-04-15 11:13:58,566 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd set nodeep-scrub on 10.0.195.79
2025-04-15 11:14:00,867 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd set nodeep-scrub on 10.0.195.79 took 2.300544 seconds
2025-04-15 11:14:00,867 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:00,867 - cephci - rados_scrub:190 - INFO - The OSD falg nodeep-scrub is set on the cluster.
2025-04-15 11:14:00,908 - cephci - ceph:1576 - INFO - Execute cephadm shell -- date +%Y:%m:%d:%H:%u on 10.0.195.79
2025-04-15 11:14:02,035 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- date +%Y:%m:%d:%H:%u on 10.0.195.79 took 1.127255 seconds
2025-04-15 11:14:02,036 - cephci - ceph:1186 - DEBUG - 2025:04:15:11:2
2025-04-15 11:14:02,036 - cephci - shell:64 - DEBUG - 2025:04:15:11:2

2025-04-15 11:14:02,077 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_begin_hour 11 on 10.0.195.79
2025-04-15 11:14:04,698 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_begin_hour 11 on 10.0.195.79 took 2.620765 seconds
2025-04-15 11:14:04,699 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:04,699 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:04,700 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_begin_hour -f json on 10.0.195.79
2025-04-15 11:14:06,248 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_begin_hour -f json on 10.0.195.79 took 1.547928 seconds
2025-04-15 11:14:06,249 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:14:06,249 - cephci - ceph:1186 - DEBUG - 11
2025-04-15 11:14:06,249 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_begin_hour is set with 11 on osds
2025-04-15 11:14:06,290 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_begin_week_day 2 on 10.0.195.79
2025-04-15 11:14:07,815 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_begin_week_day 2 on 10.0.195.79 took 1.524421 seconds
2025-04-15 11:14:07,816 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:07,816 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:07,857 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_begin_week_day -f json on 10.0.195.79
2025-04-15 11:14:09,343 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_begin_week_day -f json on 10.0.195.79 took 1.485152 seconds
2025-04-15 11:14:09,344 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:14:09,344 - cephci - ceph:1186 - DEBUG - 2
2025-04-15 11:14:09,344 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_begin_week_day is set with 2 on osds
2025-04-15 11:14:09,385 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_end_hour 13 on 10.0.195.79
2025-04-15 11:14:10,942 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_end_hour 13 on 10.0.195.79 took 1.556739 seconds
2025-04-15 11:14:10,943 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:10,943 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:10,984 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_end_hour -f json on 10.0.195.79
2025-04-15 11:14:13,475 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_end_hour -f json on 10.0.195.79 took 2.49039 seconds
2025-04-15 11:14:13,475 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_end_hour is set with 13 on osds
2025-04-15 11:14:13,477 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_end_week_day 2 on 10.0.195.79
2025-04-15 11:14:15,070 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_end_week_day 2 on 10.0.195.79 took 1.592792 seconds
2025-04-15 11:14:15,070 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:15,071 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:15,112 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_end_week_day -f json on 10.0.195.79
2025-04-15 11:14:16,697 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_end_week_day -f json on 10.0.195.79 took 1.58469 seconds
2025-04-15 11:14:16,697 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:14:16,697 - cephci - ceph:1186 - DEBUG - 2
2025-04-15 11:14:16,697 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_end_week_day is set with 2 on osds
2025-04-15 11:14:16,739 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_min_interval 5 on 10.0.195.79
2025-04-15 11:14:18,292 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_min_interval 5 on 10.0.195.79 took 1.552654 seconds
2025-04-15 11:14:18,292 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:18,293 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:18,333 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_min_interval -f json on 10.0.195.79
2025-04-15 11:14:20,788 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_min_interval -f json on 10.0.195.79 took 2.453382 seconds
2025-04-15 11:14:20,788 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_min_interval is set with 5 on osds
2025-04-15 11:14:20,790 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_max_interval 1500 on 10.0.195.79
2025-04-15 11:14:22,324 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_max_interval 1500 on 10.0.195.79 took 1.533885 seconds
2025-04-15 11:14:22,325 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:22,325 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:22,366 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_max_interval -f json on 10.0.195.79
2025-04-15 11:14:23,899 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_max_interval -f json on 10.0.195.79 took 1.532118 seconds
2025-04-15 11:14:23,900 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:14:23,900 - cephci - ceph:1186 - DEBUG - 1500.000000
2025-04-15 11:14:23,900 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_max_interval is set with 1500 on osds
2025-04-15 11:14:23,941 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_deep_scrub_interval 2400 on 10.0.195.79
2025-04-15 11:14:25,562 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_deep_scrub_interval 2400 on 10.0.195.79 took 1.620163 seconds
2025-04-15 11:14:25,563 - cephci - shell:64 - DEBUG - 
2025-04-15 11:14:25,563 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:14:25,603 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_deep_scrub_interval -f json on 10.0.195.79
2025-04-15 11:14:28,124 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_deep_scrub_interval -f json on 10.0.195.79 took 2.520416 seconds
2025-04-15 11:14:28,125 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_deep_scrub_interval is set with 2400 on osds
2025-04-15 11:14:28,125 - cephci - test_osd_ecpool_inconsistency_scenario:447 - DEBUG - Running scrub on pg : 14.0
2025-04-15 11:14:28,126 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:14:29,159 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032335 seconds
2025-04-15 11:14:29,180 - cephci - core_workflows:3420 - INFO - Dumping scrub stats before starting scrub
2025-04-15 11:14:29,180 - cephci - core_workflows:3421 - INFO - last_scrub : 1492'100
2025-04-15 11:14:29,180 - cephci - core_workflows:3422 - INFO - last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:14:29,180 - cephci - core_workflows:3434 - DEBUG - Initiated scheduled scrub
2025-04-15 11:14:29,182 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:14:30,213 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031097 seconds
2025-04-15 11:14:30,234 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:14:30,235 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:14:30,235 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:14:30,235 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:15:00,262 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:15:01,293 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030618 seconds
2025-04-15 11:15:01,316 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:15:01,316 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:15:01,316 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:15:01,316 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:15:31,335 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:15:32,366 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030537 seconds
2025-04-15 11:15:32,387 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:15:32,388 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:15:32,388 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:15:32,388 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:16:02,420 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:16:03,451 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031143 seconds
2025-04-15 11:16:03,480 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:16:03,481 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:16:03,481 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:16:03,481 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:16:33,509 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:16:34,546 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036876 seconds
2025-04-15 11:16:34,569 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:16:34,569 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:16:34,569 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:16:34,569 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:17:04,599 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:17:05,638 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03792 seconds
2025-04-15 11:17:05,670 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:17:05,671 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:17:05,671 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:17:05,671 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:17:35,693 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:17:36,725 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031327 seconds
2025-04-15 11:17:36,750 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:17:36,750 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:17:36,750 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:17:36,750 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:18:06,771 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:18:07,806 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034844 seconds
2025-04-15 11:18:07,844 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:18:07,845 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:18:07,845 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:18:07,845 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:18:37,872 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:18:38,904 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03043 seconds
2025-04-15 11:18:38,925 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:18:38,926 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:18:38,926 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:18:38,926 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:19:08,950 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:19:09,985 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034631 seconds
2025-04-15 11:19:10,092 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:19:10,092 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:19:10,093 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:19:10,093 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:19:40,125 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:19:41,156 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030656 seconds
2025-04-15 11:19:41,182 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:19:41,182 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:19:41,182 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:19:41,182 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:20:11,207 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:20:12,240 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030801 seconds
2025-04-15 11:20:12,266 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:20:12,266 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:20:12,266 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:20:12,266 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:20:42,274 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:20:43,304 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.028901 seconds
2025-04-15 11:20:43,325 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:20:43,326 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:20:43,326 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:20:43,326 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:21:13,335 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:21:14,364 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.028748 seconds
2025-04-15 11:21:14,386 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:21:14,387 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:21:14,387 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:21:14,387 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:21:44,419 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:21:45,452 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032029 seconds
2025-04-15 11:21:45,474 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:21:45,474 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:21:45,474 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:21:45,474 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:22:15,498 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:22:16,529 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.0305 seconds
2025-04-15 11:22:16,552 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:22:16,553 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:22:16,553 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:22:16,553 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:22:46,584 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:22:47,615 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030587 seconds
2025-04-15 11:22:47,637 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:22:47,637 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:22:47,637 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:22:47,638 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:23:17,670 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:23:18,700 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029599 seconds
2025-04-15 11:23:18,723 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:23:18,723 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:23:18,723 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:23:18,723 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:23:48,754 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:23:49,787 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032131 seconds
2025-04-15 11:23:49,808 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:23:49,809 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:23:49,809 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:23:49,809 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:24:19,836 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:24:20,868 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030157 seconds
2025-04-15 11:24:20,890 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:24:20,891 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:24:20,891 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:24:20,891 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:24:50,907 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:24:51,938 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030356 seconds
2025-04-15 11:24:51,961 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:24:51,961 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:24:51,961 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:24:51,961 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:25:21,982 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:25:23,013 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030808 seconds
2025-04-15 11:25:23,043 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:25:23,044 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:25:23,044 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:25:23,044 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:25:53,062 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:25:54,096 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033123 seconds
2025-04-15 11:25:54,118 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:25:54,118 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:25:54,118 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:25:54,118 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:26:24,135 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:26:25,168 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032112 seconds
2025-04-15 11:26:25,189 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:26:25,190 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:26:25,190 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:26:25,190 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:26:55,219 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:26:56,250 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030906 seconds
2025-04-15 11:26:56,275 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:26:56,275 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:26:56,275 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:26:56,276 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:27:26,292 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:27:27,323 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030279 seconds
2025-04-15 11:27:27,345 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:27:27,345 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:27:27,345 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:27:27,345 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:27:57,372 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:27:58,402 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030563 seconds
2025-04-15 11:27:58,425 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:27:58,426 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:27:58,426 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:27:58,426 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:28:28,457 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:28:29,490 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032041 seconds
2025-04-15 11:28:29,511 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:28:29,512 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:28:29,512 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:28:29,512 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:28:59,533 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:29:00,563 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029969 seconds
2025-04-15 11:29:00,584 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:29:00,585 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:29:00,585 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:29:00,585 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:29:30,616 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:29:31,647 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029886 seconds
2025-04-15 11:29:31,668 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:29:31,669 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:29:31,669 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:29:31,669 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:30:01,699 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:30:02,733 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033343 seconds
2025-04-15 11:30:02,847 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:30:02,847 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:30:02,847 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:30:02,847 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:30:32,878 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:30:33,911 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032568 seconds
2025-04-15 11:30:33,932 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:30:33,933 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:30:33,933 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:30:33,933 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:31:03,944 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:31:04,976 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03119 seconds
2025-04-15 11:31:04,998 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:31:04,998 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:31:04,998 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:31:04,998 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:31:35,021 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:31:36,052 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030892 seconds
2025-04-15 11:31:36,077 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:31:36,077 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:31:36,077 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:31:36,077 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:32:06,102 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:32:07,132 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029281 seconds
2025-04-15 11:32:07,154 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:32:07,154 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:32:07,154 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:32:07,155 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:32:37,179 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:32:38,209 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030265 seconds
2025-04-15 11:32:38,231 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:32:38,232 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:32:38,232 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:32:38,232 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:33:08,258 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:33:09,290 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030961 seconds
2025-04-15 11:33:09,312 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:33:09,312 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:33:09,312 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:33:09,313 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:33:39,331 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:33:40,363 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030752 seconds
2025-04-15 11:33:40,384 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:33:40,385 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:33:40,385 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:33:40,385 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:34:10,397 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:34:11,428 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030882 seconds
2025-04-15 11:34:11,450 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:34:11,450 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:34:11,450 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:34:11,450 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:34:41,480 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:34:42,512 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032371 seconds
2025-04-15 11:34:42,538 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:34:42,539 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:34:42,539 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:34:42,539 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:35:12,570 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:35:13,601 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029918 seconds
2025-04-15 11:35:13,622 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:35:13,622 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:35:13,623 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:35:13,623 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:35:43,652 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:35:44,683 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029719 seconds
2025-04-15 11:35:44,708 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:35:44,709 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:35:44,709 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:35:44,709 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:36:14,741 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:36:15,776 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034166 seconds
2025-04-15 11:36:15,805 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:36:15,805 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:36:15,805 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:36:15,805 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:36:45,830 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:36:46,860 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029186 seconds
2025-04-15 11:36:46,881 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:36:46,882 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:36:46,882 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:36:46,882 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:37:16,901 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:37:17,930 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.0291 seconds
2025-04-15 11:37:17,951 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:37:17,952 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:37:17,952 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:37:17,952 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:37:47,980 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:37:49,011 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030493 seconds
2025-04-15 11:37:49,033 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:37:49,033 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:37:49,033 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:37:49,034 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:38:19,063 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:38:20,095 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032198 seconds
2025-04-15 11:38:20,117 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:38:20,117 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:38:20,118 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:38:20,118 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:38:50,148 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:38:51,179 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03053 seconds
2025-04-15 11:38:51,287 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:38:51,297 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:38:51,298 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:38:51,298 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:39:21,327 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:39:22,359 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031119 seconds
2025-04-15 11:39:22,381 - cephci - core_workflows:3454 - DEBUG - Current last_scrub: 1492'100
2025-04-15 11:39:22,382 - cephci - core_workflows:3455 - DEBUG - Current last_scrub_stamp: 2025-04-15T11:11:36.108183+0000
2025-04-15 11:39:22,382 - cephci - core_workflows:3458 - DEBUG - Total time elapsed since starting scrub on PG: 14.0 is 0:00:00
2025-04-15 11:39:22,382 - cephci - core_workflows:3462 - INFO - scrub is yet to complete, pg state: active+clean+inconsistent. Sleeping for 30 secs
2025-04-15 11:39:52,408 - cephci - core_workflows:3467 - ERROR - PG :14.0 could not be scrubbed in time
2025-04-15 11:39:52,410 - cephci - test_osd_replicated_inconsistency_scenario:126 - INFO - Objects not scrubbed error
2025-04-15 11:39:52,411 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79
2025-04-15 11:39:54,463 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79 took 2.051548 seconds
2025-04-15 11:39:54,464 - cephci - shell:64 - DEBUG - 
2025-04-15 11:39:54,464 - cephci - rados_scrub:190 - INFO - The OSD falg nodeep-scrub is unset on the cluster.
2025-04-15 11:39:54,505 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph -s -f json on 10.0.195.79
2025-04-15 11:39:56,069 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph -s -f json on 10.0.195.79 took 1.563809 seconds
2025-04-15 11:39:56,070 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:39:56,070 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51","health":{"status":"HEALTH_ERR","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"1 OSD(s) experiencing slow operations in BlueStore","count":1},"muted":false},"CEPHADM_FAILED_DAEMON":{"severity":"HEALTH_WARN","summary":{"message":"1 failed cephadm daemon(s)","count":1},"muted":false},"OSD_SCRUB_ERRORS":{"severity":"HEALTH_ERR","summary":{"message":"18 scrub errors","count":18},"muted":false},"PG_DAMAGED":{"severity":"HEALTH_ERR","summary":{"message":"Possible data damage: 1 pg inconsistent","count":1},"muted":false}},"mutes":[]},"election_epoch":14,"quorum":[0,1,2],"quorum_names":["ceph-regression-juxejq-zv217a-node1-installer","ceph-regression-juxejq-zv217a-node2","ceph-regression-juxejq-zv217a-node6"],"quorum_age":17475,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":2893,"num_osds":15,"num_up_osds":14,"osd_up_since":1744715474,"num_in_osds":14,"osd_in_since":1744708780,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":689},{"state_name":"active+clean+inconsistent","count":1}],"num_pgs":690,"num_pools":9,"num_objects":269,"data_bytes":467441,"bytes_used":11149058048,"bytes_avail":364601860096,"bytes_total":375750918144},"fsmap":{"epoch":15,"btime":"2025-04-15T06:58:09:825897+0000","id":1,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":1,"rank":0,"name":"cephfs.ceph-regression-juxejq-zv217a-node2.piqpns","status":"up:active","gid":24460}],"up:standby":1},"mgrmap":{"available":true,"num_standbys":2,"modules":["cephadm","dashboard","iostat","nfs","prometheus","smb"],"services":{"dashboard":"https://10.0.195.79:8443/","prometheus":"http://10.0.195.79:9283/"}},"servicemap":{"epoch":308,"modified":"2025-04-15T11:37:45.096150+0000","services":{"rgw":{"daemons":{"summary":"","24478":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.061555+0000","gid":24478,"addr":"10.0.195.138:0/3807925065","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph ver
2025-04-15 11:39:56,070 - cephci - ceph:1186 - DEBUG - sion 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node2","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)","distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node2","id":"rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868476","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}},"34526":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.103709+0000","gid":34526,"addr":"10.0.195.171:0/2816701951","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node6","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)","distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node6","id":"rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868484","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_
2025-04-15 11:39:56,071 - cephci - ceph:1186 - DEBUG - name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}}}}}},"progress_events":{}}
2025-04-15 11:39:56,071 - cephci - test_osd_replicated_inconsistency_scenario:135 - INFO - Test scenario1: scrub error count > osd_scrub_auto_repair_num_errors,errors are not repaired
2025-04-15 11:39:56,071 - cephci - stretch_cluster:373 - DEBUG - Updating recovery thread and osd_op_queue to assist faster recovery
2025-04-15 11:39:56,111 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79
2025-04-15 11:39:57,680 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79 took 1.568566 seconds
2025-04-15 11:39:57,680 - cephci - ceph:1186 - DEBUG - mclock_scheduler
2025-04-15 11:39:57,681 - cephci - shell:64 - DEBUG - mclock_scheduler

2025-04-15 11:39:57,722 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79
2025-04-15 11:40:00,349 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79 took 2.626747 seconds
2025-04-15 11:40:00,350 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:00,351 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_max_backfills 18 on 10.0.195.79
2025-04-15 11:40:01,865 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_max_backfills 18 on 10.0.195.79 took 1.513802 seconds
2025-04-15 11:40:01,865 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:01,907 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_recovery_max_active 18 on 10.0.195.79
2025-04-15 11:40:03,719 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_recovery_max_active 18 on 10.0.195.79 took 1.811798 seconds
2025-04-15 11:40:03,720 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:03,722 - cephci - ceph:1576 - INFO - Execute ceph report -f json on 10.0.195.240
2025-04-15 11:40:04,727 - cephci - ceph:1606 - INFO - Execution of ceph report -f json on 10.0.195.240 took 1.004184 seconds
2025-04-15 11:40:04,729 - cephci - stretch_cluster:425 - INFO - Waiting for active + clean. Active alerts: dict_keys(['BLUESTORE_SLOW_OP_ALERT', 'CEPHADM_FAILED_DAEMON', 'OSD_SCRUB_ERRORS', 'PG_DAMAGED']), PG States: [{'state': 'active+clean+inconsistent', 'num': 1}, {'state': 'active+clean+scrubbing+deep', 'num': 1}, {'state': 'active+clean', 'num': 688}]. Checking status again in 120 seconds
2025-04-15 11:40:04,730 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.240
2025-04-15 11:40:05,734 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.240 took 1.00371 seconds
2025-04-15 11:40:05,735 - cephci - stretch_cluster:430 - INFO - 
ceph status : {'fsid': '79ad3a88-19c4-11f0-bea5-fa163ea10f51', 'health': {'status': 'HEALTH_ERR', 'checks': {'BLUESTORE_SLOW_OP_ALERT': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 OSD(s) experiencing slow operations in BlueStore', 'count': 1}, 'muted': False}, 'CEPHADM_FAILED_DAEMON': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 failed cephadm daemon(s)', 'count': 1}, 'muted': False}, 'OSD_SCRUB_ERRORS': {'severity': 'HEALTH_ERR', 'summary': {'message': '18 scrub errors', 'count': 18}, 'muted': False}, 'PG_DAMAGED': {'severity': 'HEALTH_ERR', 'summary': {'message': 'Possible data damage: 1 pg inconsistent', 'count': 1}, 'muted': False}}, 'mutes': []}, 'election_epoch': 14, 'quorum': [0, 1, 2], 'quorum_names': ['ceph-regression-juxejq-zv217a-node1-installer', 'ceph-regression-juxejq-zv217a-node2', 'ceph-regression-juxejq-zv217a-node6'], 'quorum_age': 17485, 'monmap': {'epoch': 3, 'min_mon_release_name': 'squid', 'num_mons': 3}, 'osdmap': {'epoch': 2901, 'num_osds': 15, 'num_up_osds': 14, 'osd_up_since': 1744715474, 'num_in_osds': 14, 'osd_in_since': 1744708780, 'num_remapped_pgs': 0}, 'pgmap': {'pgs_by_state': [{'state_name': 'active+clean', 'count': 689}, {'state_name': 'active+clean+inconsistent', 'count': 1}], 'num_pgs': 690, 'num_pools': 9, 'num_objects': 269, 'data_bytes': 467441, 'bytes_used': 11156746240, 'bytes_avail': 364594171904, 'bytes_total': 375750918144}, 'fsmap': {'epoch': 15, 'btime': '2025-04-15T06:58:09:825897+0000', 'id': 1, 'up': 1, 'in': 1, 'max': 1, 'by_rank': [{'filesystem_id': 1, 'rank': 0, 'name': 'cephfs.ceph-regression-juxejq-zv217a-node2.piqpns', 'status': 'up:active', 'gid': 24460}], 'up:standby': 1}, 'mgrmap': {'available': True, 'num_standbys': 2, 'modules': ['cephadm', 'dashboard', 'iostat', 'nfs', 'prometheus', 'smb'], 'services': {'dashboard': 'https://10.0.195.79:8443/', 'prometheus': 'http://10.0.195.79:9283/'}}, 'servicemap': {'epoch': 308, 'modified': '2025-04-15T11:37:45.096150+0000', 'services': {'rgw': {'daemons': {'summary': '', '24478': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.061555+0000', 'gid': 24478, 'addr': '10.0.195.138:0/3807925065', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node2', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node2', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868476', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}, '34526': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.103709+0000', 'gid': 34526, 'addr': '10.0.195.171:0/2816701951', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node6', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node6', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868484', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}}}}}, 'progress_events': {}}

2025-04-15 11:40:05,735 - cephci - stretch_cluster:425 - INFO - Waiting for active + clean. Active alerts: dict_keys(['BLUESTORE_SLOW_OP_ALERT', 'CEPHADM_FAILED_DAEMON', 'OSD_SCRUB_ERRORS', 'PG_DAMAGED']), PG States: [{'state': 'active+clean+inconsistent', 'num': 1}, {'state': 'active+clean+scrubbing+deep', 'num': 1}, {'state': 'active+clean', 'num': 688}]. Checking status again in 120 seconds
2025-04-15 11:40:05,737 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.240
2025-04-15 11:40:06,741 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.240 took 1.003956 seconds
2025-04-15 11:40:06,743 - cephci - stretch_cluster:430 - INFO - 
ceph status : {'fsid': '79ad3a88-19c4-11f0-bea5-fa163ea10f51', 'health': {'status': 'HEALTH_ERR', 'checks': {'BLUESTORE_SLOW_OP_ALERT': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 OSD(s) experiencing slow operations in BlueStore', 'count': 1}, 'muted': False}, 'CEPHADM_FAILED_DAEMON': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 failed cephadm daemon(s)', 'count': 1}, 'muted': False}, 'OSD_SCRUB_ERRORS': {'severity': 'HEALTH_ERR', 'summary': {'message': '18 scrub errors', 'count': 18}, 'muted': False}, 'PG_DAMAGED': {'severity': 'HEALTH_ERR', 'summary': {'message': 'Possible data damage: 1 pg inconsistent', 'count': 1}, 'muted': False}}, 'mutes': []}, 'election_epoch': 14, 'quorum': [0, 1, 2], 'quorum_names': ['ceph-regression-juxejq-zv217a-node1-installer', 'ceph-regression-juxejq-zv217a-node2', 'ceph-regression-juxejq-zv217a-node6'], 'quorum_age': 17486, 'monmap': {'epoch': 3, 'min_mon_release_name': 'squid', 'num_mons': 3}, 'osdmap': {'epoch': 2902, 'num_osds': 15, 'num_up_osds': 14, 'osd_up_since': 1744715474, 'num_in_osds': 14, 'osd_in_since': 1744708780, 'num_remapped_pgs': 0}, 'pgmap': {'pgs_by_state': [{'state_name': 'active+clean', 'count': 689}, {'state_name': 'active+clean+inconsistent', 'count': 1}], 'num_pgs': 690, 'num_pools': 9, 'num_objects': 269, 'data_bytes': 467441, 'bytes_used': 11156746240, 'bytes_avail': 364594171904, 'bytes_total': 375750918144}, 'fsmap': {'epoch': 15, 'btime': '2025-04-15T06:58:09:825897+0000', 'id': 1, 'up': 1, 'in': 1, 'max': 1, 'by_rank': [{'filesystem_id': 1, 'rank': 0, 'name': 'cephfs.ceph-regression-juxejq-zv217a-node2.piqpns', 'status': 'up:active', 'gid': 24460}], 'up:standby': 1}, 'mgrmap': {'available': True, 'num_standbys': 2, 'modules': ['cephadm', 'dashboard', 'iostat', 'nfs', 'prometheus', 'smb'], 'services': {'dashboard': 'https://10.0.195.79:8443/', 'prometheus': 'http://10.0.195.79:9283/'}}, 'servicemap': {'epoch': 308, 'modified': '2025-04-15T11:37:45.096150+0000', 'services': {'rgw': {'daemons': {'summary': '', '24478': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.061555+0000', 'gid': 24478, 'addr': '10.0.195.138:0/3807925065', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node2', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node2', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868476', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}, '34526': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.103709+0000', 'gid': 34526, 'addr': '10.0.195.171:0/2816701951', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node6', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node6', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868484', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}}}}}, 'progress_events': {}}

2025-04-15 11:40:06,744 - cephci - stretch_cluster:425 - INFO - Waiting for active + clean. Active alerts: dict_keys(['BLUESTORE_SLOW_OP_ALERT', 'CEPHADM_FAILED_DAEMON', 'OSD_SCRUB_ERRORS', 'PG_DAMAGED']), PG States: [{'state': 'active+clean+inconsistent', 'num': 1}, {'state': 'active+clean+scrubbing+deep', 'num': 1}, {'state': 'active+clean', 'num': 688}]. Checking status again in 120 seconds
2025-04-15 11:40:06,745 - cephci - ceph:1576 - INFO - Execute ceph -s -f json on 10.0.195.240
2025-04-15 11:40:07,750 - cephci - ceph:1606 - INFO - Execution of ceph -s -f json on 10.0.195.240 took 1.004069 seconds
2025-04-15 11:40:07,750 - cephci - stretch_cluster:430 - INFO - 
ceph status : {'fsid': '79ad3a88-19c4-11f0-bea5-fa163ea10f51', 'health': {'status': 'HEALTH_WARN', 'checks': {'BLUESTORE_SLOW_OP_ALERT': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 OSD(s) experiencing slow operations in BlueStore', 'count': 1}, 'muted': False}, 'CEPHADM_FAILED_DAEMON': {'severity': 'HEALTH_WARN', 'summary': {'message': '1 failed cephadm daemon(s)', 'count': 1}, 'muted': False}}, 'mutes': []}, 'election_epoch': 14, 'quorum': [0, 1, 2], 'quorum_names': ['ceph-regression-juxejq-zv217a-node1-installer', 'ceph-regression-juxejq-zv217a-node2', 'ceph-regression-juxejq-zv217a-node6'], 'quorum_age': 17487, 'monmap': {'epoch': 3, 'min_mon_release_name': 'squid', 'num_mons': 3}, 'osdmap': {'epoch': 2903, 'num_osds': 15, 'num_up_osds': 14, 'osd_up_since': 1744715474, 'num_in_osds': 14, 'osd_in_since': 1744708780, 'num_remapped_pgs': 0}, 'pgmap': {'pgs_by_state': [{'state_name': 'active+clean', 'count': 690}], 'num_pgs': 690, 'num_pools': 9, 'num_objects': 269, 'data_bytes': 467441, 'bytes_used': 11157204992, 'bytes_avail': 364593713152, 'bytes_total': 375750918144, 'recovering_objects_per_sec': 1, 'recovering_bytes_per_sec': 0, 'recovering_keys_per_sec': 195, 'num_objects_recovered': 9, 'num_bytes_recovered': 0, 'num_keys_recovered': 900}, 'fsmap': {'epoch': 15, 'btime': '2025-04-15T06:58:09:825897+0000', 'id': 1, 'up': 1, 'in': 1, 'max': 1, 'by_rank': [{'filesystem_id': 1, 'rank': 0, 'name': 'cephfs.ceph-regression-juxejq-zv217a-node2.piqpns', 'status': 'up:active', 'gid': 24460}], 'up:standby': 1}, 'mgrmap': {'available': True, 'num_standbys': 2, 'modules': ['cephadm', 'dashboard', 'iostat', 'nfs', 'prometheus', 'smb'], 'services': {'dashboard': 'https://10.0.195.79:8443/', 'prometheus': 'http://10.0.195.79:9283/'}}, 'servicemap': {'epoch': 308, 'modified': '2025-04-15T11:37:45.096150+0000', 'services': {'rgw': {'daemons': {'summary': '', '24478': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.061555+0000', 'gid': 24478, 'addr': '10.0.195.138:0/3807925065', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node2', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node2', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868476', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}, '34526': {'start_epoch': 15, 'start_stamp': '2025-04-15T06:56:25.103709+0000', 'gid': 34526, 'addr': '10.0.195.171:0/2816701951', 'metadata': {'arch': 'x86_64', 'ceph_release': 'squid', 'ceph_version': 'ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)', 'ceph_version_short': '19.2.1-126.el9cp', 'container_hostname': 'ceph-regression-juxejq-zv217a-node6', 'container_image': 'cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6', 'cpu': 'AMD EPYC-Rome Processor', 'distro': 'rhel', 'distro_description': 'Red Hat Enterprise Linux 9.5 (Plow)', 'distro_version': '9.5', 'frontend_config#0': 'beast port=80', 'frontend_type#0': 'beast', 'hostname': 'ceph-regression-juxejq-zv217a-node6', 'id': 'rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid', 'kernel_description': '#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025', 'kernel_version': '5.14.0-503.35.1.el9_5.x86_64', 'mem_swap_kb': '0', 'mem_total_kb': '7868484', 'num_handles': '1', 'os': 'Linux', 'pid': '2', 'realm_id': '', 'realm_name': '', 'zone_id': '42e4606c-35ef-44f1-951b-124b81abdf13', 'zone_name': 'default', 'zonegroup_id': 'e671bdd3-530b-40fb-990c-680fe683b65f', 'zonegroup_name': 'default'}, 'task_status': {}}}}}}, 'progress_events': {}}

2025-04-15 11:40:07,751 - cephci - stretch_cluster:438 - DEBUG - Removing recovery thread settings
2025-04-15 11:40:07,752 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79
2025-04-15 11:40:10,373 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_op_queue on 10.0.195.79 took 2.620655 seconds
2025-04-15 11:40:10,374 - cephci - shell:64 - DEBUG - mclock_scheduler

2025-04-15 11:40:10,375 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79
2025-04-15 11:40:12,915 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set osd osd_mclock_override_recovery_settings true on 10.0.195.79 took 2.539884 seconds
2025-04-15 11:40:12,915 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:12,916 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_max_backfills on 10.0.195.79
2025-04-15 11:40:14,468 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_max_backfills on 10.0.195.79 took 1.55093 seconds
2025-04-15 11:40:14,468 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:14,510 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_recovery_max_active on 10.0.195.79
2025-04-15 11:40:17,061 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_recovery_max_active on 10.0.195.79 took 2.550325 seconds
2025-04-15 11:40:17,062 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:17,062 - cephci - stretch_cluster:440 - INFO - The recovery and back-filling of the OSDs/Pool is completed
2025-04-15 11:40:17,062 - cephci - utils:1858 - DEBUG - The <function wait_for_clean_pg_sets at 0x7fbdecd8c1f0> return status is True
2025-04-15 11:40:17,063 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd set noscrub on 10.0.195.79
2025-04-15 11:40:19,593 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd set noscrub on 10.0.195.79 took 2.52967 seconds
2025-04-15 11:40:19,594 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:19,594 - cephci - rados_scrub:190 - INFO - The OSD falg noscrub is set on the cluster.
2025-04-15 11:40:19,635 - cephci - ceph:1576 - INFO - Execute cephadm shell -- date +%Y:%m:%d:%H:%u on 10.0.195.79
2025-04-15 11:40:20,719 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- date +%Y:%m:%d:%H:%u on 10.0.195.79 took 1.08409 seconds
2025-04-15 11:40:20,720 - cephci - shell:64 - DEBUG - 2025:04:15:11:2

2025-04-15 11:40:20,761 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_begin_hour 11 on 10.0.195.79
2025-04-15 11:40:23,321 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_begin_hour 11 on 10.0.195.79 took 2.56019 seconds
2025-04-15 11:40:23,322 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:23,322 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:23,323 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_begin_hour -f json on 10.0.195.79
2025-04-15 11:40:25,818 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_begin_hour -f json on 10.0.195.79 took 2.494082 seconds
2025-04-15 11:40:25,818 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_begin_hour is set with 11 on osds
2025-04-15 11:40:25,819 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_begin_week_day 2 on 10.0.195.79
2025-04-15 11:40:27,342 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_begin_week_day 2 on 10.0.195.79 took 1.522111 seconds
2025-04-15 11:40:27,342 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:27,343 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:27,384 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_begin_week_day -f json on 10.0.195.79
2025-04-15 11:40:29,201 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_begin_week_day -f json on 10.0.195.79 took 1.815976 seconds
2025-04-15 11:40:29,201 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:40:29,201 - cephci - ceph:1186 - DEBUG - 2
2025-04-15 11:40:29,201 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_begin_week_day is set with 2 on osds
2025-04-15 11:40:29,242 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_end_hour 13 on 10.0.195.79
2025-04-15 11:40:30,753 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_end_hour 13 on 10.0.195.79 took 1.511117 seconds
2025-04-15 11:40:30,754 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:30,754 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:30,795 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_end_hour -f json on 10.0.195.79
2025-04-15 11:40:33,275 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_end_hour -f json on 10.0.195.79 took 2.479638 seconds
2025-04-15 11:40:33,276 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_end_hour is set with 13 on osds
2025-04-15 11:40:33,277 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_end_week_day 2 on 10.0.195.79
2025-04-15 11:40:35,827 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_end_week_day 2 on 10.0.195.79 took 2.54897 seconds
2025-04-15 11:40:35,827 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:35,828 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:35,829 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_end_week_day -f json on 10.0.195.79
2025-04-15 11:40:38,593 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_end_week_day -f json on 10.0.195.79 took 2.763744 seconds
2025-04-15 11:40:38,593 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_end_week_day is set with 2 on osds
2025-04-15 11:40:38,595 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_min_interval 5 on 10.0.195.79
2025-04-15 11:40:41,164 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_min_interval 5 on 10.0.195.79 took 2.569559 seconds
2025-04-15 11:40:41,165 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:41,165 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:41,166 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_min_interval -f json on 10.0.195.79
2025-04-15 11:40:42,806 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_min_interval -f json on 10.0.195.79 took 1.639269 seconds
2025-04-15 11:40:42,806 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:40:42,806 - cephci - ceph:1186 - DEBUG - 5.000000
2025-04-15 11:40:42,807 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_min_interval is set with 5 on osds
2025-04-15 11:40:42,847 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_scrub_max_interval 1500 on 10.0.195.79
2025-04-15 11:40:44,611 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_scrub_max_interval 1500 on 10.0.195.79 took 1.762899 seconds
2025-04-15 11:40:44,611 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:44,611 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:44,652 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_scrub_max_interval -f json on 10.0.195.79
2025-04-15 11:40:46,200 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_scrub_max_interval -f json on 10.0.195.79 took 1.547761 seconds
2025-04-15 11:40:46,201 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:40:46,201 - cephci - ceph:1186 - DEBUG - 1500.000000
2025-04-15 11:40:46,201 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_scrub_max_interval is set with 1500 on osds
2025-04-15 11:40:46,242 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config set  osd osd_deep_scrub_interval 2400 on 10.0.195.79
2025-04-15 11:40:47,889 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config set  osd osd_deep_scrub_interval 2400 on 10.0.195.79 took 1.646432 seconds
2025-04-15 11:40:47,890 - cephci - shell:64 - DEBUG - 
2025-04-15 11:40:47,890 - cephci - rados_scrub:44 - INFO - Getting  osd configurations
2025-04-15 11:40:47,931 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config get osd osd_deep_scrub_interval -f json on 10.0.195.79
2025-04-15 11:40:49,468 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config get osd osd_deep_scrub_interval -f json on 10.0.195.79 took 1.536531 seconds
2025-04-15 11:40:49,469 - cephci - ceph:1186 - DEBUG - 
2025-04-15 11:40:49,469 - cephci - ceph:1186 - DEBUG - 2400.000000
2025-04-15 11:40:49,469 - cephci - rados_scrub:61 - INFO - Configuration parameter osd_deep_scrub_interval is set with 2400 on osds
2025-04-15 11:40:49,469 - cephci - test_osd_ecpool_inconsistency_scenario:456 - DEBUG - Running deep-scrub on pg : 14.0
2025-04-15 11:40:49,471 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:40:50,503 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031434 seconds
2025-04-15 11:40:50,528 - cephci - core_workflows:3479 - INFO - Dumping deep-scrub stats before starting deep-scrub
2025-04-15 11:40:50,529 - cephci - core_workflows:3480 - INFO - last_deep_scrub : 1492'100
2025-04-15 11:40:50,529 - cephci - core_workflows:3481 - INFO - last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:40:50,529 - cephci - core_workflows:3494 - DEBUG - Initiated scheduled deep-scrub
2025-04-15 11:40:50,530 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:40:51,561 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030219 seconds
2025-04-15 11:40:51,583 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:40:51,583 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:40:51,584 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:40:51,584 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:41:21,613 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:41:22,645 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03205 seconds
2025-04-15 11:41:22,667 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:41:22,668 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:41:22,668 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:41:22,668 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:41:52,696 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:41:53,727 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03048 seconds
2025-04-15 11:41:53,749 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:41:53,749 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:41:53,749 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:41:53,749 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:42:23,761 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:42:24,792 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029947 seconds
2025-04-15 11:42:24,814 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:42:24,815 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:42:24,815 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:42:24,815 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:42:54,841 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:42:55,870 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.028885 seconds
2025-04-15 11:42:55,892 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:42:55,892 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:42:55,892 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:42:55,892 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:43:25,924 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:43:26,957 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032942 seconds
2025-04-15 11:43:26,987 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:43:26,988 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:43:26,990 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:43:26,990 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:43:57,002 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:43:58,035 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031724 seconds
2025-04-15 11:43:58,057 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:43:58,057 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:43:58,057 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:43:58,058 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:44:28,074 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:44:29,107 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032913 seconds
2025-04-15 11:44:29,131 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:44:29,132 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:44:29,132 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:44:29,132 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:44:59,159 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:45:00,191 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031861 seconds
2025-04-15 11:45:00,214 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:45:00,214 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:45:00,214 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:45:00,214 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:45:30,224 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:45:31,258 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033447 seconds
2025-04-15 11:45:31,282 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:45:31,282 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:45:31,282 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:45:31,283 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:46:01,302 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:46:02,334 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031407 seconds
2025-04-15 11:46:02,357 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:46:02,358 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:46:02,358 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:46:02,358 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:46:32,379 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:46:33,414 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033809 seconds
2025-04-15 11:46:33,437 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:46:33,437 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:46:33,438 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:46:33,438 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:47:03,468 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:47:04,504 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035671 seconds
2025-04-15 11:47:04,538 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:47:04,539 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:47:04,539 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:47:04,539 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:47:34,567 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:47:35,598 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030875 seconds
2025-04-15 11:47:35,707 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:47:35,707 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:47:35,707 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:47:35,707 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:48:05,735 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:48:06,771 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034986 seconds
2025-04-15 11:48:06,803 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:48:06,803 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:48:06,803 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:48:06,803 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:48:36,828 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:48:37,861 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032236 seconds
2025-04-15 11:48:37,884 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:48:37,884 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:48:37,884 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:48:37,884 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:49:07,906 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:49:08,939 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03202 seconds
2025-04-15 11:49:08,961 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:49:08,961 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:49:08,962 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:49:08,962 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:49:38,987 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:49:40,020 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032725 seconds
2025-04-15 11:49:40,043 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:49:40,043 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:49:40,043 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:49:40,043 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:50:10,073 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:50:11,106 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032298 seconds
2025-04-15 11:50:11,132 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:50:11,132 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:50:11,132 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:50:11,132 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:50:41,160 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:50:42,190 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029624 seconds
2025-04-15 11:50:42,212 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:50:42,212 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:50:42,212 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:50:42,212 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:51:12,230 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:51:13,261 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03083 seconds
2025-04-15 11:51:13,282 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:51:13,282 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:51:13,282 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:51:13,282 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:51:43,314 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:51:44,348 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033595 seconds
2025-04-15 11:51:44,373 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:51:44,373 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:51:44,373 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:51:44,373 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:52:14,393 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:52:15,424 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031377 seconds
2025-04-15 11:52:15,449 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:52:15,449 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:52:15,449 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:52:15,449 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:52:45,475 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:52:46,511 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035411 seconds
2025-04-15 11:52:46,544 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:52:46,545 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:52:46,545 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:52:46,545 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:53:16,573 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:53:17,605 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030715 seconds
2025-04-15 11:53:17,627 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:53:17,628 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:53:17,628 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:53:17,628 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:53:47,647 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:53:48,678 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031164 seconds
2025-04-15 11:53:48,703 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:53:48,703 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:53:48,703 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:53:48,703 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:54:18,732 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:54:19,765 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031983 seconds
2025-04-15 11:54:19,788 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:54:19,788 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:54:19,789 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:54:19,789 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:54:49,818 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:54:50,850 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031752 seconds
2025-04-15 11:54:50,872 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:54:50,872 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:54:50,872 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:54:50,872 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:55:20,902 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:55:21,933 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030998 seconds
2025-04-15 11:55:21,955 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:55:21,955 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:55:21,955 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:55:21,956 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:55:51,985 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:55:53,016 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03072 seconds
2025-04-15 11:55:53,037 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:55:53,037 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:55:53,037 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:55:53,037 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:56:23,059 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:56:24,092 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032657 seconds
2025-04-15 11:56:24,188 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:56:24,189 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:56:24,189 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:56:24,189 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:56:54,208 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:56:55,241 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032284 seconds
2025-04-15 11:56:55,263 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:56:55,263 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:56:55,264 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:56:55,264 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:57:25,293 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:57:26,328 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034798 seconds
2025-04-15 11:57:26,349 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:57:26,350 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:57:26,350 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:57:26,350 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:57:56,381 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:57:57,413 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031942 seconds
2025-04-15 11:57:57,437 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:57:57,437 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:57:57,437 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:57:57,437 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:58:27,462 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:58:28,496 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033701 seconds
2025-04-15 11:58:28,517 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:58:28,517 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:58:28,517 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:58:28,517 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:58:58,531 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:58:59,565 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033419 seconds
2025-04-15 11:58:59,587 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:58:59,588 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:58:59,588 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:58:59,588 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 11:59:29,592 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 11:59:30,624 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031719 seconds
2025-04-15 11:59:30,647 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 11:59:30,647 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 11:59:30,647 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 11:59:30,647 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:00:00,679 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:00:01,715 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034384 seconds
2025-04-15 12:00:01,736 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:00:01,736 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:00:01,736 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:00:01,736 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:00:31,756 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:00:32,792 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035442 seconds
2025-04-15 12:00:32,813 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:00:32,814 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:00:32,814 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:00:32,814 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:01:02,837 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:01:03,870 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032392 seconds
2025-04-15 12:01:03,897 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:01:03,898 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:01:03,898 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:01:03,898 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:01:33,925 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:01:34,958 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032518 seconds
2025-04-15 12:01:34,982 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:01:34,982 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:01:34,982 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:01:34,983 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:02:05,006 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:02:06,038 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031572 seconds
2025-04-15 12:02:06,060 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:02:06,060 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:02:06,061 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:02:06,061 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:02:36,074 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:02:37,106 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031075 seconds
2025-04-15 12:02:37,132 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:02:37,133 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:02:37,133 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:02:37,133 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:03:07,163 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:03:08,199 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035908 seconds
2025-04-15 12:03:08,235 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:03:08,235 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:03:08,235 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:03:08,235 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:03:38,266 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:03:39,299 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03218 seconds
2025-04-15 12:03:39,322 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:03:39,322 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:03:39,322 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:03:39,322 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:04:09,335 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:04:10,366 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030587 seconds
2025-04-15 12:04:10,389 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:04:10,389 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:04:10,389 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:04:10,390 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:04:40,421 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:04:41,453 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031585 seconds
2025-04-15 12:04:41,475 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:04:41,476 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:04:41,476 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:04:41,476 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:05:11,500 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:05:12,531 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030938 seconds
2025-04-15 12:05:12,553 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:05:12,553 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:05:12,554 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:05:12,554 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:05:42,575 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:05:43,606 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031295 seconds
2025-04-15 12:05:43,629 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:05:43,629 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:05:43,629 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:05:43,629 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:06:13,661 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:06:14,692 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030822 seconds
2025-04-15 12:06:14,721 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:06:14,721 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:06:14,721 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:06:14,721 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:06:44,752 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:06:45,783 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029978 seconds
2025-04-15 12:06:45,804 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:06:45,804 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:06:45,804 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:06:45,804 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:07:15,833 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:07:16,868 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034179 seconds
2025-04-15 12:07:16,965 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:07:16,965 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:07:16,965 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:07:16,965 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:07:46,994 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:07:48,025 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030735 seconds
2025-04-15 12:07:48,047 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:07:48,048 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:07:48,048 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:07:48,048 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:08:18,079 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:08:19,110 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030726 seconds
2025-04-15 12:08:19,133 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:08:19,133 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:08:19,133 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:08:19,133 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:08:49,165 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:08:50,196 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030704 seconds
2025-04-15 12:08:50,218 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:08:50,219 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:08:50,219 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:08:50,219 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:09:20,250 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:09:21,282 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.03167 seconds
2025-04-15 12:09:21,305 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:09:21,305 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:09:21,306 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:09:21,306 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:09:51,329 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:09:52,361 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031352 seconds
2025-04-15 12:09:52,383 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:09:52,383 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:09:52,383 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:09:52,383 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:10:22,407 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:10:23,437 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030493 seconds
2025-04-15 12:10:23,458 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:10:23,459 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:10:23,459 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:10:23,459 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:10:53,469 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:10:54,503 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033108 seconds
2025-04-15 12:10:54,526 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:10:54,527 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:10:54,527 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:10:54,527 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:11:24,559 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:11:25,594 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035075 seconds
2025-04-15 12:11:25,628 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:11:25,629 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:11:25,629 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:11:25,629 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:11:55,660 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:11:56,693 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031798 seconds
2025-04-15 12:11:56,715 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:11:56,715 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:11:56,715 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:11:56,716 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:12:26,745 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:12:27,777 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031751 seconds
2025-04-15 12:12:27,800 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:12:27,801 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:12:27,801 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:12:27,801 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:12:57,833 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:12:58,864 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031083 seconds
2025-04-15 12:12:58,886 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:12:58,886 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:12:58,886 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:12:58,886 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:13:28,909 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:13:29,943 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033365 seconds
2025-04-15 12:13:29,966 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:13:29,966 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:13:29,966 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:13:29,966 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:13:59,989 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:14:01,019 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.029909 seconds
2025-04-15 12:14:01,055 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:14:01,056 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:14:01,056 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:14:01,056 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:14:31,088 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:14:32,119 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031389 seconds
2025-04-15 12:14:32,140 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:14:32,140 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:14:32,141 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:14:32,141 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:15:02,173 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:15:03,206 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.033041 seconds
2025-04-15 12:15:03,229 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:15:03,229 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:15:03,229 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:15:03,229 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:15:33,257 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:15:34,289 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031445 seconds
2025-04-15 12:15:34,311 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:15:34,311 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:15:34,311 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:15:34,311 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:16:04,325 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:16:05,357 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.031806 seconds
2025-04-15 12:16:05,440 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:16:05,441 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:16:05,441 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:16:05,441 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:16:35,472 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:16:36,509 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035863 seconds
2025-04-15 12:16:36,543 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:16:36,544 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:16:36,544 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:16:36,544 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:17:06,546 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:17:07,583 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.036001 seconds
2025-04-15 12:17:07,604 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:17:07,604 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:17:07,604 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:17:07,605 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:17:37,636 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:17:38,671 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.034218 seconds
2025-04-15 12:17:38,698 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:17:38,698 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:17:38,712 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:17:38,712 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:18:08,744 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:18:09,776 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032053 seconds
2025-04-15 12:18:09,798 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:18:09,799 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:18:09,799 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:18:09,799 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:18:39,828 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:18:40,859 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030513 seconds
2025-04-15 12:18:40,881 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:18:40,881 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:18:40,881 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:18:40,881 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:19:10,891 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:19:11,922 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.030369 seconds
2025-04-15 12:19:11,945 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:19:11,946 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:19:11,946 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:19:11,946 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:19:41,951 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:19:42,987 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.035501 seconds
2025-04-15 12:19:43,013 - cephci - core_workflows:3517 - DEBUG - Current last_deep_scrub: 1492'100
2025-04-15 12:19:43,013 - cephci - core_workflows:3518 - DEBUG - Current last_deep_scrub_stamp: 2025-04-15T11:39:40.385113+0000
2025-04-15 12:19:43,013 - cephci - core_workflows:3521 - DEBUG - Total time elapsed since starting deep-scrub on PG: 14.0 is 0:00:00
2025-04-15 12:19:43,013 - cephci - core_workflows:3525 - INFO - Deep-scrub is yet to complete, pg state: active+clean. Sleeping for 30 secs
2025-04-15 12:20:13,045 - cephci - ceph:1576 - INFO - Execute ceph pg dump_json pgs on 10.0.195.240
2025-04-15 12:20:14,077 - cephci - ceph:1606 - INFO - Execution of ceph pg dump_json pgs on 10.0.195.240 took 1.032005 seconds
2025-04-15 12:20:14,100 - cephci - core_workflows:3506 - INFO - Scrubbing complete on the PG: 14.0
2025-04-15 12:20:14,101 - cephci - core_workflows:3507 - DEBUG - Final last_deep_scrub: 1492'100
2025-04-15 12:20:14,101 - cephci - core_workflows:3508 - DEBUG - Final last_deep_scrub_stamp: 2025-04-15T12:19:42.144374+0000
2025-04-15 12:20:14,101 - cephci - core_workflows:3511 - DEBUG - Total time taken for scrubbing to complete on the pg : 14.0 is 0:40:01.759261
2025-04-15 12:20:14,101 - cephci - test_osd_ecpool_inconsistency_scenario:460 - INFO - Deep scrub completed on pg : 14.0
2025-04-15 12:30:14,203 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:30:15,354 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.149735 seconds
2025-04-15 12:30:15,354 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:30:15,354 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:30:15,354 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:30:45,369 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:30:47,719 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 2.349392 seconds
2025-04-15 12:30:47,720 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:30:47,720 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:31:17,725 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:31:18,860 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.135081 seconds
2025-04-15 12:31:18,861 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:31:18,862 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:31:48,884 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:31:51,107 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 2.223007 seconds
2025-04-15 12:31:51,108 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:31:51,108 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:32:21,140 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:32:22,426 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.285077 seconds
2025-04-15 12:32:22,427 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:32:22,427 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:32:22,427 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:32:52,444 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:32:53,842 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.397879 seconds
2025-04-15 12:32:53,843 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:32:53,843 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:32:53,844 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:33:23,875 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:33:25,131 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.255923 seconds
2025-04-15 12:33:25,132 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:33:25,132 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:33:25,132 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:33:55,164 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:33:56,345 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.181281 seconds
2025-04-15 12:33:56,346 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:33:56,346 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:33:56,346 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:34:26,364 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:34:28,740 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 2.37516 seconds
2025-04-15 12:34:28,740 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:34:28,740 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:34:58,750 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:34:59,983 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.232797 seconds
2025-04-15 12:34:59,984 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:34:59,984 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:34:59,984 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:35:30,008 - cephci - ceph:1576 - INFO - Execute cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79
2025-04-15 12:35:31,209 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- rados  list-inconsistent-obj  14.0 -f json on 10.0.195.79 took 1.200553 seconds
2025-04-15 12:35:31,209 - cephci - ceph:1186 - DEBUG - {"epoch":1537,"inconsistents":[]}
2025-04-15 12:35:31,210 - cephci - test_osd_ecpool_inconsistency_scenario:474 - DEBUG - The inconsistent object details in the pg-14.0 is - {'epoch': 1537, 'inconsistents': []}
2025-04-15 12:35:31,210 - cephci - test_osd_ecpool_inconsistency_scenario:478 - INFO -  The inconsistent object count after deep-scrub is 0
2025-04-15 12:36:01,219 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset noscrub on 10.0.195.79
2025-04-15 12:36:04,868 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset noscrub on 10.0.195.79 took 3.647857 seconds
2025-04-15 12:36:04,868 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:04,868 - cephci - rados_scrub:190 - INFO - The OSD falg noscrub is unset on the cluster.
2025-04-15 12:36:04,869 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph -s -f json on 10.0.195.79
2025-04-15 12:36:06,541 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph -s -f json on 10.0.195.79 took 1.671727 seconds
2025-04-15 12:36:06,542 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:36:06,542 - cephci - ceph:1186 - DEBUG - {"fsid":"79ad3a88-19c4-11f0-bea5-fa163ea10f51","health":{"status":"HEALTH_WARN","checks":{"BLUESTORE_SLOW_OP_ALERT":{"severity":"HEALTH_WARN","summary":{"message":"1 OSD(s) experiencing slow operations in BlueStore","count":1},"muted":false},"CEPHADM_FAILED_DAEMON":{"severity":"HEALTH_WARN","summary":{"message":"1 failed cephadm daemon(s)","count":1},"muted":false}},"mutes":[]},"election_epoch":14,"quorum":[0,1,2],"quorum_names":["ceph-regression-juxejq-zv217a-node1-installer","ceph-regression-juxejq-zv217a-node2","ceph-regression-juxejq-zv217a-node6"],"quorum_age":20846,"monmap":{"epoch":3,"min_mon_release_name":"squid","num_mons":3},"osdmap":{"epoch":5888,"num_osds":15,"num_up_osds":14,"osd_up_since":1744715474,"num_in_osds":14,"osd_in_since":1744708780,"num_remapped_pgs":0},"pgmap":{"pgs_by_state":[{"state_name":"active+clean","count":690}],"num_pgs":690,"num_pools":9,"num_objects":269,"data_bytes":467441,"bytes_used":13562634240,"bytes_avail":362188283904,"bytes_total":375750918144},"fsmap":{"epoch":15,"btime":"2025-04-15T06:58:09:825897+0000","id":1,"up":1,"in":1,"max":1,"by_rank":[{"filesystem_id":1,"rank":0,"name":"cephfs.ceph-regression-juxejq-zv217a-node2.piqpns","status":"up:active","gid":24460}],"up:standby":1},"mgrmap":{"available":true,"num_standbys":2,"modules":["cephadm","dashboard","iostat","nfs","prometheus","smb"],"services":{"dashboard":"https://10.0.195.79:8443/","prometheus":"http://10.0.195.79:9283/"}},"servicemap":{"epoch":351,"modified":"2025-04-15T12:34:21.909891+0000","services":{"rgw":{"daemons":{"summary":"","24478":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.061555+0000","gid":24478,"addr":"10.0.195.138:0/3807925065","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node2","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e2024
2025-04-15 12:36:06,542 - cephci - ceph:1186 - DEBUG - 7c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)","distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node2","id":"rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868476","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}},"34526":{"start_epoch":15,"start_stamp":"2025-04-15T06:56:25.103709+0000","gid":34526,"addr":"10.0.195.171:0/2816701951","metadata":{"arch":"x86_64","ceph_release":"squid","ceph_version":"ceph version 19.2.1-126.el9cp (cfd2907537ba633f7d638895efd70ef5d0f1c99b) squid (stable)","ceph_version_short":"19.2.1-126.el9cp","container_hostname":"ceph-regression-juxejq-zv217a-node6","container_image":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","cpu":"AMD EPYC-Rome Processor","distro":"rhel","distro_description":"Red Hat Enterprise Linux 9.5 (Plow)","distro_version":"9.5","frontend_config#0":"beast port=80","frontend_type#0":"beast","hostname":"ceph-regression-juxejq-zv217a-node6","id":"rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","kernel_description":"#1 SMP PREEMPT_DYNAMIC Mon Mar 24 11:15:27 EDT 2025","kernel_version":"5.14.0-503.35.1.el9_5.x86_64","mem_swap_kb":"0","mem_total_kb":"7868484","num_handles":"1","os":"Linux","pid":"2","realm_id":"","realm_name":"","zone_id":"42e4606c-35ef-44f1-951b-124b81abdf13","zone_name":"default","zonegroup_id":"e671bdd3-530b-40fb-990c-680fe683b65f","zonegroup_name":"default"},"task_status":{}}}}}},"progress_events":{}}
2025-04-15 12:36:06,543 - cephci - test_osd_replicated_inconsistency_scenario:334 - ERROR - The inconsistent objects not created
2025-04-15 12:36:06,543 - cephci - test_osd_replicated_inconsistency_scenario:148 - ERROR - Deep scrub repaired the 18 inconsistent objects
2025-04-15 12:36:06,543 - cephci - core_workflows:4558 - DEBUG - Printing cluster health and status
2025-04-15 12:36:06,583 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 12:36:08,269 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 1.685704 seconds
2025-04-15 12:36:08,270 - cephci - ceph:1186 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
2025-04-15 12:36:08,270 - cephci - ceph:1186 - DEBUG - [WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
2025-04-15 12:36:08,270 - cephci - ceph:1186 - DEBUG -      osd.6 observed slow operation indications in BlueStore
2025-04-15 12:36:08,270 - cephci - ceph:1186 - DEBUG - [WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
2025-04-15 12:36:08,270 - cephci - ceph:1186 - DEBUG -     daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state
2025-04-15 12:36:08,271 - cephci - shell:64 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state

2025-04-15 12:36:08,271 - cephci - core_workflows:4560 - INFO - 
****
 Cluster health detail: 
 HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state
 
****
2025-04-15 12:36:08,272 - cephci - ceph:1576 - INFO - Execute ceph -s on 10.0.195.240
2025-04-15 12:36:09,279 - cephci - ceph:1606 - INFO - Execution of ceph -s on 10.0.195.240 took 1.00715 seconds
2025-04-15 12:36:09,280 - cephci - core_workflows:4561 - INFO - 
****
 Cluster status: 
   cluster:
    id:     79ad3a88-19c4-11f0-bea5-fa163ea10f51
    health: HEALTH_WARN
            1 OSD(s) experiencing slow operations in BlueStore
            1 failed cephadm daemon(s)
 
  services:
    mon: 3 daemons, quorum ceph-regression-juxejq-zv217a-node1-installer,ceph-regression-juxejq-zv217a-node2,ceph-regression-juxejq-zv217a-node6 (age 5h)
    mgr: ceph-regression-juxejq-zv217a-node1-installer.odtafx(active, since 5h), standbys: ceph-regression-juxejq-zv217a-node2.oxadyy, ceph-regression-juxejq-zv217a-node6.agdlmk
    mds: 1/1 daemons up, 1 standby
    osd: 15 osds: 14 up (since 84m), 14 in (since 3h)
    rgw: 2 daemons active (2 hosts, 1 zones)
 
  data:
    volumes: 1/1 healthy
    pools:   9 pools, 690 pgs
    objects: 269 objects, 456 KiB
    usage:   13 GiB used, 337 GiB / 350 GiB avail
    pgs:     690 active+clean
 
 
****
2025-04-15 12:36:09,280 - cephci - test_osd_replicated_inconsistency_scenario:239 - INFO - Execution of finally block
2025-04-15 12:36:09,281 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79
2025-04-15 12:36:11,228 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79 took 1.946236 seconds
2025-04-15 12:36:11,228 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:11,228 - cephci - rados_scrub:190 - INFO - The OSD falg nodeep-scrub is unset on the cluster.
2025-04-15 12:36:11,270 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset noscrub on 10.0.195.79
2025-04-15 12:36:13,230 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset noscrub on 10.0.195.79 took 1.959789 seconds
2025-04-15 12:36:13,231 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:13,231 - cephci - rados_scrub:190 - INFO - The OSD falg noscrub is unset on the cluster.
2025-04-15 12:36:13,272 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79
2025-04-15 12:36:14,694 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79 took 1.421362 seconds
2025-04-15 12:36:14,694 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:24,705 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:36:24,706 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:36:26,271 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.564402 seconds
2025-04-15 12:36:26,271 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:36:26,272 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"debug_mgr","value":"20/20","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false
2025-04-15 12:36:26,272 - cephci - ceph:1186 - DEBUG - ,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/nginx-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advan
2025-04-15 12:36:26,273 - cephci - ceph:1186 - DEBUG - ced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair_num_errors","value":"17","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_hour","value":"11","level"
2025-04-15 12:36:26,273 - cephci - ceph:1186 - DEBUG - :"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_hour","value":"13","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask"
2025-04-15 12:36:26,274 - cephci - ceph:1186 - DEBUG - :""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:36:26,274 - cephci - monitor_configurations:351 - ERROR - The Config: debug_osd not listed under in the dump
2025-04-15 12:36:26,274 - cephci - monitor_configurations:277 - INFO - Value for config: debug_osd was removed
2025-04-15 12:36:26,312 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm mgr debug_mgr on 10.0.195.79
2025-04-15 12:36:27,819 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm mgr debug_mgr on 10.0.195.79 took 1.505854 seconds
2025-04-15 12:36:27,819 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:37,820 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:36:37,821 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:36:39,349 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.52756 seconds
2025-04-15 12:36:39,350 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:36:39,350 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:36:39,350 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:36:39,351 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair_num_errors","value":"17","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_hour","value":"11","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_week_day","value":
2025-04-15 12:36:39,351 - cephci - ceph:1186 - DEBUG - "2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_hour","value":"13","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true
2025-04-15 12:36:39,351 - cephci - ceph:1186 - DEBUG - ,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:36:39,352 - cephci - monitor_configurations:351 - ERROR - The Config: debug_mgr not listed under in the dump
2025-04-15 12:36:39,352 - cephci - monitor_configurations:277 - INFO - Value for config: debug_mgr was removed
2025-04-15 12:36:39,353 - cephci - ceph:1576 - INFO - Execute ceph config set mon mon_allow_pool_delete true on 10.0.195.240
2025-04-15 12:36:40,356 - cephci - ceph:1606 - INFO - Execution of ceph config set mon mon_allow_pool_delete true on 10.0.195.240 took 1.003159 seconds
2025-04-15 12:36:40,358 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:36:41,362 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.003658 seconds
2025-04-15 12:36:41,364 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79
2025-04-15 12:36:43,860 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd pool ls detail -f json on 10.0.195.79 took 2.496081 seconds
2025-04-15 12:36:43,862 - cephci - ceph:1576 - INFO - Execute ceph osd pool delete replicated_pool replicated_pool --yes-i-really-really-mean-it on 10.0.195.240
2025-04-15 12:36:44,866 - cephci - ceph:1606 - INFO - Execution of ceph osd pool delete replicated_pool replicated_pool --yes-i-really-really-mean-it on 10.0.195.240 took 1.003587 seconds
2025-04-15 12:36:46,870 - cephci - ceph:1576 - INFO - Execute ceph df -f json on 10.0.195.240
2025-04-15 12:36:47,874 - cephci - ceph:1606 - INFO - Execution of ceph df -f json on 10.0.195.240 took 1.003589 seconds
2025-04-15 12:36:47,874 - cephci - core_workflows:1199 - INFO - Pool:replicated_pool deleted Successfully
2025-04-15 12:36:47,874 - cephci - utils:1858 - DEBUG - The <bound method RadosOrchestrator.delete_pool of <ceph.rados.core_workflows.RadosOrchestrator object at 0x7fbded72ea30>> return status is True
2025-04-15 12:36:47,875 - cephci - test_osd_replicated_inconsistency_scenario:246 - INFO - deleted the pool successfully
2025-04-15 12:36:47,876 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_auto_repair_num_errors on 10.0.195.79
2025-04-15 12:36:49,687 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_auto_repair_num_errors on 10.0.195.79 took 1.81105 seconds
2025-04-15 12:36:49,688 - cephci - shell:64 - DEBUG - 
2025-04-15 12:36:59,698 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:36:59,699 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:37:01,279 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.579424 seconds
2025-04-15 12:37:01,280 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:37:01,280 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:37:01,280 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:37:01,280 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_auto_repair","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_hour","value":"11","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_hour","value":"13","level":"a
2025-04-15 12:37:01,281 - cephci - ceph:1186 - DEBUG - dvanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80"
2025-04-15 12:37:01,281 - cephci - ceph:1186 - DEBUG - ,"level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:37:01,282 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_auto_repair_num_errors not listed under in the dump
2025-04-15 12:37:01,282 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_auto_repair_num_errors was removed
2025-04-15 12:37:01,321 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_auto_repair on 10.0.195.79
2025-04-15 12:37:02,876 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_auto_repair on 10.0.195.79 took 1.5552 seconds
2025-04-15 12:37:02,877 - cephci - shell:64 - DEBUG - 
2025-04-15 12:37:12,887 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:37:12,889 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:37:15,472 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 2.583525 seconds
2025-04-15 12:37:15,473 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_auto_repair not listed under in the dump
2025-04-15 12:37:15,474 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_auto_repair was removed
2025-04-15 12:37:15,475 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_begin_hour on 10.0.195.79
2025-04-15 12:37:17,039 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_begin_hour on 10.0.195.79 took 1.563222 seconds
2025-04-15 12:37:17,039 - cephci - shell:64 - DEBUG - 
2025-04-15 12:37:27,040 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:37:27,042 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:37:28,639 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.59606 seconds
2025-04-15 12:37:28,639 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:37:28,640 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:37:28,640 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:37:28,640 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_begin_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_hour","value":"13","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","
2025-04-15 12:37:28,641 - cephci - ceph:1186 - DEBUG - level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:37:28,641 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_begin_hour not listed under in the dump
2025-04-15 12:37:28,641 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_begin_hour was removed
2025-04-15 12:37:28,681 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_begin_week_day on 10.0.195.79
2025-04-15 12:37:30,510 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_begin_week_day on 10.0.195.79 took 1.828105 seconds
2025-04-15 12:37:30,511 - cephci - shell:64 - DEBUG - 
2025-04-15 12:37:40,521 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:37:40,522 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:37:42,062 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.53955 seconds
2025-04-15 12:37:42,063 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:37:42,063 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:37:42,063 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:37:42,063 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_hour","value":"13","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.0000
2025-04-15 12:37:42,064 - cephci - ceph:1186 - DEBUG - 00","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:37:42,064 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_begin_week_day not listed under in the dump
2025-04-15 12:37:42,065 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_begin_week_day was removed
2025-04-15 12:37:42,104 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_end_hour on 10.0.195.79
2025-04-15 12:37:43,670 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_end_hour on 10.0.195.79 took 1.565516 seconds
2025-04-15 12:37:43,670 - cephci - shell:64 - DEBUG - 
2025-04-15 12:37:53,681 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:37:53,682 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:37:55,272 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.589626 seconds
2025-04-15 12:37:55,273 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:37:55,273 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:37:55,273 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:37:55,273 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_end_week_day","value":"2","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_io
2025-04-15 12:37:55,274 - cephci - ceph:1186 - DEBUG - ps_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:37:55,274 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_end_hour not listed under in the dump
2025-04-15 12:37:55,274 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_end_hour was removed
2025-04-15 12:37:55,314 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_end_week_day on 10.0.195.79
2025-04-15 12:37:57,955 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_end_week_day on 10.0.195.79 took 2.640345 seconds
2025-04-15 12:37:57,956 - cephci - shell:64 - DEBUG - 
2025-04-15 12:38:07,966 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:38:07,969 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:38:09,637 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.666742 seconds
2025-04-15 12:38:09,637 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:38:09,637 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:38:09,638 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:38:09,638 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_min_interval","value":"5.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mcl
2025-04-15 12:38:09,638 - cephci - ceph:1186 - DEBUG - ock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:38:09,639 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_end_week_day not listed under in the dump
2025-04-15 12:38:09,639 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_end_week_day was removed
2025-04-15 12:38:09,679 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_min_interval on 10.0.195.79
2025-04-15 12:38:11,392 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_min_interval on 10.0.195.79 took 1.713093 seconds
2025-04-15 12:38:11,393 - cephci - shell:64 - DEBUG - 
2025-04-15 12:38:21,404 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:38:21,406 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:38:23,124 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.717312 seconds
2025-04-15 12:38:23,125 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:38:23,125 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:38:23,125 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:38:23,125 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_scrub_max_interval","value":"1500.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","na
2025-04-15 12:38:23,126 - cephci - ceph:1186 - DEBUG - me":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:38:23,126 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_min_interval not listed under in the dump
2025-04-15 12:38:23,126 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_min_interval was removed
2025-04-15 12:38:23,167 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_scrub_max_interval on 10.0.195.79
2025-04-15 12:38:25,821 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_scrub_max_interval on 10.0.195.79 took 2.653768 seconds
2025-04-15 12:38:25,822 - cephci - shell:64 - DEBUG - 
2025-04-15 12:38:35,832 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:38:35,840 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:38:37,405 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.564584 seconds
2025-04-15 12:38:37,406 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:38:37,406 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:38:37,406 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:38:37,407 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_deep_scrub_interval","value":"2400.000000","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"os
2025-04-15 12:38:37,407 - cephci - ceph:1186 - DEBUG - d.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:38:37,408 - cephci - monitor_configurations:351 - ERROR - The Config: osd_scrub_max_interval not listed under in the dump
2025-04-15 12:38:37,408 - cephci - monitor_configurations:277 - INFO - Value for config: osd_scrub_max_interval was removed
2025-04-15 12:38:37,447 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd osd_deep_scrub_interval on 10.0.195.79
2025-04-15 12:38:39,080 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd osd_deep_scrub_interval on 10.0.195.79 took 1.631881 seconds
2025-04-15 12:38:39,080 - cephci - shell:64 - DEBUG - 
2025-04-15 12:38:49,091 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:38:49,092 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:38:51,723 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 2.630801 seconds
2025-04-15 12:38:51,724 - cephci - monitor_configurations:351 - ERROR - The Config: osd_deep_scrub_interval not listed under in the dump
2025-04-15 12:38:51,725 - cephci - monitor_configurations:277 - INFO - Value for config: osd_deep_scrub_interval was removed
2025-04-15 12:38:51,726 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79
2025-04-15 12:38:53,225 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm osd debug_osd on 10.0.195.79 took 1.498524 seconds
2025-04-15 12:38:53,226 - cephci - shell:64 - DEBUG - 
2025-04-15 12:39:03,228 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:39:03,230 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:39:04,735 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.505517 seconds
2025-04-15 12:39:04,736 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:39:04,736 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:39:04,736 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:39:04,737 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"sect
2025-04-15 12:39:04,737 - cephci - ceph:1186 - DEBUG - ion":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:39:04,737 - cephci - monitor_configurations:351 - ERROR - The Config: debug_osd not listed under in the dump
2025-04-15 12:39:04,738 - cephci - monitor_configurations:277 - INFO - Value for config: debug_osd was removed
2025-04-15 12:39:04,777 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset noscrub on 10.0.195.79
2025-04-15 12:39:07,100 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset noscrub on 10.0.195.79 took 2.322561 seconds
2025-04-15 12:39:07,101 - cephci - shell:64 - DEBUG - 
2025-04-15 12:39:07,101 - cephci - rados_scrub:190 - INFO - The OSD falg noscrub is unset on the cluster.
2025-04-15 12:39:07,142 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79
2025-04-15 12:39:10,178 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph osd unset nodeep-scrub on 10.0.195.79 took 3.035463 seconds
2025-04-15 12:39:10,179 - cephci - shell:64 - DEBUG - 
2025-04-15 12:39:10,179 - cephci - rados_scrub:190 - INFO - The OSD falg nodeep-scrub is unset on the cluster.
2025-04-15 12:39:10,180 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm global osd_pool_default_pg_autoscale_mode on 10.0.195.79
2025-04-15 12:39:11,682 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm global osd_pool_default_pg_autoscale_mode on 10.0.195.79 took 1.501856 seconds
2025-04-15 12:39:11,683 - cephci - shell:64 - DEBUG - 
2025-04-15 12:39:21,693 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:39:21,694 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:39:23,128 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.433536 seconds
2025-04-15 12:39:23,129 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:39:23,129 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:39:23,129 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:39:23,130 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"sect
2025-04-15 12:39:23,130 - cephci - ceph:1186 - DEBUG - ion":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:39:23,130 - cephci - monitor_configurations:351 - ERROR - The Config: osd_pool_default_pg_autoscale_mode not listed under in the dump
2025-04-15 12:39:23,131 - cephci - monitor_configurations:277 - INFO - Value for config: osd_pool_default_pg_autoscale_mode was removed
2025-04-15 12:39:23,171 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config rm mgr debug_mgr on 10.0.195.79
2025-04-15 12:39:25,725 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config rm mgr debug_mgr on 10.0.195.79 took 2.554391 seconds
2025-04-15 12:39:25,726 - cephci - shell:64 - DEBUG - 
2025-04-15 12:39:35,728 - cephci - monitor_configurations:271 - DEBUG - verifying the value removed
2025-04-15 12:39:35,731 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph config dump -f json on 10.0.195.79
2025-04-15 12:39:37,210 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph config dump -f json on 10.0.195.79 took 1.478148 seconds
2025-04-15 12:39:37,211 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:39:37,211 - cephci - ceph:1186 - DEBUG - [{"section":"global","name":"container_image","value":"cp.stg.icr.io/cp/ibm-ceph/ceph-8-rhel9@sha256:eb77daa2b92c9d1d4fa9ccdaff1c53c069828e8e20247c2ad18dd55fae3ea2e6","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"global","name":"log_to_file","value":"true","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"global","name":"mon_cluster_log_to_file","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"global","name":"public_network","value":"10.0.195.0/24","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mon","name":"auth_allow_insecure_global_id_reclaim","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mon","name":"mon_allow_pool_delete","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/balancer/active","value":"false","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_alertmanager","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-alertmanager:v4.15.0-202503170806.p0.g870ade5.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_grafana","value":"cp.stg.icr.io/cp/ibm-ceph/grafana-rhel9:10.4.8-30","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_haproxy","value":"cp.stg.icr.io/cp/ibm-ceph/haproxy-rhel9:2.4.22-48","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_keepalived","value":"cp.stg.icr.io/cp/ibm-ceph/keepalived-rhel9:2.2.8-46","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_loki","value":"cp.stg.icr.io/cp/ibm-ceph/logging-loki-rhel8:v3.1.0-15","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nginx","value":"cp.stg.icr.io/cp/ibm-ceph/ngin
2025-04-15 12:39:37,211 - cephci - ceph:1186 - DEBUG - x-124-rhel9:1-25.1726696143","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_node_exporter","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus-node-exporter:v4.15.0-202503170806.p0.gaed837c.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_nvmeof","value":"cp.stg.icr.io/cp/ibm-ceph/nvmeof-rhel9:1.4.6-1","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_oauth2_proxy","value":"cp.stg.icr.io/cp/ibm-ceph/oauth2-proxy-rhel9:v7.6.0-13","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_prometheus","value":"cp.stg.icr.io/cp/ibm-ceph/prometheus:v4.15.0-202503170806.p0.g1b43998.assembly.stream.el8","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_promtail","value":"cp.stg.icr.io/cp/ibm-ceph/promtail-rhel9:v3.0.0-19","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba","value":"cp.stg.icr.io/cp/ibm-ceph/samba-server-rhel9:v8.1-1577.20250411173507.6437ce2","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_samba_metrics","value":"cp.stg.icr.io/cp/ibm-ceph/samba-metrics-rhel9:v8.1-1578.20250411175054.57788c4","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_image_snmp_gateway","value":"cp.stg.icr.io/cp/ibm-ceph/snmp-notifier-rhel9:1.2.1-97","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/container_init","value":"True","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/cephadm/migration_current","value":"7","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ALERTMANAGER_API_HOST","va
2025-04-15 12:39:37,211 - cephci - ceph:1186 - DEBUG - lue":"http://ceph-regression-juxejq-zv217a-node1-installer:9093","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_SSL_VERIFY","value":"false","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/GRAFANA_API_URL","value":"https://ceph-regression-juxejq-zv217a-node1-installer:3000","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/PROMETHEUS_API_HOST","value":"http://ceph-regression-juxejq-zv217a-node1-installer:9095","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_ACCESS_KEY","value":"0L2FC180AC17HWG8MB60","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/RGW_API_SECRET_KEY","value":"OHUpLzaP5bHZ0NvQ5N7fkSOGT3UltJWxBiDDUUcL","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/dashboard/ssl_server_port","value":"8443","level":"advanced","can_update_at_runtime":false,"mask":""},{"section":"mgr","name":"mgr/orchestrator/orchestrator","value":"cephadm","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_mclock_override_recovery_settings","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd","name":"osd_memory_target_autotune","value":"true","level":"advanced","can_update_at_runtime":true,"mask":""},{"section":"osd.10","name":"osd_mclock_max_capacity_iops_hdd","value":"373.216489","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.11","name":"osd_mclock_max_capacity_iops_hdd","value":"393.692019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.12","name":"osd_mclock_max_capacity_iops_hdd","value":"353.339019","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.13","name":"osd_mclock_max_capacity_iops_hdd","value":"488.938867","level":"basic","can_update_at_runtime":true,"mask":""},{"sect
2025-04-15 12:39:37,212 - cephci - ceph:1186 - DEBUG - ion":"osd.14","name":"osd_mclock_max_capacity_iops_hdd","value":"455.780697","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.2","name":"osd_mclock_max_capacity_iops_hdd","value":"489.952034","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.4","name":"osd_mclock_max_capacity_iops_hdd","value":"495.110298","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.6","name":"osd_mclock_max_capacity_iops_hdd","value":"418.316583","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.8","name":"osd_mclock_max_capacity_iops_hdd","value":"490.526606","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"osd.9","name":"osd_mclock_max_capacity_iops_hdd","value":"408.512109","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"mds.cephfs","name":"mds_join_fs","value":"cephfs","level":"basic","can_update_at_runtime":true,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node2.ojeeue","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""},{"section":"client.rgw.rgw.1.ceph-regression-juxejq-zv217a-node6.vkxzid","name":"rgw_frontends","value":"beast port=80","level":"basic","can_update_at_runtime":false,"mask":""}]
2025-04-15 12:39:37,212 - cephci - monitor_configurations:351 - ERROR - The Config: debug_mgr not listed under in the dump
2025-04-15 12:39:37,212 - cephci - monitor_configurations:277 - INFO - Value for config: debug_mgr was removed
2025-04-15 12:39:47,222 - cephci - core_workflows:4558 - DEBUG - Printing cluster health and status
2025-04-15 12:39:47,224 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph health detail on 10.0.195.79
2025-04-15 12:39:48,822 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph health detail on 10.0.195.79 took 1.59719 seconds
2025-04-15 12:39:48,822 - cephci - ceph:1186 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
2025-04-15 12:39:48,823 - cephci - ceph:1186 - DEBUG - [WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
2025-04-15 12:39:48,823 - cephci - ceph:1186 - DEBUG -      osd.6 observed slow operation indications in BlueStore
2025-04-15 12:39:48,823 - cephci - ceph:1186 - DEBUG - [WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
2025-04-15 12:39:48,823 - cephci - ceph:1186 - DEBUG -     daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state
2025-04-15 12:39:48,823 - cephci - shell:64 - DEBUG - HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state

2025-04-15 12:39:48,823 - cephci - core_workflows:4560 - INFO - 
****
 Cluster health detail: 
 HEALTH_WARN 1 OSD(s) experiencing slow operations in BlueStore; 1 failed cephadm daemon(s)
[WRN] BLUESTORE_SLOW_OP_ALERT: 1 OSD(s) experiencing slow operations in BlueStore
     osd.6 observed slow operation indications in BlueStore
[WRN] CEPHADM_FAILED_DAEMON: 1 failed cephadm daemon(s)
    daemon node-exporter.ceph-regression-juxejq-zv217a-node1-installer on ceph-regression-juxejq-zv217a-node1-installer is in error state
 
****
2025-04-15 12:39:48,824 - cephci - ceph:1576 - INFO - Execute ceph -s on 10.0.195.240
2025-04-15 12:39:49,828 - cephci - ceph:1606 - INFO - Execution of ceph -s on 10.0.195.240 took 1.00352 seconds
2025-04-15 12:39:49,828 - cephci - core_workflows:4561 - INFO - 
****
 Cluster status: 
   cluster:
    id:     79ad3a88-19c4-11f0-bea5-fa163ea10f51
    health: HEALTH_WARN
            1 OSD(s) experiencing slow operations in BlueStore
            1 failed cephadm daemon(s)
 
  services:
    mon: 3 daemons, quorum ceph-regression-juxejq-zv217a-node1-installer,ceph-regression-juxejq-zv217a-node2,ceph-regression-juxejq-zv217a-node6 (age 5h)
    mgr: ceph-regression-juxejq-zv217a-node1-installer.odtafx(active, since 5h), standbys: ceph-regression-juxejq-zv217a-node2.oxadyy, ceph-regression-juxejq-zv217a-node6.agdlmk
    mds: 1/1 daemons up, 1 standby
    osd: 15 osds: 14 up (since 88m), 14 in (since 3h)
    rgw: 2 daemons active (2 hosts, 1 zones)
 
  data:
    volumes: 1/1 healthy
    pools:   8 pools, 689 pgs
    objects: 219 objects, 456 KiB
    usage:   13 GiB used, 337 GiB / 350 GiB avail
    pgs:     689 active+clean
 
 
****
2025-04-15 12:39:49,830 - cephci - ceph:1576 - INFO - Execute cephadm shell -- ceph crash ls-new -f json on 10.0.195.79
2025-04-15 12:39:51,333 - cephci - ceph:1606 - INFO - Execution of cephadm shell -- ceph crash ls-new -f json on 10.0.195.79 took 1.503215 seconds
2025-04-15 12:39:51,334 - cephci - ceph:1186 - DEBUG - 
2025-04-15 12:39:51,334 - cephci - ceph:1186 - DEBUG - []
2025-04-15 12:39:51,375 - cephci - ceph:1576 - INFO - Execute podman --version | awk {'print $3'} on 10.0.195.79
2025-04-15 12:39:52,379 - cephci - ceph:1606 - INFO - Execution of podman --version | awk {'print $3'} on 10.0.195.79 took 1.003495 seconds
2025-04-15 12:39:52,380 - cephci - run:1065 - INFO - Podman Version 5.2.2
2025-04-15 12:39:52,381 - cephci - ceph:1576 - INFO - Execute docker --version | awk {'print $3'} on 10.0.195.79
2025-04-15 12:39:53,384 - cephci - ceph:1606 - INFO - Execution of docker --version | awk {'print $3'} on 10.0.195.79 took 1.003112 seconds
2025-04-15 12:39:53,386 - cephci - ceph:1576 - INFO - Execute ceph --version | awk '{print $3}' on 10.0.195.240
2025-04-15 12:39:54,391 - cephci - ceph:1606 - INFO - Execution of ceph --version | awk '{print $3}' on 10.0.195.240 took 1.004224 seconds
2025-04-15 12:39:54,391 - cephci - run:1081 - INFO - ceph Version 19.2.1-126.el9cp
2025-04-15 12:39:54,394 - cephci - run:1040 - INFO - ceph_clusters_file rerun/regression-JUxEjq-ZV217A
2025-04-15 12:39:54,395 - cephci - run:927 - INFO - Test <module 'test_osd_replicated_inconsistency_scenario' from '/home/jenkins/ceph-builds/openstack/IBM/8.1/rhel-9/Regression/19.2.1-126/rados/99/cephci/tests/rados/test_osd_replicated_inconsistency_scenario.py'> failed
